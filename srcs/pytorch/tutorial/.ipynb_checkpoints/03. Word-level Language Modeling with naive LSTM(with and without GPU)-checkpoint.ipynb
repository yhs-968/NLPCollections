{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data and preprocessing modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95\n",
      "[\n",
      " 93\n",
      " 77\n",
      " 75\n",
      " 26\n",
      " 47\n",
      " 90\n",
      " 42\n",
      " 82\n",
      " 77\n",
      " 48\n",
      " 47\n",
      " 22\n",
      " 19\n",
      " 68\n",
      " 25\n",
      " 40\n",
      "  3\n",
      "  0\n",
      " 10\n",
      " 11\n",
      " 53\n",
      " 52\n",
      " 19\n",
      " 39\n",
      "  8\n",
      " 10\n",
      " 21\n",
      " 40\n",
      "  4\n",
      "  0\n",
      " 40\n",
      " 77\n",
      " 23\n",
      " 56\n",
      " 18\n",
      " 79\n",
      " 54\n",
      "  2\n",
      " 94\n",
      "[torch.LongTensor of size 39]\n",
      ", \n",
      " 93\n",
      " 40\n",
      "  5\n",
      "  0\n",
      " 29\n",
      " 34\n",
      " 10\n",
      " 44\n",
      " 69\n",
      " 64\n",
      "  7\n",
      " 60\n",
      " 57\n",
      " 47\n",
      " 26\n",
      " 61\n",
      " 80\n",
      " 53\n",
      " 52\n",
      " 32\n",
      " 66\n",
      " 47\n",
      "  2\n",
      " 94\n",
      "[torch.LongTensor of size 24]\n",
      ", \n",
      " 93\n",
      " 40\n",
      "  6\n",
      "  0\n",
      "  7\n",
      " 63\n",
      " 19\n",
      " 33\n",
      " 37\n",
      "  0\n",
      " 59\n",
      " 10\n",
      " 74\n",
      " 70\n",
      " 38\n",
      "  7\n",
      " 49\n",
      "  1\n",
      " 46\n",
      " 30\n",
      " 53\n",
      " 51\n",
      " 24\n",
      " 15\n",
      " 28\n",
      " 62\n",
      "  1\n",
      " 83\n",
      " 58\n",
      " 45\n",
      " 13\n",
      "  7\n",
      " 81\n",
      "  0\n",
      " 84\n",
      " 27\n",
      " 45\n",
      " 40\n",
      " 86\n",
      " 12\n",
      "  9\n",
      " 87\n",
      " 67\n",
      " 17\n",
      " 48\n",
      "  0\n",
      " 78\n",
      " 31\n",
      "  1\n",
      " 85\n",
      " 43\n",
      " 89\n",
      " 73\n",
      " 14\n",
      "  2\n",
      " 94\n",
      "[torch.LongTensor of size 56]\n",
      ", \n",
      " 93\n",
      " 77\n",
      " 60\n",
      " 65\n",
      " 82\n",
      " 47\n",
      " 32\n",
      " 26\n",
      " 16\n",
      " 50\n",
      "  2\n",
      " 94\n",
      "[torch.LongTensor of size 12]\n",
      ", \n",
      " 93\n",
      "  7\n",
      " 35\n",
      " 55\n",
      " 20\n",
      " 71\n",
      " 76\n",
      " 77\n",
      " 88\n",
      " 56\n",
      " 77\n",
      " 75\n",
      " 36\n",
      " 41\n",
      " 72\n",
      "  4\n",
      "  2\n",
      " 94\n",
      "[torch.LongTensor of size 18]\n",
      "]\n",
      "<sos> the term deep learning was introduced to the machine learning community by rina dechter in 1986 , and artificial neural networks by igor aizenberg and colleagues in 2000 , in the context of boolean threshold neurons . <eos> <sos> in 2005 , faustino gomez and jürgen schmidhuber published a paper on learning deep pomdps through neural networks for reinforcement learning . <eos> <sos> in 2006 , a publication by geoff hinton , osindero and teh showed how a many - layered feedforward neural network could be effectively pre - trained one layer at a time , treating each layer in turn as an unsupervised restricted boltzmann machine , then fine - tuning it using supervised backpropagation . <eos> <sos> the paper referred to learning for deep belief nets . <eos> <sos> a google ngram chart shows that the usage of the term has increased since 2000 . <eos>\n"
     ]
    }
   ],
   "source": [
    "from modules.preprocess import Vocab\n",
    "with open('./data/dl_history.txt') as f:\n",
    "    text = f.read()\n",
    "vocab = Vocab(text, max_size = 100, lower = True, one_hot = True)\n",
    "print(len(vocab)) # size of the vocabulary\n",
    "sents = vocab.sents2id(text)\n",
    "print(sents)\n",
    "print(vocab.id2sents(sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers)\n",
    "\n",
    "    def forward(self, input, h, c):\n",
    "        output,(h,c) = self.lstm(input,(h,c))        \n",
    "        \n",
    "        return output,h,c\n",
    "\n",
    "    def init_h0c0(self, batch = 1):\n",
    "        # dimension: num_layers*num_directions, batch_size, hidden_size\n",
    "        h0 = Variable(torch.zeros(self.num_layers, batch_size, self.hidden_size)) \n",
    "        c0 = Variable(torch.zeros(self.num_layers, batch_size, self.hidden_size)) \n",
    "        \n",
    "        return h0,c0\n",
    "\n",
    "input_size = len(vocab)\n",
    "hidden_size = len(vocab)\n",
    "num_layers = 1\n",
    "rnn = LSTM(input_size, hidden_size, num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    0     1     2  ...     47    48    49\n",
      "   50    51    52  ...     97    98    99\n",
      "  100   101   102  ...    147   148   149\n",
      "       ...          ⋱          ...       \n",
      " 2000  2001  2002  ...   2047  2048  2049\n",
      " 2050  2051  2052  ...   2097  2098  2099\n",
      " 2100  2101  2102  ...   2147  2148  2149\n",
      "[torch.FloatTensor of size 43x50]\n",
      "\n",
      "\n",
      "(0 ,.,.) = \n",
      "     0     1     2  ...     47    48    49\n",
      "\n",
      "(1 ,.,.) = \n",
      "    50    51    52  ...     97    98    99\n",
      "\n",
      "(2 ,.,.) = \n",
      "   100   101   102  ...    147   148   149\n",
      "...\n",
      "\n",
      "(40,.,.) = \n",
      "  2000  2001  2002  ...   2047  2048  2049\n",
      "\n",
      "(41,.,.) = \n",
      "  2050  2051  2052  ...   2097  2098  2099\n",
      "\n",
      "(42,.,.) = \n",
      "  2100  2101  2102  ...   2147  2148  2149\n",
      "[torch.FloatTensor of size 43x1x50]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Little note about the view function\n",
    "seq_len = 43\n",
    "batch_size = 1\n",
    "dim = 50\n",
    "x = torch.arange(0,seq_len*dim).view(seq_len,dim)\n",
    "print(x)\n",
    "print(x.view(seq_len, batch_size, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "(0 ,.,.) = \n",
      " -0.0109  0.0016 -0.0046  ...   0.0170  0.0443 -0.0024\n",
      "\n",
      "(1 ,.,.) = \n",
      " -0.0349  0.0147 -0.0097  ...   0.0061  0.0721 -0.0135\n",
      "\n",
      "(2 ,.,.) = \n",
      " -0.0060  0.0321 -0.0458  ...   0.0028  0.0796 -0.0228\n",
      "...\n",
      "\n",
      "(36,.,.) = \n",
      " -0.0309  0.0299 -0.0641  ...  -0.0082  0.0660  0.0324\n",
      "\n",
      "(37,.,.) = \n",
      " -0.0293  0.0369 -0.0775  ...  -0.0083  0.0665  0.0051\n",
      "\n",
      "(38,.,.) = \n",
      " -0.0247  0.0331 -0.0893  ...  -0.0110  0.0446 -0.0181\n",
      "[torch.FloatTensor of size 39x1x95]\n",
      " Variable containing:\n",
      "(0 ,.,.) = \n",
      "\n",
      "Columns 0 to 8 \n",
      "  -0.0247  0.0331 -0.0893 -0.0057  0.0413 -0.0887  0.0023  0.0023 -0.0090\n",
      "\n",
      "Columns 9 to 17 \n",
      "   0.0039  0.0270 -0.0131 -0.0102 -0.0174 -0.0651 -0.0715 -0.0036  0.0009\n",
      "\n",
      "Columns 18 to 26 \n",
      "  -0.0004  0.0570  0.0807 -0.0421 -0.0495 -0.0908  0.0251  0.0197 -0.0697\n",
      "\n",
      "Columns 27 to 35 \n",
      "  -0.0460 -0.0654 -0.0115 -0.0731 -0.0160 -0.0313 -0.0555 -0.0118  0.0041\n",
      "\n",
      "Columns 36 to 44 \n",
      "   0.0023  0.0323  0.0253 -0.0347 -0.0227  0.0306  0.0009  0.0001  0.0201\n",
      "\n",
      "Columns 45 to 53 \n",
      "  -0.0261  0.0700  0.0091 -0.0570  0.0244 -0.0308 -0.0129 -0.0020  0.0135\n",
      "\n",
      "Columns 54 to 62 \n",
      "   0.0267  0.0247  0.0383 -0.0223  0.0637  0.0254 -0.0018  0.0017 -0.0696\n",
      "\n",
      "Columns 63 to 71 \n",
      "   0.0666  0.0349  0.0222 -0.0024  0.0680  0.0025  0.0029 -0.0172 -0.0030\n",
      "\n",
      "Columns 72 to 80 \n",
      "   0.0368 -0.0670  0.0009  0.0072  0.1262 -0.0384  0.0322  0.0666 -0.0253\n",
      "\n",
      "Columns 81 to 89 \n",
      "   0.0884 -0.0425 -0.0242  0.0727  0.0904 -0.0431  0.0682  0.0022  0.0012\n",
      "\n",
      "Columns 90 to 94 \n",
      "   0.0331  0.0008 -0.0110  0.0446 -0.0181\n",
      "[torch.FloatTensor of size 1x1x95]\n",
      " Variable containing:\n",
      "(0 ,.,.) = \n",
      "\n",
      "Columns 0 to 8 \n",
      "  -0.0499  0.0649 -0.1740 -0.0111  0.0928 -0.1905  0.0048  0.0046 -0.0181\n",
      "\n",
      "Columns 9 to 17 \n",
      "   0.0087  0.0541 -0.0286 -0.0224 -0.0332 -0.1262 -0.1464 -0.0077  0.0018\n",
      "\n",
      "Columns 18 to 26 \n",
      "  -0.0007  0.1185  0.1636 -0.0806 -0.0911 -0.1967  0.0498  0.0390 -0.1303\n",
      "\n",
      "Columns 27 to 35 \n",
      "  -0.0890 -0.1301 -0.0235 -0.1547 -0.0322 -0.0639 -0.1112 -0.0237  0.0078\n",
      "\n",
      "Columns 36 to 44 \n",
      "   0.0043  0.0722  0.0470 -0.0655 -0.0445  0.0601  0.0018  0.0002  0.0378\n",
      "\n",
      "Columns 45 to 53 \n",
      "  -0.0568  0.1339  0.0174 -0.1069  0.0444 -0.0623 -0.0248 -0.0037  0.0286\n",
      "\n",
      "Columns 54 to 62 \n",
      "   0.0531  0.0530  0.0870 -0.0480  0.1215  0.0523 -0.0034  0.0034 -0.1488\n",
      "\n",
      "Columns 63 to 71 \n",
      "   0.1366  0.0713  0.0442 -0.0049  0.1328  0.0051  0.0057 -0.0379 -0.0065\n",
      "\n",
      "Columns 72 to 80 \n",
      "   0.0739 -0.1339  0.0017  0.0136  0.2583 -0.0765  0.0661  0.1342 -0.0539\n",
      "\n",
      "Columns 81 to 89 \n",
      "   0.1783 -0.0852 -0.0458  0.1455  0.1670 -0.0877  0.1308  0.0043  0.0024\n",
      "\n",
      "Columns 90 to 94 \n",
      "   0.0659  0.0016 -0.0218  0.0872 -0.0356\n",
      "[torch.FloatTensor of size 1x1x95]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Process one string with a zero-vector inital hidden state / cell state\n",
    "inputs = vocab.id2emb(sents[0])\n",
    "seq_len = inputs.size()[0]\n",
    "batch_size = 1\n",
    "inputs = Variable(inputs.view(seq_len, batch_size, -1))\n",
    "h0,c0 = rnn.init_h0c0()\n",
    "\n",
    "output,h,c = rnn(inputs, h0, c0)\n",
    "print(output,h,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the Training dataset\n",
    "onehots = [vocab.id2emb(sent) for sent in sents]\n",
    "\n",
    "# Build inputs / targets as lists of tensors\n",
    "inputs = [sent[:-1,:] for sent in onehots]\n",
    "targets = [sent[1:,:] for sent in onehots]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = len(vocab)\n",
    "hidden_size = len(vocab)\n",
    "num_layers = 1\n",
    "batch_size = 1\n",
    "rnn = LSTM(input_size, hidden_size, num_layers)\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(rnn.parameters(), lr = .005)\n",
    "\n",
    "def run_epoch(inputs, targets):\n",
    "    # flush the gradients\n",
    "    optimizer.zero_grad()\n",
    "    # initial hidden state(h0)\n",
    "    h,c = rnn.init_h0c0()\n",
    "    # training loss\n",
    "    loss = 0\n",
    "    # Run a RNN through the training samples\n",
    "    for i in range(len(inputs)):\n",
    "        input = inputs[i]\n",
    "        target = targets[i]\n",
    "        \n",
    "        seq_len = input.size()[0]\n",
    "        \n",
    "        input = Variable(input.view(seq_len, batch_size, -1))\n",
    "        target = Variable(target.view(seq_len, batch_size, -1))\n",
    "        # Note: new hidden layer output is generated for every loop, so we have to send the\n",
    "        # hidden weights to cuda for every loop\n",
    "        output, h, c = rnn(input, h, c) \n",
    "        loss += loss_fn(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return output, loss.data[0]\n",
    "\n",
    "def train(inputs, targets, n_epochs = 100, print_every = 10):\n",
    "    total_loss = 0.0\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        output, loss = run_epoch(inputs, targets)\n",
    "        if epoch % print_every == 0:\n",
    "            print('Epoch: %2i / Loss: %.7f' % (epoch, loss))\n",
    "            \n",
    "def test(input_sent):\n",
    "    h, c = rnn.init_h0c0()\n",
    "    seq_len = input_sent.size()[0]\n",
    "    input_sent = Variable(input_sent.view(seq_len, batch_size, -1))\n",
    "    \n",
    "    output, h, c = rnn(input_sent, h, c)\n",
    "    _, argmaxs = torch.max(output, dim = 0)\n",
    "    sent = argmaxs.view(-1).data.numpy().tolist()\n",
    "    for i in sent:\n",
    "        print(vocab[i],end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100 / Loss: 0.0120398\n",
      "Epoch: 200 / Loss: 0.0044662\n",
      "Epoch: 300 / Loss: 0.0022816\n",
      "Epoch: 400 / Loss: 0.0014084\n",
      "Epoch: 500 / Loss: 0.0009812\n",
      "Epoch: 600 / Loss: 0.0007276\n",
      "Epoch: 700 / Loss: 0.0005505\n",
      "Epoch: 800 / Loss: 0.0004282\n",
      "Epoch: 900 / Loss: 0.0003496\n",
      "Epoch: 1000 / Loss: 0.0002941\n"
     ]
    }
   ],
   "source": [
    "# run_epoch\n",
    "train(inputs, targets, n_epochs = 1000, print_every = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "belief and has be each has , , context , boltzmann boolean fine hinton google , aizenberg backpropagation geoff colleagues 1986 dechter and fine . at . deep a has , , , has feedforward , geoff community , community deep . 2005 a boolean effectively , 1986 aizenberg 2006 1986 belief chart by google , for , , as 2000 hinton deep feedforward , - colleagues hinton as boolean a effectively . , 1986 - faustino a and gomez 2000 has 2006 belief 1986 effectively , fine fine aizenberg 2000 . effectively deep hinton \n",
      ". 2005 colleagues - - - - an an . 2005 chart - - and . . as 2000 2005 - backpropagation colleagues - 1986 . at 2000 . 1986 - 2000 boolean colleagues 2000 and 1986 - , as be 1986 2005 . 2006 a . as . 2000 1986 - boltzmann belief 2005 as an artificial as as and backpropagation an 2000 aizenberg artificial by 1986 as a . . 2000 . be . 1986 - 1986 1986 be . 1986 2000 1986 2006 as as 2000 aizenberg , chart an 1986 community \n",
      "for dechter neural - 2005 - - feedforward aizenberg igor and 2005 how faustino networks community . introduced context 2005 as by . - colleagues a 2005 gomez context 1986 boolean layered . 2006 and 2000 1986 a at , has 1986 2005 many artificial effectively boltzmann . it be 2005 chart a by - 2005 . and each an . 2000 could 2000 . 1986 and increased 2006 artificial as at 2005 network artificial . . - layer could 2005 fine colleagues deep geoff machine hinton in 2005 nets , aizenberg neural 1986 neurons \n",
      "1986 - an and aizenberg , aizenberg . - . , 1986 . , 1986 2000 a an , a 2000 . aizenberg 2006 . 2000 2006 1986 aizenberg . , , 2005 a 1986 1986 aizenberg an , - a 2000 2006 . 1986 1986 , 2000 and , aizenberg . 2000 2000 1986 a - . , , 2006 a - an . . 2006 2000 . . . 2000 2006 . . - 2005 , 2005 and 1986 . 1986 . , and 2005 . and 1986 2005 a 2000 a and \n",
      "1986 - be - backpropagation 2005 backpropagation an - - - 2005 . - 1986 2000 aizenberg 2006 an an 1986 . 1986 a 2005 . 2006 at - an - , 2005 - , - artificial aizenberg , . 2000 as , be - - , 2000 - - aizenberg 2000 aizenberg 1986 artificial . aizenberg , , - - . and - 1986 . as . a - - 2000 at - artificial and 2005 2006 2000 and 2005 - 2000 artificial backpropagation a , - a - , an aizenberg 2006 belief \n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "torch.manual_seed(7)\n",
    "for i in range(len(inputs)):\n",
    "    try:\n",
    "        test(inputs[i])\n",
    "        print()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with Variable-length sequences for cuDNN in PyTorch\n",
    "- References\n",
    "    - [Simple working example how to use packing for variable-length sequence inputs for rnn](https://discuss.pytorch.org/t/simple-working-example-how-to-use-packing-for-variable-length-sequence-inputs-for-rnn/2120)\n",
    "    - [Feeding Data to PyTorch RNNs](https://djosix.github.io/2017/09/05/Feeding-Data-to-Pytorch-RNNs/)\n",
    "    - [How to use pad_packed_sequence in PyTorch](https://www.snip2code.com/Snippet/1950100/How-to-use-pad_packed_sequence-in-pytorc)\n",
    "    - [RNN sequence padding with batch_first](https://github.com/pytorch/pytorch/issues/1176)\n",
    "    - [padded_rnn.py](https://gist.github.com/MaximumEntropy/918d4ad7c931bc14b475008c00aa09f1)\n",
    "    - [About the variable length input in RNN scenario](https://discuss.pytorch.org/t/about-the-variable-length-input-in-rnn-scenario/345/7)\n",
    "    - [How can i compute seq2seq loss using mask?](https://discuss.pytorch.org/t/how-can-i-compute-seq2seq-loss-using-mask/861/7)\n",
    "- Steps\n",
    "    1. Pad the input sequences to the same length\n",
    "    2. Sort them by their lengths (asc order)\n",
    "    3. Use torch.nn.utils.rnn.pack_padded_sequence()\n",
    "    4. RNN\n",
    "    5. Use torch.nn.utils.rnn.pad_packed_sequence()\n",
    "    6. Unsort output sequences\n",
    "    7. Unpad output sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95\n",
      "[\n",
      " 93\n",
      " 77\n",
      " 75\n",
      " 26\n",
      " 47\n",
      " 90\n",
      " 42\n",
      " 82\n",
      " 77\n",
      " 48\n",
      " 47\n",
      " 22\n",
      " 19\n",
      " 68\n",
      " 25\n",
      " 40\n",
      "  3\n",
      "  0\n",
      " 10\n",
      " 11\n",
      " 53\n",
      " 52\n",
      " 19\n",
      " 39\n",
      "  8\n",
      " 10\n",
      " 21\n",
      " 40\n",
      "  4\n",
      "  0\n",
      " 40\n",
      " 77\n",
      " 23\n",
      " 56\n",
      " 18\n",
      " 79\n",
      " 54\n",
      "  2\n",
      " 94\n",
      "[torch.LongTensor of size 39]\n",
      ", \n",
      " 93\n",
      " 40\n",
      "  5\n",
      "  0\n",
      " 29\n",
      " 34\n",
      " 10\n",
      " 44\n",
      " 69\n",
      " 64\n",
      "  7\n",
      " 60\n",
      " 57\n",
      " 47\n",
      " 26\n",
      " 61\n",
      " 80\n",
      " 53\n",
      " 52\n",
      " 32\n",
      " 66\n",
      " 47\n",
      "  2\n",
      " 94\n",
      "[torch.LongTensor of size 24]\n",
      ", \n",
      " 93\n",
      " 40\n",
      "  6\n",
      "  0\n",
      "  7\n",
      " 63\n",
      " 19\n",
      " 33\n",
      " 37\n",
      "  0\n",
      " 59\n",
      " 10\n",
      " 74\n",
      " 70\n",
      " 38\n",
      "  7\n",
      " 49\n",
      "  1\n",
      " 46\n",
      " 30\n",
      " 53\n",
      " 51\n",
      " 24\n",
      " 15\n",
      " 28\n",
      " 62\n",
      "  1\n",
      " 83\n",
      " 58\n",
      " 45\n",
      " 13\n",
      "  7\n",
      " 81\n",
      "  0\n",
      " 84\n",
      " 27\n",
      " 45\n",
      " 40\n",
      " 86\n",
      " 12\n",
      "  9\n",
      " 87\n",
      " 67\n",
      " 17\n",
      " 48\n",
      "  0\n",
      " 78\n",
      " 31\n",
      "  1\n",
      " 85\n",
      " 43\n",
      " 89\n",
      " 73\n",
      " 14\n",
      "  2\n",
      " 94\n",
      "[torch.LongTensor of size 56]\n",
      ", \n",
      " 93\n",
      " 77\n",
      " 60\n",
      " 65\n",
      " 82\n",
      " 47\n",
      " 32\n",
      " 26\n",
      " 16\n",
      " 50\n",
      "  2\n",
      " 94\n",
      "[torch.LongTensor of size 12]\n",
      ", \n",
      " 93\n",
      "  7\n",
      " 35\n",
      " 55\n",
      " 20\n",
      " 71\n",
      " 76\n",
      " 77\n",
      " 88\n",
      " 56\n",
      " 77\n",
      " 75\n",
      " 36\n",
      " 41\n",
      " 72\n",
      "  4\n",
      "  2\n",
      " 94\n",
      "[torch.LongTensor of size 18]\n",
      "]\n",
      "<sos> the term deep learning was introduced to the machine learning community by rina dechter in 1986 , and artificial neural networks by igor aizenberg and colleagues in 2000 , in the context of boolean threshold neurons . <eos> <sos> in 2005 , faustino gomez and jürgen schmidhuber published a paper on learning deep pomdps through neural networks for reinforcement learning . <eos> <sos> in 2006 , a publication by geoff hinton , osindero and teh showed how a many - layered feedforward neural network could be effectively pre - trained one layer at a time , treating each layer in turn as an unsupervised restricted boltzmann machine , then fine - tuning it using supervised backpropagation . <eos> <sos> the paper referred to learning for deep belief nets . <eos> <sos> a google ngram chart shows that the usage of the term has increased since 2000 . <eos>\n"
     ]
    }
   ],
   "source": [
    "from modules.preprocess import Vocab\n",
    "with open('./data/dl_history.txt') as f:\n",
    "    text = f.read()\n",
    "vocab = Vocab(text, max_size = 100, lower = True, one_hot = True)\n",
    "print(len(vocab)) # size of the vocabulary\n",
    "sents = vocab.sents2id(text)\n",
    "print(sents)\n",
    "print(vocab.id2sents(sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A little exercise on `pack_padded_sequence`, and `pad_packed_sequence`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_seq_len = 20\n",
    "num_batches = 10\n",
    "x = Variable(torch.randn(max_seq_len, num_batches, 30))\n",
    "lens = list(range(max_seq_len, max_seq_len - num_batches, -1)) # sequence of lengths of each batches\n",
    "x_packed = pack_padded_sequence(x, lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[20, 19, 18, 17, 16, 15, 14, 13, 12, 11]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "(0 ,.,.) = \n",
       " -1.0381  0.4584 -0.2096  ...  -0.6112  0.8608  0.8062\n",
       " -1.4102 -0.3467 -1.4297  ...  -0.2382 -0.1597  0.4433\n",
       " -0.0803 -0.0278 -0.2597  ...   0.5534  1.3503  2.4055\n",
       "           ...             ⋱             ...          \n",
       " -1.9267  0.5453  0.7812  ...   0.2765 -0.1865  0.6421\n",
       " -0.1019 -0.7288  1.4008  ...  -0.0032 -0.6151  1.9332\n",
       " -1.5708  1.7737  0.3035  ...  -1.3612 -0.3341 -1.0302\n",
       "\n",
       "(1 ,.,.) = \n",
       "  0.7460  0.2938  0.0664  ...   0.1086  0.7538  0.3079\n",
       "  0.1405  1.1716  0.3962  ...  -1.1237 -1.2468  2.7688\n",
       " -0.2353  0.2731  0.0772  ...   0.6473  0.6643 -0.8425\n",
       "           ...             ⋱             ...          \n",
       " -0.1701 -0.8283 -0.3167  ...  -0.7884 -1.6525  0.8426\n",
       "  2.1250 -0.7835  1.3479  ...  -1.7399  0.3085 -0.7343\n",
       " -0.2546  0.2770 -1.0888  ...   0.1493 -1.5022 -1.6231\n",
       "\n",
       "(2 ,.,.) = \n",
       "  0.5846  0.9017 -0.8200  ...   0.4783  0.2394  1.0083\n",
       " -0.0760  0.7519  0.7903  ...   1.6909  0.2236 -1.0112\n",
       "  0.1779 -0.3156  0.3472  ...  -1.3540  0.1746  0.3665\n",
       "           ...             ⋱             ...          \n",
       "  1.0611  0.8954 -0.7117  ...   0.1057  2.6461  0.1509\n",
       "  0.9604  0.0626  1.0461  ...  -0.2133  0.4628  1.3604\n",
       "  0.5928  0.1539 -0.3298  ...  -1.6325 -0.7247 -0.6966\n",
       "...\n",
       "\n",
       "(17,.,.) = \n",
       "  0.6711 -0.7016 -1.0775  ...  -0.9319 -1.3176  1.1683\n",
       "  2.2685  0.6266  0.6636  ...  -0.1249 -1.2228  0.4039\n",
       "  0.9496 -0.6177  1.1500  ...  -0.1316  0.7738  0.8918\n",
       "           ...             ⋱             ...          \n",
       " -1.4822 -0.7324  0.6110  ...  -0.1773 -0.6013 -0.8784\n",
       "  0.4468 -1.4979 -0.0970  ...  -1.2873  1.3951  0.6633\n",
       " -0.0122  0.4938 -0.9825  ...   0.3675  0.5047  1.6126\n",
       "\n",
       "(18,.,.) = \n",
       " -0.0565 -1.1523 -1.0487  ...  -1.0301  0.1426 -0.8216\n",
       "  0.1549  0.0952  0.3328  ...   1.9159  1.2383 -1.9206\n",
       " -0.9849 -0.3866 -0.6676  ...   0.4192 -0.1785 -0.8875\n",
       "           ...             ⋱             ...          \n",
       "  0.3676 -0.8216  0.7939  ...  -1.0875 -0.6933 -0.5422\n",
       " -0.1056 -0.3630  0.3305  ...   1.9381 -0.3519 -0.8409\n",
       "  0.1298  1.7773  1.3805  ...  -0.8079 -0.1534 -1.1105\n",
       "\n",
       "(19,.,.) = \n",
       "  0.6701  0.5796  0.4989  ...   0.4067 -0.4743  2.4214\n",
       " -1.5280  1.5557  1.2194  ...  -0.8719  0.2982  0.0187\n",
       " -1.1686 -0.1825  0.3854  ...  -3.0586  0.5506  1.6744\n",
       "           ...             ⋱             ...          \n",
       "  0.0968  2.5581  0.9129  ...   1.1050  0.3008 -0.2416\n",
       " -0.7639  0.3051  1.2994  ...   2.0298  0.2172 -0.2694\n",
       " -0.9019 -0.4003  0.0534  ...  -0.1025 -0.0596 -0.2740\n",
       "[torch.FloatTensor of size 20x10x30]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PackedSequence(data=Variable containing:\n",
       "-1.0381  0.4584 -0.2096  ...  -0.6112  0.8608  0.8062\n",
       "-1.4102 -0.3467 -1.4297  ...  -0.2382 -0.1597  0.4433\n",
       "-0.0803 -0.0278 -0.2597  ...   0.5534  1.3503  2.4055\n",
       "          ...             ⋱             ...          \n",
       "-0.0565 -1.1523 -1.0487  ...  -1.0301  0.1426 -0.8216\n",
       " 0.1549  0.0952  0.3328  ...   1.9159  1.2383 -1.9206\n",
       " 0.6701  0.5796  0.4989  ...   0.4067 -0.4743  2.4214\n",
       "[torch.FloatTensor of size 155x30]\n",
       ", batch_sizes=[10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_packed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the packing/unpacking functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To utilize cuDNN for variable length input on PyTorch, we need to use `torch.utils.nn.rnn.pack_padded_sequence` and `torch.utils.nn.rnn.pad_packed_sequence`.\n",
    "- `pack_padded_sequence` packs a **padded** tensor into a `PackedSequence` object, which is internally handled by nn.LSTM.\n",
    "- `pad_padded_sequence` unpacks a `PackedSequence object` into a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the Training dataset\n",
    "onehots = [vocab.id2emb(sent) for sent in sents]\n",
    "\n",
    "# Build inputs / targets as lists of tensors\n",
    "inputs = [sent[:-1,:] for sent in onehots]\n",
    "targets = [sent[1:,:] for sent in onehots]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PackedSequence(data=Variable containing:\n",
      "    0     0     0  ...      0     1     0\n",
      "    0     0     0  ...      0     1     0\n",
      "    0     0     0  ...      0     1     0\n",
      "       ...          ⋱          ...       \n",
      "    0     0     0  ...      0     0     0\n",
      "    0     0     0  ...      0     0     0\n",
      "    0     0     1  ...      0     0     0\n",
      "[torch.cuda.FloatTensor of size 144x95 (GPU 0)]\n",
      ", batch_sizes=[5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 4, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "[Variable containing:\n",
      "    0     0     0  ...      0     1     0\n",
      "    0     0     0  ...      0     0     0\n",
      "    0     0     0  ...      0     0     0\n",
      "       ...          ⋱          ...       \n",
      "    0     0     0  ...      0     0     0\n",
      "    0     0     0  ...      0     0     0\n",
      "    0     0     1  ...      0     0     0\n",
      "[torch.cuda.FloatTensor of size 38x95 (GPU 0)]\n",
      ", Variable containing:\n",
      "    0     0     0  ...      0     1     0\n",
      "    0     0     0  ...      0     0     0\n",
      "    0     0     0  ...      0     0     0\n",
      "       ...          ⋱          ...       \n",
      "    0     0     0  ...      0     0     0\n",
      "    0     0     0  ...      0     0     0\n",
      "    0     0     1  ...      0     0     0\n",
      "[torch.cuda.FloatTensor of size 23x95 (GPU 0)]\n",
      ", Variable containing:\n",
      "    0     0     0  ...      0     1     0\n",
      "    0     0     0  ...      0     0     0\n",
      "    0     0     0  ...      0     0     0\n",
      "       ...          ⋱          ...       \n",
      "    0     0     0  ...      0     0     0\n",
      "    0     0     0  ...      0     0     0\n",
      "    0     0     1  ...      0     0     0\n",
      "[torch.cuda.FloatTensor of size 55x95 (GPU 0)]\n",
      ", Variable containing:\n",
      "    0     0     0  ...      0     1     0\n",
      "    0     0     0  ...      0     0     0\n",
      "    0     0     0  ...      0     0     0\n",
      "       ...          ⋱          ...       \n",
      "    0     0     0  ...      0     0     0\n",
      "    0     0     0  ...      0     0     0\n",
      "    0     0     1  ...      0     0     0\n",
      "[torch.cuda.FloatTensor of size 11x95 (GPU 0)]\n",
      ", Variable containing:\n",
      "    0     0     0  ...      0     1     0\n",
      "    0     0     0  ...      0     0     0\n",
      "    0     0     0  ...      0     0     0\n",
      "       ...          ⋱          ...       \n",
      "    0     0     0  ...      0     0     0\n",
      "    0     0     0  ...      0     0     0\n",
      "    0     0     1  ...      0     0     0\n",
      "[torch.cuda.FloatTensor of size 17x95 (GPU 0)]\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Helper functions\n",
    "def pack(seq):\n",
    "    '''\n",
    "    Packs a list of variable-length tensors into a packed sequence\n",
    "    \n",
    "    Args:\n",
    "        seq: 2 dim tensor, where each row corresponds to an individual element.\n",
    "        \n",
    "    Returns:\n",
    "        packed: PackedSequence\n",
    "        orders: ordered indices for the original sequence before the sorting.\n",
    "        later used to retrieve the original ordering of the sequences.\n",
    "        \n",
    "    '''\n",
    "    seq_sorted = []\n",
    "    orders = []\n",
    "    \n",
    "    for i, tensor in sorted(enumerate(seq), key = lambda t: -t[1].size()[0]):\n",
    "        seq_sorted.append(tensor)\n",
    "        orders.append(i)\n",
    "        \n",
    "    lengths = list(map(lambda t: t.size()[0], seq_sorted))\n",
    "    \n",
    "    max_seq_len = lengths[0]\n",
    "    dim = seq_sorted[0].size()[1]\n",
    "    batch_size = len(seq_sorted)\n",
    "    \n",
    "    # Build a padded sequence\n",
    "    padded_sequence = Variable(torch.zeros(max_seq_len, batch_size, dim))\n",
    "    if torch.cuda.is_available():\n",
    "        padded_sequence = padded_sequence.cuda()\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        padded_sequence[:lengths[i], i, :] = seq_sorted[i]\n",
    "    \n",
    "    # pack the padded sequence\n",
    "    packed = pack_padded_sequence(padded_sequence, lengths)\n",
    "    \n",
    "    return packed, orders\n",
    "\n",
    "def unpack(packed, orders):\n",
    "    '''\n",
    "    Unpacks a packed sequence\n",
    "    \n",
    "    Args:\n",
    "        packed: PackedSequence\n",
    "        \n",
    "    Returns:\n",
    "        unpacked_masked\n",
    "    '''\n",
    "    unpacked, lengths = pad_packed_sequence(packed)\n",
    "    \n",
    "    # Masking\n",
    "    unpacked_masked = [unpacked[:lengths[batch], batch, :] for batch in range(len(lengths))]\n",
    "    \n",
    "    # Unsort\n",
    "    unpacked_masked = [tensor for i, tensor in sorted(zip(orders, unpacked_masked))]\n",
    "        \n",
    "    return unpacked_masked\n",
    "    \n",
    "packed, orders = pack(inputs)\n",
    "print(packed)\n",
    "unpacked = unpack(packed, orders)\n",
    "print(unpacked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the LSTM Cell\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            self.cuda()\n",
    "\n",
    "    def forward(self, input, h, c):\n",
    "        output,(h,c) = self.lstm(input,(h,c))\n",
    "        return output,h,c\n",
    "\n",
    "    def init_h0c0(self, batch_size = 1):\n",
    "        # dimension: num_layers*num_directions, batch_size, hidden_size\n",
    "        h0 = Variable(torch.zeros(self.num_layers, batch_size, self.hidden_size))\n",
    "        c0 = Variable(torch.zeros(self.num_layers, batch_size, self.hidden_size))\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            h0 = h0.cuda()\n",
    "            c0 = c0.cuda()\n",
    "        \n",
    "        return h0,c0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.Size([38, 95]), torch.Size([23, 95]), torch.Size([55, 95]), torch.Size([11, 95]), torch.Size([17, 95])]\n"
     ]
    }
   ],
   "source": [
    "# Test over one input\n",
    "input_size = len(vocab)\n",
    "hidden_size = len(vocab)\n",
    "num_layers = 1\n",
    "rnn = LSTM(input_size, hidden_size, num_layers).cuda()\n",
    "\n",
    "inputs_packed, orders = pack(inputs)\n",
    "h0,c0 = rnn.init_h0c0(batch_size = 5)\n",
    "outputs_packed, h, c = rnn(inputs_packed, h0, c0)\n",
    "outputs = unpack(outputs_packed, orders)\n",
    "\n",
    "print(list(map(lambda t: t.size(),outputs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100 / Loss: 0.0133031\n",
      "Epoch: 200 / Loss: 0.0062557\n",
      "Epoch: 300 / Loss: 0.0042884\n",
      "Epoch: 400 / Loss: 0.0035189\n",
      "Epoch: 500 / Loss: 0.0031153\n",
      "Epoch: 600 / Loss: 0.0029421\n",
      "Epoch: 700 / Loss: 0.0027505\n",
      "Epoch: 800 / Loss: 0.0026356\n",
      "Epoch: 900 / Loss: 0.0025679\n",
      "Epoch: 1000 / Loss: 0.0025200\n",
      "effectively and has be each , . , context . could boolean belief backpropagation google chart aizenberg deep geoff colleagues . dechter and fine chart at . deep 2006 fine an and colleagues . backpropagation , geoff as boolean community deep . 2005 feedforward . an belief 1986 aizenberg a 2000 boltzmann chart by google , for . deep belief - belief faustino faustino boolean - . 1986 as dechter boolean effectively 1986 effectively a - faustino a an gomez 2000 backpropagation 2006 feedforward an a . belief effectively . 2000 belief belief chart hinton \n",
      ". - colleagues chart chart - - an boolean at 2005 be as as as boltzmann by as - at artificial artificial 2000 - as as at as as 1986 be 1986 boolean as 2000 , an - as boolean , - backpropagation . 2006 at as chart as - backpropagation as boltzmann belief as , as artificial as as and backpropagation as - aizenberg , by backpropagation backpropagation a 2006 as chart and - as artificial , - chart be - artificial as artificial boltzmann as backpropagation as as backpropagation as a as community \n",
      "for dechter neural and artificial - - backpropagation 2006 igor and layer how faustino networks community it introduced - 2005 2000 2006 layer . colleagues 2006 it gomez context - boolean layered it 2006 2000 , as a at chart has it by many 2005 effectively boltzmann 2000 it be . chart artificial by . 2000 - 2000 each an , belief could 2000 . , 2005 increased layer artificial as hinton . network artificial it layer , layer jürgen boolean fine . deep geoff machine hinton in backpropagation nets in . and . neurons \n",
      "1986 2000 an a aizenberg , , , 1986 1986 - 2006 an - 2005 2000 a - 2000 2005 1986 2000 - 2006 2006 2005 2006 2005 aizenberg and 2000 2000 2005 - 2005 , 2000 2000 2006 2000 , 2000 a 1986 1986 2006 - 2000 - 2000 aizenberg an 2000 a aizenberg , - aizenberg 2006 aizenberg - . . 2000 2000 . 1986 2000 2006 2006 1986 - 2000 2000 2000 - 2000 , 2000 2000 a 2000 1986 2006 2000 2000 - 2000 1986 1986 a aizenberg 2000 2005 and \n",
      "- - be backpropagation backpropagation , , , 2006 artificial backpropagation backpropagation - a , backpropagation at and - - 1986 2006 and be a backpropagation and aizenberg aizenberg and an at , - backpropagation - artificial 2006 2006 backpropagation , as artificial 2005 a and - artificial a artificial backpropagation a backpropagation as and . aizenberg - a backpropagation , an an - - , a 2006 a be 2006 2000 at an artificial and 2005 2006 aizenberg 2006 and aizenberg and 1986 - . - backpropagation a 1986 artificial backpropagation be aizenberg belief \n"
     ]
    }
   ],
   "source": [
    "# Train the network\n",
    "\n",
    "input_size = len(vocab)\n",
    "hidden_size = len(vocab)\n",
    "num_layers = 1\n",
    "batch_size = 1\n",
    "rnn = LSTM(input_size, hidden_size, num_layers).cuda()\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(rnn.parameters(), lr = .005)\n",
    "\n",
    "def run_epoch(inputs, targets):\n",
    "    # flush the gradients\n",
    "    optimizer.zero_grad()\n",
    "    # initial hidden state(h0)\n",
    "    h,c = rnn.init_h0c0(batch_size = 5)\n",
    "    # training loss\n",
    "    loss = 0\n",
    "    \n",
    "    targets = [Variable(tensor).cuda() for tensor in targets]\n",
    "    # Run a RNN through the training samples\n",
    "    inputs_packed, orders = pack(inputs)\n",
    "    outputs_packed, h, c = rnn(inputs_packed, h, c)\n",
    "    outputs = unpack(outputs_packed, orders)\n",
    "    \n",
    "    for out, target in zip(outputs, targets):\n",
    "        loss += loss_fn(out, target)\n",
    "        \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return outputs, loss.data[0]\n",
    "\n",
    "def train(inputs, targets, n_epochs = 100, print_every = 10):\n",
    "    total_loss = 0.0\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        output, loss = run_epoch(inputs, targets)\n",
    "        if epoch % print_every == 0:\n",
    "            print('Epoch: %2i / Loss: %.7f' % (epoch, loss))\n",
    "            \n",
    "def test(input_sent):\n",
    "    h, c = rnn.init_h0c0()\n",
    "    seq_len = input_sent.size()[0]\n",
    "    input_sent = Variable(input_sent.view(seq_len, batch_size, -1))\n",
    "    \n",
    "    output, h, c = rnn(input_sent, h, c)\n",
    "    _, argmaxs = torch.max(output, dim = 0)\n",
    "    \n",
    "    # flatten the sorted indices\n",
    "    sent = argmaxs.view(-1).data.cpu().numpy().tolist()\n",
    "    for i in sent:\n",
    "        print(vocab[i],end=' ')\n",
    "        \n",
    "# run_epoch(inputs, targets)\n",
    "train(inputs, targets, n_epochs = 1000, print_every = 100)\n",
    "torch.manual_seed(7)\n",
    "for i in range(len(inputs)):\n",
    "    try:\n",
    "        test(inputs[i].cuda())\n",
    "        print()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
