{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "[[48, 43, 42, 18, 26, 46, 47, 45, 43, 27, 26, 47, 15, 47, 47, 22, 47, 47, 47, 9, 47, 8, 12, 47, 31, 30, 15, 21, 47, 12, 47, 22, 4, 0, 22, 43, 16, 33, 47, 47, 32, 2, 49], [48, 7, 47, 8, 22, 5, 0, 19, 47, 12, 23, 38, 47, 10, 35, 47, 26, 18, 47, 7, 47, 8, 47, 31, 30, 20, 47, 26, 2, 49], [48, 22, 47, 0, 10, 37, 15, 47, 47, 0, 34, 12, 47, 7, 47, 9, 6, 8, 47, 47, 10, 47, 1, 25, 47, 31, 29, 17, 47, 47, 36, 1, 47, 47, 24, 47, 10, 44, 0, 47, 47, 24, 22, 47, 13, 11, 47, 47, 14, 27, 0, 47, 47, 1, 47, 47, 47, 41, 47, 2, 49], [48, 7, 47, 8, 43, 35, 47, 45, 26, 20, 18, 47, 28, 2, 49], [48, 10, 47, 47, 47, 39, 47, 43, 47, 33, 43, 42, 47, 47, 40, 4, 2, 49], [48, 7, 47, 8, 49]]\n",
      "_BEGIN_ the term deep learning was UNK to the machine learning UNK by UNK UNK in UNK UNK UNK ][ UNK ] and UNK neural networks by igor UNK and UNK in 2000 , in the context of UNK UNK neurons . _END_ _BEGIN_ [ UNK ] in 2005 , faustino UNK and jürgen schmidhuber UNK a paper UNK learning deep UNK [ UNK ] UNK neural networks for UNK learning . _END_ _BEGIN_ in UNK , a publication by UNK UNK , osindero and UNK [ UNK ][ 27 ] UNK UNK a UNK - layered UNK neural network could UNK UNK pre - UNK UNK layer UNK a time , UNK UNK layer in UNK as an UNK UNK boltzmann machine , UNK UNK - UNK UNK UNK supervised UNK . _END_ _BEGIN_ [ UNK ] the paper UNK to learning for deep UNK nets . _END_ _BEGIN_ a UNK UNK UNK shows UNK the UNK of the term UNK UNK since 2000 . _END_ _BEGIN_ [ UNK ] _END_\n",
      ",\n",
      "0\n",
      "torch.Size([43, 50])\n",
      "['_BEGIN_', 'the', 'term', 'deep', 'learning', 'was', 'UNK', 'to', 'the', 'machine', 'learning', 'UNK', 'by', 'UNK', 'UNK', 'in', 'UNK', 'UNK', 'UNK', '][', 'UNK', ']', 'and', 'UNK', 'neural', 'networks', 'by', 'igor', 'UNK', 'and', 'UNK', 'in', '2000', ',', 'in', 'the', 'context', 'of', 'UNK', 'UNK', 'neurons', '.', '_END_']\n"
     ]
    }
   ],
   "source": [
    "from custom_utils.preprocess import Vocab\n",
    "with open('./data/dl_history.txt') as f:\n",
    "    text = f.read()\n",
    "\n",
    "vocab = Vocab(text, top_k = 50)\n",
    "\n",
    "print(vocab.V)\n",
    "\n",
    "sents = vocab.sents2id(text)\n",
    "print(sents)\n",
    "print(vocab.id2sents(sents))\n",
    "print(vocab[0])\n",
    "print(vocab[vocab[0]])\n",
    "\n",
    "onehot = vocab.sent2onehot(sents[0])\n",
    "print(onehot.size())\n",
    "print(vocab.onehot2sent(onehot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build the Training dataset\n",
    "onehots = [vocab.sent2onehot(sent) for sent in sents]\n",
    "\n",
    "# Build inputs / targets as lists of tensors\n",
    "inputs = [sent[:-1,:] for sent in onehots]\n",
    "targets = [sent[1:,:] for sent in onehots]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the reference for general batch processing in RNNs, see [here](https://www.quora.com/How-are-inputs-fed-into-the-LSTM-RNN-network-in-mini-batch-method)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\n",
       "     0     0     0  ...      0     1     0\n",
       "     0     0     0  ...      0     0     0\n",
       "     0     0     0  ...      0     0     0\n",
       "        ...          ⋱          ...       \n",
       "     0     0     0  ...      1     0     0\n",
       "     0     0     0  ...      0     0     0\n",
       "     0     0     1  ...      0     0     0\n",
       " [torch.FloatTensor of size 42x50], \n",
       "     0     0     0  ...      0     1     0\n",
       "     0     0     0  ...      0     0     0\n",
       "     0     0     0  ...      1     0     0\n",
       "        ...          ⋱          ...       \n",
       "     0     0     0  ...      1     0     0\n",
       "     0     0     0  ...      0     0     0\n",
       "     0     0     1  ...      0     0     0\n",
       " [torch.FloatTensor of size 29x50], \n",
       "     0     0     0  ...      0     1     0\n",
       "     0     0     0  ...      0     0     0\n",
       "     0     0     0  ...      1     0     0\n",
       "        ...          ⋱          ...       \n",
       "     0     0     0  ...      0     0     0\n",
       "     0     0     0  ...      1     0     0\n",
       "     0     0     1  ...      0     0     0\n",
       " [torch.FloatTensor of size 60x50], \n",
       " \n",
       " Columns 0 to 12 \n",
       "     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     1     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     1     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "     0     0     1     0     0     0     0     0     0     0     0     0     0\n",
       " \n",
       " Columns 13 to 25 \n",
       "     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     1     0     0     0     0     0\n",
       "     0     0     0     0     0     1     0     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       " \n",
       " Columns 26 to 38 \n",
       "     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     0     1     0     0     0\n",
       "     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "     1     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "     0     0     1     0     0     0     0     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       " \n",
       " Columns 39 to 49 \n",
       "     0     0     0     0     0     0     0     0     0     1     0\n",
       "     0     0     0     0     0     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     1     0     0\n",
       "     0     0     0     0     0     0     0     0     0     0     0\n",
       "     0     0     0     0     1     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     1     0     0\n",
       "     0     0     0     0     0     0     1     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     1     0     0\n",
       "     0     0     0     0     0     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     0     0     0\n",
       " [torch.FloatTensor of size 14x50], \n",
       " \n",
       " Columns 0 to 12 \n",
       "     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     0     0     1     0     0\n",
       "     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "     0     0     0     0     1     0     0     0     0     0     0     0     0\n",
       "     0     0     1     0     0     0     0     0     0     0     0     0     0\n",
       " \n",
       " Columns 13 to 25 \n",
       "     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       " \n",
       " Columns 26 to 38 \n",
       "     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     1     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       " \n",
       " Columns 39 to 49 \n",
       "     0     0     0     0     0     0     0     0     0     1     0\n",
       "     0     0     0     0     0     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     1     0     0\n",
       "     0     0     0     0     0     0     0     0     1     0     0\n",
       "     0     0     0     0     0     0     0     0     1     0     0\n",
       "     1     0     0     0     0     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     1     0     0\n",
       "     0     0     0     0     1     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     1     0     0\n",
       "     0     0     0     0     0     0     0     0     0     0     0\n",
       "     0     0     0     0     1     0     0     0     0     0     0\n",
       "     0     0     0     1     0     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     1     0     0\n",
       "     0     0     0     0     0     0     0     0     1     0     0\n",
       "     0     1     0     0     0     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     0     0     0\n",
       " [torch.FloatTensor of size 17x50], \n",
       " \n",
       " Columns 0 to 12 \n",
       "     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     1     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     1     0     0     0     0\n",
       " \n",
       " Columns 13 to 25 \n",
       "     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       " \n",
       " Columns 26 to 38 \n",
       "     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       " \n",
       " Columns 39 to 49 \n",
       "     0     0     0     0     0     0     0     0     0     1     0\n",
       "     0     0     0     0     0     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     1     0     0\n",
       "     0     0     0     0     0     0     0     0     0     0     0\n",
       " [torch.FloatTensor of size 4x50]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Encoder and Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, num_layers = 1):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            self.cuda()\n",
    "        \n",
    "    def forward(self, input, h, c):\n",
    "        output,(h,c) = self.lstm(input,(h,c))\n",
    "        return output,h,c\n",
    "        \n",
    "    def init_h0c0(self, batch_size = 1):\n",
    "        # dimension: num_layers*num_directions, batch_size, hidden_size\n",
    "        h0 = Variable(torch.zeros(self.num_layers, batch_size, self.hidden_size))\n",
    "        c0 = Variable(torch.zeros(self.num_layers, batch_size, self.hidden_size))\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            h0 = h0.cuda()\n",
    "            c0 = c0.cuda()\n",
    "        \n",
    "        return h0,c0\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_size, output_size, num_layers = 1):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers)\n",
    "        \n",
    "        # hidden-to-logit\n",
    "        self.h2l = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            self.cuda()\n",
    "        \n",
    "    def forward(self, input, h, c):\n",
    "        '''\n",
    "        Args:\n",
    "            input: 1 x (batch_size) x (output_size)\n",
    "            h: (num_layers) x (batch_size) x (output_size)\n",
    "            c: (num_layers) x (batch_size) x (output_size)\n",
    "            \n",
    "        Returns:\n",
    "            output\n",
    "        '''\n",
    "        output,(h,c) = self.lstm(input,(h,c))\n",
    "        y = self.predict(h)\n",
    "        \n",
    "        return y, h, c\n",
    "    \n",
    "    def predict(self, h):\n",
    "        '''\n",
    "        Predict the next state, given the previous hidden state\n",
    "        '''\n",
    "        # flatten the hidden state\n",
    "        h_flat = h.view(-1, h.size(2))\n",
    "        logit = self.h2l(h_flat)\n",
    "        \n",
    "        _, argmaxs = torch.max(logit, dim = 1)\n",
    "        \n",
    "        pred = Variable(torch.zeros(h_flat.size())).cuda()\n",
    "        \n",
    "        argmaxs = argmaxs.view(-1).data.cpu().numpy().tolist()\n",
    "        for i, i_max in enumerate(argmaxs):\n",
    "            pred[i, i_max] = 1\n",
    "            \n",
    "        pred = pred.view(h.size())\n",
    "        \n",
    "        return pred\n",
    "    \n",
    "    def init_pred(self, h_encoder):\n",
    "        '''\n",
    "        Args:\n",
    "            h_encoder: final hidden state from the encoder. \n",
    "            (num_layers) x (batch_size) x (hidden_size)\n",
    "        '''\n",
    "        y0 = self.predict(h_encoder)\n",
    "        \n",
    "        return y0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from custom_utils.packing import pack, unpack\n",
    "\n",
    "# Build the Training dataset\n",
    "onehots = [vocab.sent2onehot(sent) for sent in sents]\n",
    "# Build inputs / targets as lists of tensors\n",
    "inputs = [sent[:-1,:] for sent in onehots]\n",
    "targets = [sent[1:,:] for sent in onehots]\n",
    "\n",
    "def generate_batch(inputs, targets, batch_size = 100):\n",
    "    i = 0\n",
    "    for i in range(len(inputs) // batch_size):\n",
    "        yield inputs[i:i+batch_size], targets[i:i+batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we can use the hidden states and cell states from the `lstm` directly, since the `lstm` module handles `PackedSequence` automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = vocab.V\n",
    "hidden_size = vocab.V\n",
    "output_size = vocab.V\n",
    "num_layers = 1\n",
    "batch_size  = 2\n",
    "\n",
    "encoder = Encoder(input_size, hidden_size).cuda()\n",
    "decoder = Decoder(hidden_size, output_size).cuda()\n",
    "\n",
    "i, t = next(generate_batch(inputs, targets, batch_size))\n",
    "inputs_packed, orders = pack(i)\n",
    "h0, c0 = encoder.init_h0c0(batch_size)\n",
    "outputs_packed, h_e, c_e = encoder(inputs_packed, h0, c0)\n",
    "# Activate the line below, if you need to use the output \n",
    "# outputs_unpacked = unpack(outputs_packed, orders)\n",
    "y0 = decoder.init_pred(h_e)\n",
    "y, h_decoder, c_decoder = decoder(y0, h_e, c_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train the network(requires a target sequence)\n",
    "\n",
    "input_size = vocab.V\n",
    "hidden_size = vocab.V\n",
    "output_size = vocab.V\n",
    "num_layers = 1\n",
    "batch_size = 1\n",
    "\n",
    "encoder = Encoder(input_size, hidden_size).cuda()\n",
    "decoder = Decoder(hidden_size, output_size).cuda()\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "params = list(encoder.parameters()) + list(decoder.parameters())\n",
    "optimizer = optim.Adam(params, lr = .005)\n",
    "\n",
    "def run_epoch(inputs, targets):\n",
    "    # flush the gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # initial hidden state(h0)\n",
    "    h0,c0 = encoder.init_h0c0(batch_size = 6)\n",
    "    # training loss\n",
    "    loss = 0\n",
    "    \n",
    "    # Feed the training data\n",
    "    targets = [Variable(tensor).cuda() for tensor in targets]\n",
    "    inputs_packed, orders = pack(inputs)\n",
    "    \n",
    "    # Run a RNN encoder-decoder through the training samples  \n",
    "    _, h_encoder, c_encoder = encoder(inputs_packed, h0, c0)\n",
    "    y0 = decoder.init_pred(h_encoder)\n",
    "    y, h_decoder, c_decoder = decoder(y0, h_encoder, c_encoder)\n",
    "    \n",
    "    for out, target in zip(outputs, targets):\n",
    "        loss += loss_fn(out, target)\n",
    "        \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return outputs, loss.data[0]\n",
    "\n",
    "def train(inputs, targets, n_epochs = 100, print_every = 10):\n",
    "    total_loss = 0.0\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        output, loss = run_epoch(inputs, targets)\n",
    "        if epoch % print_every == 0:\n",
    "            print('Epoch: %2i / Loss: %.7f' % (epoch, loss))\n",
    "            \n",
    "def test(input_sent):\n",
    "    h, c = rnn.init_h0c0()\n",
    "    seq_len = input_sent.size()[0]\n",
    "    input_sent = Variable(input_sent.view(seq_len, batch_size, -1))\n",
    "    \n",
    "    output, h, c = rnn(input_sent, h, c)\n",
    "    _, argmaxs = torch.max(output, dim = 0)\n",
    "    \n",
    "    # flatten the sorted indices\n",
    "    sent = argmaxs.view(-1).data.cpu().numpy().tolist()\n",
    "    for i in sent:\n",
    "        print(vocab[i],end=' ')\n",
    "        \n",
    "# run_epoch(inputs, targets)\n",
    "train(inputs, targets, n_epochs = 1000, print_every = 100)\n",
    "torch.manual_seed(7)\n",
    "test(inputs[0].cuda())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
