{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "1. Word Vectors\n",
    "2. Word-Document Matrix\n",
    "3. Window based Co-occurence Matrix\n",
    "4. Matrix Decompostion using SVD\n",
    "5. Clustering the points using spherical k-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Representing Words as One-hot vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original document of the `wiki.dl.history.txt` is [this]('wiki.dl.history.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to a survey,[4] the expression Deep Learning was introduced to the Machine Learning community by Rina Dechter in 1986,[22] and later to Artificial Neural Networks by Igor Aizenberg and colleagues in 2000, in the context of Boolean threshold neurons.[23] In 2005, Faustino Gomez and Jürgen Schmidhuber published a paper on learning deep POMDPs[24] through neural networks for reinforcement learning. In 2006, a publication by Hinton, Osindero, and Teh[25][26] drew attention by showing how many-layered feedforward neural network could be effectively pre-trained one layer at a time, treating each layer in turn as an unsupervised restricted Boltzmann machine, then fine-tuning it using supervised backpropagation.[27] The paper referred to learning for deep belief nets. A Google Ngram chart shows that the usage of the term has taken off since 2000.[28] The underlying concepts and many techniques, however, date to earlier decades.\n",
      "\n",
      "The first general, working learning algorithm for super\n"
     ]
    }
   ],
   "source": [
    "# open the text file to process\n",
    "with open('wiki.dl.history.txt') as f:\n",
    "    corpus_raw = f.read()\n",
    "print(corpus_raw[:1000])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, remove the special patterns like `[Number]`(e.g. `[4]`) so that we can extract only the pure words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to a survey, the expression Deep Learning was introduced to the Machine Learning community by Rina Dechter in 1986, and later to Artificial Neural Networks by Igor Aizenberg and colleagues in 2000, in the context of Boolean threshold neurons. In 2005, Faustino Gomez and Jürgen Schmidhuber published a paper on learning deep POMDPs through neural networks for reinforcement learning. In 2006, a publication by Hinton, Osindero, and Teh drew attention by showing how many-layered feedforward neural network could be effectively pre-trained one layer at a time, treating each layer in turn as an unsupervised restricted Boltzmann machine, then fine-tuning it using supervised backpropagation. The paper referred to learning for deep belief nets. A Google Ngram chart shows that the usage of the term has taken off since 2000. The underlying concepts and many techniques, however, date to earlier decades.\n",
      "\n",
      "The first general, working learning algorithm for supervised, deep, feedforward, multi\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "corpus = re.sub('\\[[0-9]*\\]','',corpus_raw)\n",
    "print(corpus[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['according',\n",
       " 'to',\n",
       " 'a',\n",
       " 'survey,',\n",
       " 'the',\n",
       " 'expression',\n",
       " 'deep',\n",
       " 'learning',\n",
       " 'was',\n",
       " 'introduced']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# separate the corpus into words\n",
    "corpus_list = corpus.lower().split()\n",
    "corpus_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of words in the vocabulary: 654\n"
     ]
    }
   ],
   "source": [
    "# Build the Vocabulary\n",
    "vocabulary = list({word for word in corpus_list})\n",
    "n_words = len(vocabulary)\n",
    "print('The number of words in the vocabulary: %i' % n_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build the one-hot vectors for the vocabulary\n",
    "one_hot = dict()\n",
    "for i,word in enumerate(vocabulary):\n",
    "    one_hot[word] = [1 if i==j else 0 for j in range(n_words)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "# One-hot representations of a document\n",
    "corpus2id = [one_hot[word] for word in corpus_list]\n",
    "print(corpus2id[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Word-Document Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('deep', 27), ('neural', 23), ('speech', 18), ('learning', 15), ('recognition', 12), ('networks', 10), ('network', 8), ('published', 6), ('used', 6), ('learning.', 5), ('using', 5), ('training', 5), ('generative', 5), ('recognition.', 5), ('layer', 4), ('unsupervised', 4), ('google', 4), ('many', 4), ('layers', 4), ('computer', 4), ('systems', 4), ('3-d', 4), ('object', 4), ('model', 4), ('cresceptron', 4), ('models', 4), ('speaker', 4), ('machine', 3), ('later', 3), ('colleagues', 3), ('schmidhuber', 3), ('paper', 3), ('feedforward', 3), ('could', 3), ('one', 3), ('supervised', 3), ('however,', 3), ('first', 3), ('algorithm', 3), ('method', 3), ('data', 3), ('called', 3), ('demonstrated', 3), ('et', 3), ('al.', 3), ('recognizing', 3), ('days.', 3), ('brain', 3), ('pre-training', 3), ('recurrent', 3)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "counts = Counter(corpus_list).most_common()\n",
    "\n",
    "# Remove stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stops = set(stopwords.words('english'))\n",
    "counts = [(word, count) for (word,count) in counts if word not in stops]\n",
    "\n",
    "# Choose only top-V words from the document as a Vocabulary\n",
    "V = 50\n",
    "counts = counts[:V]\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'deep': 0, 'neural': 1, 'speech': 2, 'learning': 3, 'recognition': 4, 'networks': 5, 'network': 6, 'published': 7, 'used': 8, 'learning.': 9, 'using': 10, 'training': 11, 'generative': 12, 'recognition.': 13, 'layer': 14, 'unsupervised': 15, 'google': 16, 'many': 17, 'layers': 18, 'computer': 19, 'systems': 20, '3-d': 21, 'object': 22, 'model': 23, 'cresceptron': 24, 'models': 25, 'speaker': 26, 'machine': 27, 'later': 28, 'colleagues': 29, 'schmidhuber': 30, 'paper': 31, 'feedforward': 32, 'could': 33, 'one': 34, 'supervised': 35, 'however,': 36, 'first': 37, 'algorithm': 38, 'method': 39, 'data': 40, 'called': 41, 'demonstrated': 42, 'et': 43, 'al.': 44, 'recognizing': 45, 'days.': 46, 'brain': 47, 'pre-training': 48, 'recurrent': 49}\n"
     ]
    }
   ],
   "source": [
    "# Word-ID Mapping\n",
    "word2id = {tup[0]: i for i,tup in enumerate(counts)}\n",
    "id2word = {i: tup[0] for i,tup in enumerate(counts)}\n",
    "print(word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[27]\n",
      " [23]\n",
      " [18]\n",
      " [15]\n",
      " [12]\n",
      " [10]\n",
      " [ 8]\n",
      " [ 6]\n",
      " [ 6]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [ 4]\n",
      " [ 4]\n",
      " [ 4]\n",
      " [ 4]\n",
      " [ 4]\n",
      " [ 4]\n",
      " [ 4]\n",
      " [ 4]\n",
      " [ 4]\n",
      " [ 4]\n",
      " [ 4]\n",
      " [ 4]\n",
      " [ 4]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]]\n"
     ]
    }
   ],
   "source": [
    "# Obtain the word-count vector representation of a document\n",
    "import numpy as np\n",
    "doc1 = np.zeros((V,1), dtype=int)\n",
    "for i, tup in enumerate(counts):\n",
    "    count = tup[1]\n",
    "    doc1[i,:] = count\n",
    "print(doc1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the origial document for `wiki.nlp.history` is [here](https://en.wikipedia.org/wiki/Natural_language_processing#History)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]\n",
      " [0]\n",
      " [1]\n",
      " [5]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [2]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [6]\n",
      " [0]\n",
      " [0]\n",
      " [8]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [4]\n",
      " [0]\n",
      " [7]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [5]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [2]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "with open('wiki.nlp.history.txt') as f:\n",
    "    corpus_ = re.sub('\\[[0-9]*\\]','',f.read())\n",
    "corpus_nlp = corpus_.lower().split()\n",
    "doc2 = np.zeros((V,1), int)\n",
    "for word in corpus_nlp:\n",
    "    if word in word2id:\n",
    "        doc2[word2id[word],:] += 1\n",
    "print(doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[27,  1],\n",
       "       [23,  0],\n",
       "       [18,  1],\n",
       "       [15,  5],\n",
       "       [12,  1],\n",
       "       [10,  0],\n",
       "       [ 8,  0],\n",
       "       [ 6,  1],\n",
       "       [ 6,  0],\n",
       "       [ 5,  0],\n",
       "       [ 5,  2],\n",
       "       [ 5,  0],\n",
       "       [ 5,  0],\n",
       "       [ 5,  0],\n",
       "       [ 4,  0],\n",
       "       [ 4,  1],\n",
       "       [ 4,  0],\n",
       "       [ 4,  6],\n",
       "       [ 4,  0],\n",
       "       [ 4,  0],\n",
       "       [ 4,  8],\n",
       "       [ 4,  0],\n",
       "       [ 4,  0],\n",
       "       [ 4,  0],\n",
       "       [ 4,  0],\n",
       "       [ 4,  4],\n",
       "       [ 4,  0],\n",
       "       [ 3,  7],\n",
       "       [ 3,  0],\n",
       "       [ 3,  0],\n",
       "       [ 3,  0],\n",
       "       [ 3,  0],\n",
       "       [ 3,  0],\n",
       "       [ 3,  0],\n",
       "       [ 3,  0],\n",
       "       [ 3,  1],\n",
       "       [ 3,  5],\n",
       "       [ 3,  1],\n",
       "       [ 3,  0],\n",
       "       [ 3,  0],\n",
       "       [ 3,  2],\n",
       "       [ 3,  1],\n",
       "       [ 3,  0],\n",
       "       [ 3,  0],\n",
       "       [ 3,  0],\n",
       "       [ 3,  0],\n",
       "       [ 3,  0],\n",
       "       [ 3,  0],\n",
       "       [ 3,  0],\n",
       "       [ 3,  0]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.hstack((doc1,doc2))\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Window based Co-occurence Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define helper functions\n",
    "\n",
    "def window_generator(corpus_list, window_size):\n",
    "    '''Generates the window of window_size using the words in the corpus.\n",
    "    \n",
    "    Args:\n",
    "        corpus_list: list of word strings in the corpus. e.g. ['hello','my','name','is']\n",
    "        window_size: an integer that indicates the size of the window\n",
    "        \n",
    "    Returns:\n",
    "        window: list of word strings, which is a subset of the corpus.\n",
    "    '''\n",
    "    corpus_len = len(corpus_list)\n",
    "    for i in range(corpus_len):\n",
    "        # left end(inclusive)\n",
    "        left_end = max(0, i - window_size)\n",
    "        # right end(inclusive)\n",
    "        right_end = min(i + window_size, corpus_len - 1)\n",
    "        window = corpus_list[left_end:right_end+1]\n",
    "        yield window\n",
    "        \n",
    "def count_cooc(window):\n",
    "    '''Count the co-occurences of words within the specified window, and updates the affinity matrix\n",
    "    \n",
    "    Args:\n",
    "        window: list of word strings, which is a subset of the corpus. e.g. ['hello','my','name','is']\n",
    "        \n",
    "    Returns:\n",
    "        None\n",
    "    '''\n",
    "    queue = [word for word in window if word in word2id]\n",
    "    while len(queue) >= 2:\n",
    "        this_word = queue[0] # current word to process\n",
    "        del queue[0]\n",
    "        for other_word in queue:\n",
    "            # base case: same word is repeated\n",
    "            if this_word == other_word:\n",
    "                continue\n",
    "            # usual case: update the co-occurence matrix\n",
    "            X[word2id[this_word], word2id[other_word]] += 1\n",
    "            X[word2id[other_word], word2id[this_word]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neural deep\n",
      "learning deep\n",
      "learning speech\n",
      "recognition speech\n",
      "networks deep\n",
      "networks neural\n",
      "networks speech\n",
      "network deep\n",
      "network neural\n",
      "published neural\n",
      "published network\n",
      "learning. deep\n",
      "generative deep\n",
      "generative speech\n",
      "generative using\n",
      "recognition. speech\n",
      "unsupervised used\n",
      "google used\n",
      "many speech\n",
      "many recognition.\n",
      "systems used\n",
      "3-d recognition\n",
      "object recognition\n",
      "object 3-d\n",
      "model 3-d\n",
      "model object\n",
      "cresceptron used\n",
      "models deep\n",
      "models speech\n",
      "models using\n",
      "models generative\n",
      "speaker speech\n",
      "speaker recognition\n",
      "speaker networks\n",
      "speaker recognition.\n",
      "machine learning\n",
      "machine learning.\n",
      "later published\n",
      "schmidhuber published\n",
      "schmidhuber used\n",
      "schmidhuber unsupervised\n",
      "paper learning\n",
      "paper published\n",
      "feedforward deep\n",
      "feedforward neural\n",
      "feedforward networks\n",
      "feedforward network\n",
      "could neural\n",
      "could network\n",
      "one speech\n",
      "one recognition.\n",
      "one layer\n",
      "supervised deep\n",
      "supervised learning.\n",
      "supervised using\n",
      "however, neural\n",
      "however, training\n",
      "however, many\n",
      "first used\n",
      "algorithm learning\n",
      "method deep\n",
      "method learning\n",
      "data training\n",
      "data method\n",
      "called learning\n",
      "called method\n",
      "demonstrated learning\n",
      "et however,\n",
      "al. et\n",
      "recognizing used\n",
      "recognizing 3-d\n",
      "days. many\n",
      "brain used\n",
      "brain google\n",
      "pre-training deep\n",
      "pre-training used\n",
      "pre-training using\n",
      "pre-training unsupervised\n",
      "recurrent neural\n",
      "recurrent learning\n",
      "recurrent network\n"
     ]
    }
   ],
   "source": [
    "X = np.zeros((V,V), dtype=int) # affinity matrix\n",
    "window_size = 1\n",
    "# Generate the windows and count the co-occurences\n",
    "for window in window_generator(corpus_list, window_size):\n",
    "    count_cooc(window)\n",
    "    \n",
    "# Check which words co-occurred\n",
    "it = np.nditer(X, flags=['multi_index'])\n",
    "while not it.finished:\n",
    "    if it[0] != 0:\n",
    "        i,j = it.multi_index\n",
    "        if i > j:\n",
    "            print(id2word[i],id2word[j])\n",
    "    it.iternext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Matrix Decomposition using SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 50)\n",
      "(50, 50) (50, 50) (50, 50)\n",
      "Preserved variance: 1.0000\n"
     ]
    }
   ],
   "source": [
    "from numpy.linalg import svd\n",
    "print(X.shape)\n",
    "\n",
    "# Default SVD\n",
    "U, s, V_T = svd(X)\n",
    "S = np.diag(s)\n",
    "V = V_T.T\n",
    "print(U.shape,S.shape,V.shape)\n",
    "print(\"Preserved variance: %.4f\" % 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Low-rank approximation using the Truncated SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 10) (10, 10) (50, 10)\n",
      "Preserved variance: 0.9136\n"
     ]
    }
   ],
   "source": [
    "k = 10\n",
    "U_k = U[:,:k]\n",
    "s_k = s[:k]\n",
    "S_k = np.diag(s_k)\n",
    "V_k = V[:,:k]\n",
    "print(U_k.shape,S_k.shape,V_k.shape)\n",
    "print(\"Preserved variance: %.4f\" % ((s_k**2).sum()/(s**2).sum()))\n",
    "X_k_np = np.dot(np.dot(U_k,S_k),V_k.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  7.79459557e-03,  -4.60781844e-03,   3.85495896e-03, ...,\n",
       "         -1.21302405e-03,   1.56091796e-02,  -7.82903066e-03],\n",
       "       [ -4.60781844e-03,   1.05034395e-02,  -6.52047100e-03, ...,\n",
       "         -9.38019056e-04,  -4.01760331e-03,   3.22364157e-02],\n",
       "       [  3.85495896e-03,  -6.52047100e-03,  -3.07212427e-02, ...,\n",
       "          9.35489148e-03,   7.02801345e-05,   4.89849804e-03],\n",
       "       ..., \n",
       "       [ -1.21302405e-03,  -9.38019056e-04,   9.35489148e-03, ...,\n",
       "         -2.49822036e-03,  -8.43468738e-03,   1.31145968e-04],\n",
       "       [  1.56091796e-02,  -4.01760331e-03,   7.02801345e-05, ...,\n",
       "         -8.43468738e-03,  -2.26320948e-02,  -3.85254850e-02],\n",
       "       [ -7.82903066e-03,   3.22364157e-02,   4.89849804e-03, ...,\n",
       "          1.31145968e-04,  -3.85254850e-02,  -4.27370242e-02]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X - X_k_np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since 91% of the variance is captured by the latent representation, the errors are often very small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Clustering using spherical k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "V_k_normalized = V_k.copy()\n",
    "for row in range(V_k_normalized.shape[0]):\n",
    "    v_k = V_k_normalized[row]\n",
    "    norm = np.sqrt(np.dot(v_k,v_k))\n",
    "    if norm == 0:\n",
    "        continue\n",
    "    V_k_normalized[row] = v_k/norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['recognition', 'used', 'using', 'training', 'generative', 'recognition.', 'layer', 'unsupervised', 'google', 'many', 'systems', '3-d', 'model', 'cresceptron', 'models', 'later', 'schmidhuber', 'one', 'supervised', 'first', 'data', 'et', 'al.', 'recognizing', 'days.', 'brain', 'pre-training']\n",
      "['deep', 'machine', 'paper', 'algorithm', 'method', 'called', 'demonstrated']\n",
      "['networks', 'network', 'published', 'feedforward', 'could', 'however,', 'recurrent']\n",
      "['speech', 'object', 'speaker']\n",
      "['neural', 'learning', 'learning.', 'layers', 'computer', 'colleagues']\n"
     ]
    }
   ],
   "source": [
    "from spherecluster import SphericalKMeans\n",
    "\n",
    "# Run spherical K-means\n",
    "K = 5\n",
    "skm = SphericalKMeans(n_clusters=K, n_jobs=-1, random_state=0).fit(X)\n",
    "\n",
    "# Print out the cluster members\n",
    "for cluster in range(K):\n",
    "    members = [id2word[i] for i,label in enumerate(skm.labels_) if label==cluster]\n",
    "    print(members)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that some of the clusters in the 10-dimensional Latent Space are of good quality, even if the size of our data is very small."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
