{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook is essentially a modification of:\n",
    "1. [Recurrent Neural Networks in Tensorflow 2](https://r2rt.com/recurrent-neural-networks-in-tensorflow-ii.html)\n",
    "2. [Recurrent Neural Networks in Tensorflow 3](https://r2rt.com/recurrent-neural-networks-in-tensorflow-iii-variable-length-sequences.html)\n",
    "\n",
    "[Theories behind RNN & LSTM](https://r2rt.com/written-memories-understanding-deriving-and-extending-the-lstm.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "1. Loading data\n",
    "2. Using the `BasicLSTMCell`\n",
    "3. Stacking the LSTM cells\n",
    "4. Adding Dropout\n",
    "5. Using `GRUCell`\n",
    "6. Generating words\n",
    "7. Layer Normalization(skipped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data and preprocess\n",
    "* `wiki.dl.txt` represents the content of [the wikipedia article for Deep Learning](https://en.wikipedia.org/wiki/Deep_learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deep learning (also known as deep structured learning or hierarchical learning) is part of a broader family of machine learning methods based on learning data representations, as opposed to task-specific algorithms. learning can be supervised, partially supervised or unsupervised.\n",
      "\n",
      "some representations are loosely based on interpretation of information processing and communication patterns in a biological nervous system, such as neural coding that attempts to define a relationship between various stimuli and associated neuronal responses in the brain. research attempts to create efficient systems to learn these representations from large-scale, unlabeled data sets.\n",
      "\n",
      "deep learning architectures such as deep neural networks, deep belief networks and recurrent neural networks have been applied to fields including computer vision, speech recognition, natural language processing, audio recognition, social network filtering, machine translation and bioinformatics where they produced results \n",
      "[['<<', 'deep', 'learning', '(', 'also', 'known', 'as', 'deep', 'structured', 'learning', 'or', 'hierarchical', 'learning', ')', 'is', 'part', 'of', 'a', 'broader', 'family', 'of', 'machine', 'learning', 'methods', 'based', 'on', 'learning', 'data', 'representations', ',', 'as', 'opposed', 'to', 'task-specific', 'algorithms', '.', '>>'], ['<<', 'learning', 'can', 'be', 'supervised', ',', 'partially', 'supervised', 'or', 'unsupervised', '.', '>>'], ['<<', 'some', 'representations', 'are', 'loosely', 'based', 'on', 'interpretation', 'of', 'information', 'processing', 'and', 'communication', 'patterns', 'in', 'a', 'biological', 'nervous', 'system', ',', 'such', 'as', 'neural', 'coding', 'that', 'attempts', 'to', 'define', 'a', 'relationship', 'between', 'various', 'stimuli', 'and', 'associated', 'neuronal', 'responses', 'in', 'the', 'brain', '.', '>>']]\n",
      "[[1674, 9, 10, 11, 57, 396, 15, 9, 676, 10, 43, 276, 10, 12, 23, 212, 4, 7, 397, 677, 4, 38, 10, 89, 75, 21, 10, 27, 76, 1, 15, 678, 8, 277, 124, 2, 1675], [1674, 10, 41, 35, 84, 1, 679, 84, 43, 77, 2, 1675], [1674, 167, 76, 24, 680, 75, 21, 125, 4, 112, 45, 6, 398, 681, 5, 7, 213, 682, 90, 1, 31, 15, 16, 683, 17, 399, 8, 400, 7, 278, 102, 214, 684, 6, 401, 685, 686, 5, 3, 58, 2, 1675]]\n"
     ]
    }
   ],
   "source": [
    "# Open and preprocess\n",
    "import re\n",
    "with open('wiki.dl.txt') as f:\n",
    "    # remove special characters\n",
    "    corpus = re.sub('\\[[0-9]*\\]','',f.read()) \n",
    "    # decapitalize\n",
    "    corpus = corpus.lower()\n",
    "    print(corpus[:1000])\n",
    "\n",
    "# tokenize until word units\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "sentences = sent_tokenize(corpus)\n",
    "sents_tokenized = [word_tokenize(sent) for sent in sentences]\n",
    "\n",
    "# Build a vocabulary\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "count = Counter(chain(*sents_tokenized))\n",
    "vocab_size = 5000\n",
    "count = count.most_common(vocab_size - 3)\n",
    "# WARNING: indices start from 1(for padding)\n",
    "vocab = {tup[0]:i + 1 for i,tup in enumerate(count)}\n",
    "START = '<<'\n",
    "END = '>>'\n",
    "UNKNOWN = '_UNK_'\n",
    "vocab[START] = len(vocab) + 1\n",
    "vocab[END] = len(vocab) + 1\n",
    "vocab[UNKNOWN] = len(vocab) + 1\n",
    "vocab_size = min(vocab_size, len(vocab))\n",
    "\n",
    "rev_vocab = dict([(v,k) for k,v in vocab.items()])\n",
    "\n",
    "# Filter words in the training sentences\n",
    "for sent in sents_tokenized:\n",
    "    sent.insert(0, START)\n",
    "    sent.append(END)\n",
    "    for i in range(len(sent)):\n",
    "        word = sent[i]\n",
    "        if word not in vocab:\n",
    "            sent[i] = UNKNOWN\n",
    "\n",
    "# Convert the words into indices            \n",
    "sents2id = []\n",
    "for sent in sents_tokenized:\n",
    "    sents2id.append([])\n",
    "    for word in sent:\n",
    "        sents2id[-1].append(vocab[word])\n",
    "\n",
    "print(sents_tokenized[:3])\n",
    "print(sents2id[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training epochs generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow version:1.3.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print('tensorflow version:%s' % tf.__version__)\n",
    "import numpy as np\n",
    "\n",
    "# Training batch generator\n",
    "def gen_samples(num_steps,batch_size):\n",
    "    '''\n",
    "    Generate a training example(a sentence) over a single epoch,\n",
    "    using the sentences data\n",
    "    \n",
    "    Args:\n",
    "        num_steps: the size of the words-window to consider\n",
    "        batch_size: size of each batches\n",
    "        \n",
    "    Yields:\n",
    "        batch: list of words. The number of examples in the batch is batch_size,\n",
    "        and the length of each examples is seq_length.\n",
    "    '''\n",
    "    x_batch = []\n",
    "    y_batch = []\n",
    "    for sent in sents2id:\n",
    "        sent_length = len(sent)\n",
    "        # Too short sentences will be ignored\n",
    "        if sent_length < num_steps + 1:\n",
    "            break\n",
    "        for i in range(sent_length-num_steps-1):\n",
    "            x_batch.append(sent[i:i+num_steps])\n",
    "            y_batch.append(sent[i+1:i+num_steps+1])\n",
    "            if len(x_batch) == batch_size:\n",
    "                yield x_batch,y_batch\n",
    "                x_batch = []\n",
    "                y_batch = []\n",
    "                \n",
    "def gen_epochs(n,num_steps,batch_size):\n",
    "    '''\n",
    "    Generate a whole training epoch\n",
    "    \n",
    "    Args:\n",
    "        n: the number of training epochs\n",
    "        num_steps: the size of the words-window to consider\n",
    "        batch_size: size of each batches\n",
    "    Yields:\n",
    "        a training set generated with num_steps, batch_size over \n",
    "        the whole training corpus\n",
    "    '''\n",
    "    for n in range(n):\n",
    "        yield gen_samples(num_steps,batch_size)\n",
    "        \n",
    "def train_network(graph, n_epochs, batch_size, num_steps,\n",
    "                  verbose=True, save=False):\n",
    "    tf.set_random_seed(123)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        # list that stores losses for each training epochs\n",
    "        training_losses = []\n",
    "        for i_epoch,epoch in enumerate(\n",
    "            gen_epochs(n_epochs, num_steps, batch_size)):\n",
    "            total_loss = 0\n",
    "            n_batches = 0\n",
    "            state = None\n",
    "            for X, Y in epoch:\n",
    "                n_batches += 1\n",
    "                feed_dict = {graph['x']:X, graph['y']:Y}\n",
    "                batch_loss, batch_state, _ =\\\n",
    "                sess.run([graph['loss'],\n",
    "                          graph['final_state'],\n",
    "                          graph['optimizer']],\n",
    "                         feed_dict)\n",
    "                total_loss += batch_loss\n",
    "\n",
    "            avg_epoch_loss = total_loss/n_batches\n",
    "            training_losses.append(avg_epoch_loss)\n",
    "\n",
    "            if verbose:\n",
    "                print('Average loss in Epoch %i:%.4f' %\n",
    "                      (i_epoch,avg_epoch_loss))\n",
    "\n",
    "        # Store the training result\n",
    "        if isinstance(save, str):\n",
    "            graph['saver'].save(sess, save)\n",
    "\n",
    "    return training_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Basic LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we are using distributed representation for each words(i.e. word2vec). To understand what `tf.nn.embedding_lookup` is doing, refer to:\n",
    "1. See the 'Inputs' section of [TF documentation for RNN](https://www.tensorflow.org/tutorials/recurrent) to understand what the function does\n",
    "2. And look [here](http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/) for the rationale. Take a look at the section **Adding an embedding layer**\n",
    "3. If you don't want to use word2vec, [this](https://stackoverflow.com/questions/35056909/input-to-lstm-network-tensorflow) might be helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "original source: https://r2rt.com/recurrent-neural-networks-in-tensorflow-ii.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss in Epoch 0:7.4219\n",
      "Average loss in Epoch 1:7.4095\n",
      "Average loss in Epoch 2:7.3648\n",
      "Average loss in Epoch 3:6.8196\n",
      "Average loss in Epoch 4:6.1925\n",
      "Average loss in Epoch 5:5.9669\n",
      "Average loss in Epoch 6:5.8511\n",
      "Average loss in Epoch 7:5.7761\n",
      "Average loss in Epoch 8:5.7209\n",
      "Average loss in Epoch 9:5.6772\n",
      "Average loss in Epoch 10:5.6410\n",
      "Average loss in Epoch 11:5.6102\n",
      "Average loss in Epoch 12:5.5832\n",
      "Average loss in Epoch 13:5.5592\n",
      "Average loss in Epoch 14:5.5375\n",
      "Average loss in Epoch 15:5.5176\n",
      "Average loss in Epoch 16:5.4992\n",
      "Average loss in Epoch 17:5.4818\n",
      "Average loss in Epoch 18:5.4654\n",
      "Average loss in Epoch 19:5.4497\n",
      "Average loss in Epoch 20:5.4344\n",
      "Average loss in Epoch 21:5.4197\n",
      "Average loss in Epoch 22:5.4051\n",
      "Average loss in Epoch 23:5.3907\n",
      "Average loss in Epoch 24:5.3766\n",
      "Average loss in Epoch 25:5.3626\n",
      "Average loss in Epoch 26:5.3487\n",
      "Average loss in Epoch 27:5.3349\n",
      "Average loss in Epoch 28:5.3213\n",
      "Average loss in Epoch 29:5.3079\n",
      "Average loss in Epoch 30:5.2947\n",
      "Average loss in Epoch 31:5.2818\n",
      "Average loss in Epoch 32:5.2690\n",
      "Average loss in Epoch 33:5.2556\n",
      "Average loss in Epoch 34:5.2418\n",
      "Average loss in Epoch 35:5.2278\n",
      "Average loss in Epoch 36:5.2136\n",
      "Average loss in Epoch 37:5.1991\n",
      "Average loss in Epoch 38:5.1841\n",
      "Average loss in Epoch 39:5.1685\n",
      "Average loss in Epoch 40:5.1522\n",
      "Average loss in Epoch 41:5.1355\n",
      "Average loss in Epoch 42:5.1184\n",
      "Average loss in Epoch 43:5.1010\n",
      "Average loss in Epoch 44:5.0832\n",
      "Average loss in Epoch 45:5.0650\n",
      "Average loss in Epoch 46:5.0467\n",
      "Average loss in Epoch 47:5.0281\n",
      "Average loss in Epoch 48:5.0092\n",
      "Average loss in Epoch 49:4.9900\n",
      "Average loss in Epoch 50:4.9704\n",
      "Average loss in Epoch 51:4.9505\n",
      "Average loss in Epoch 52:4.9301\n",
      "Average loss in Epoch 53:4.9092\n",
      "Average loss in Epoch 54:4.8879\n",
      "Average loss in Epoch 55:4.8663\n",
      "Average loss in Epoch 56:4.8442\n",
      "Average loss in Epoch 57:4.8217\n",
      "Average loss in Epoch 58:4.7989\n",
      "Average loss in Epoch 59:4.7757\n",
      "Average loss in Epoch 60:4.7522\n",
      "Average loss in Epoch 61:4.7283\n",
      "Average loss in Epoch 62:4.7041\n",
      "Average loss in Epoch 63:4.6794\n",
      "Average loss in Epoch 64:4.6544\n",
      "Average loss in Epoch 65:4.6291\n",
      "Average loss in Epoch 66:4.6034\n",
      "Average loss in Epoch 67:4.5773\n",
      "Average loss in Epoch 68:4.5509\n",
      "Average loss in Epoch 69:4.5242\n",
      "Average loss in Epoch 70:4.4972\n",
      "Average loss in Epoch 71:4.4699\n",
      "Average loss in Epoch 72:4.4423\n",
      "Average loss in Epoch 73:4.4144\n",
      "Average loss in Epoch 74:4.3862\n",
      "Average loss in Epoch 75:4.3577\n",
      "Average loss in Epoch 76:4.3289\n",
      "Average loss in Epoch 77:4.2997\n",
      "Average loss in Epoch 78:4.2703\n",
      "Average loss in Epoch 79:4.2406\n",
      "Average loss in Epoch 80:4.2107\n",
      "Average loss in Epoch 81:4.1806\n",
      "Average loss in Epoch 82:4.1503\n",
      "Average loss in Epoch 83:4.1198\n",
      "Average loss in Epoch 84:4.0893\n",
      "Average loss in Epoch 85:4.0586\n",
      "Average loss in Epoch 86:4.0278\n",
      "Average loss in Epoch 87:3.9969\n",
      "Average loss in Epoch 88:3.9659\n",
      "Average loss in Epoch 89:3.9349\n",
      "Average loss in Epoch 90:3.9037\n",
      "Average loss in Epoch 91:3.8724\n",
      "Average loss in Epoch 92:3.8411\n",
      "Average loss in Epoch 93:3.8096\n",
      "Average loss in Epoch 94:3.7781\n",
      "Average loss in Epoch 95:3.7466\n",
      "Average loss in Epoch 96:3.7150\n",
      "Average loss in Epoch 97:3.6833\n",
      "Average loss in Epoch 98:3.6515\n",
      "Average loss in Epoch 99:3.6196\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[7.4219453811645506,\n",
       " 7.4095009613037108,\n",
       " 7.3647918891906734,\n",
       " 6.8195610618591305,\n",
       " 6.1925234031677245,\n",
       " 5.9669471549987794,\n",
       " 5.8511255264282225,\n",
       " 5.7760960006713864,\n",
       " 5.7208665847778324,\n",
       " 5.6771656990051271,\n",
       " 5.6410123252868649,\n",
       " 5.6101623725891114,\n",
       " 5.5832146263122562,\n",
       " 5.5592173767089843,\n",
       " 5.5375051689147945,\n",
       " 5.5176248741149898,\n",
       " 5.4991607093811039,\n",
       " 5.481809883117676,\n",
       " 5.4654388236999516,\n",
       " 5.4497043609619142,\n",
       " 5.4344337272644045,\n",
       " 5.4196513366699222,\n",
       " 5.4050824928283694,\n",
       " 5.3907295799255373,\n",
       " 5.3765571784973147,\n",
       " 5.3625544929504398,\n",
       " 5.3486833381652836,\n",
       " 5.3349454689025881,\n",
       " 5.3213220024108887,\n",
       " 5.3078727912902828,\n",
       " 5.2946855926513674,\n",
       " 5.281846790313721,\n",
       " 5.2690480613708495,\n",
       " 5.2556065177917484,\n",
       " 5.2418182945251468,\n",
       " 5.2277877426147459,\n",
       " 5.2135727500915525,\n",
       " 5.1990779113769534,\n",
       " 5.1841147422790526,\n",
       " 5.1684820365905759,\n",
       " 5.1521724700927738,\n",
       " 5.1354609298706055,\n",
       " 5.1184075355529783,\n",
       " 5.1009625816345219,\n",
       " 5.0831543350219723,\n",
       " 5.0650280380249022,\n",
       " 5.046668281555176,\n",
       " 5.0280907058715822,\n",
       " 5.0092020988464352,\n",
       " 4.9900041007995606,\n",
       " 4.9704459381103518,\n",
       " 4.9504846191406253,\n",
       " 4.9300615501403806,\n",
       " 4.9092165946960451,\n",
       " 4.8879289054870609,\n",
       " 4.8662648391723629,\n",
       " 4.8441745567321775,\n",
       " 4.8217229652404789,\n",
       " 4.7989083290100094,\n",
       " 4.7757361602783206,\n",
       " 4.7521930313110348,\n",
       " 4.7283411979675289,\n",
       " 4.7040525627136232,\n",
       " 4.6794060516357421,\n",
       " 4.6544162559509275,\n",
       " 4.6290861892700192,\n",
       " 4.6033541679382326,\n",
       " 4.577307052612305,\n",
       " 4.5509277915954591,\n",
       " 4.5241895484924317,\n",
       " 4.4971902084350583,\n",
       " 4.469885425567627,\n",
       " 4.4422853183746334,\n",
       " 4.4143918609619144,\n",
       " 4.3861970424652101,\n",
       " 4.3576902389526371,\n",
       " 4.3288686466217037,\n",
       " 4.2997276401519775,\n",
       " 4.2702940464019772,\n",
       " 4.2406014060974124,\n",
       " 4.2106789493560788,\n",
       " 4.1805540847778317,\n",
       " 4.1502594375610355,\n",
       " 4.1198213577270506,\n",
       " 4.0892637538909913,\n",
       " 4.0585919475555423,\n",
       " 4.027813320159912,\n",
       " 3.9969284629821775,\n",
       " 3.9659425544738771,\n",
       " 3.9348651218414306,\n",
       " 3.903695936203003,\n",
       " 3.8724280643463134,\n",
       " 3.8410642814636229,\n",
       " 3.8096281719207763,\n",
       " 3.7781336021423342,\n",
       " 3.7465874099731447,\n",
       " 3.7149712467193603,\n",
       " 3.6832862949371337,\n",
       " 3.6514922332763673,\n",
       " 3.6195577239990233]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.contrib import rnn\n",
    "\n",
    "def build_graph(batch_size, num_steps, state_size,\n",
    "                num_classes = vocab_size, learning_rate=1e-4):\n",
    "    \n",
    "    # wipe out all previously built graphs\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # word ids, without one-hot encoding\n",
    "    x = tf.placeholder(tf.int32, [batch_size, num_steps], name='x')\n",
    "    y = tf.placeholder(tf.int32, [batch_size, num_steps], name='y')\n",
    "\n",
    "    # vector representation for each words(word2vec)\n",
    "    word_embeddings = tf.get_variable('embedding_matrix',\n",
    "                                 [num_classes, state_size])\n",
    "\n",
    "    # rnn_inputs is a tensor of dim [batch_size,num_steps,state_size]\n",
    "    rnn_inputs = tf.nn.embedding_lookup(word_embeddings, x)\n",
    "\n",
    "    cell = rnn.BasicLSTMCell(state_size, state_is_tuple=True)\n",
    "    init_state = cell.zero_state(batch_size, tf.float32)\n",
    "    rnn_outputs, final_state =\\\n",
    "    tf.nn.dynamic_rnn(cell,rnn_inputs,initial_state=init_state)\n",
    "\n",
    "    with tf.variable_scope('softmax'):\n",
    "        W = tf.get_variable('W', [state_size, num_classes])\n",
    "        b = tf.get_variable('b', [num_classes],\n",
    "                            initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "    #reshape rnn_outputs and y so we can get the logits in a single matmul\n",
    "    rnn_outputs = tf.reshape(rnn_outputs, [-1, state_size])\n",
    "    y_reshaped = tf.reshape(y, [-1])\n",
    "\n",
    "    logits = tf.matmul(rnn_outputs, W) + b\n",
    "\n",
    "    # Make predictions\n",
    "    predictions = tf.nn.softmax(logits)\n",
    "\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            logits=logits, labels=y_reshaped))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "    \n",
    "    return dict(\n",
    "        x=x,\n",
    "        y=y,\n",
    "        init_state=init_state,\n",
    "        final_state=final_state,\n",
    "        loss=loss,\n",
    "        optimizer=optimizer,\n",
    "        preds=predictions,\n",
    "        saver=tf.train.Saver()\n",
    "    )\n",
    "\n",
    "batch_size = 32\n",
    "num_steps = 10\n",
    "state_size = 500\n",
    "num_epochs = 100\n",
    "\n",
    "graph = build_graph(batch_size, num_steps, state_size)\n",
    "train_network(graph, num_epochs, batch_size, num_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Stacking the RNN cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss in Epoch 0:7.4218\n",
      "Average loss in Epoch 1:7.2267\n",
      "Average loss in Epoch 2:6.2358\n",
      "Average loss in Epoch 3:5.8123\n",
      "Average loss in Epoch 4:5.6463\n",
      "Average loss in Epoch 5:5.5572\n",
      "Average loss in Epoch 6:5.5023\n",
      "Average loss in Epoch 7:5.4551\n",
      "Average loss in Epoch 8:5.4100\n",
      "Average loss in Epoch 9:5.3771\n",
      "Average loss in Epoch 10:5.3582\n",
      "Average loss in Epoch 11:5.3469\n",
      "Average loss in Epoch 12:5.3400\n",
      "Average loss in Epoch 13:5.3352\n",
      "Average loss in Epoch 14:5.3317\n",
      "Average loss in Epoch 15:5.3289\n",
      "Average loss in Epoch 16:5.3265\n",
      "Average loss in Epoch 17:5.3245\n",
      "Average loss in Epoch 18:5.3228\n",
      "Average loss in Epoch 19:5.3212\n",
      "Average loss in Epoch 20:5.3198\n",
      "Average loss in Epoch 21:5.3185\n",
      "Average loss in Epoch 22:5.3172\n",
      "Average loss in Epoch 23:5.3160\n",
      "Average loss in Epoch 24:5.3148\n",
      "Average loss in Epoch 25:5.3137\n",
      "Average loss in Epoch 26:5.3125\n",
      "Average loss in Epoch 27:5.3113\n",
      "Average loss in Epoch 28:5.3101\n",
      "Average loss in Epoch 29:5.3086\n",
      "Average loss in Epoch 30:5.3084\n",
      "Average loss in Epoch 31:5.3060\n",
      "Average loss in Epoch 32:5.3047\n",
      "Average loss in Epoch 33:5.3033\n",
      "Average loss in Epoch 34:5.3018\n",
      "Average loss in Epoch 35:5.3020\n",
      "Average loss in Epoch 36:5.3008\n",
      "Average loss in Epoch 37:5.2984\n",
      "Average loss in Epoch 38:5.2974\n",
      "Average loss in Epoch 39:5.2961\n",
      "Average loss in Epoch 40:5.2947\n",
      "Average loss in Epoch 41:5.2979\n",
      "Average loss in Epoch 42:5.3008\n",
      "Average loss in Epoch 43:5.2988\n",
      "Average loss in Epoch 44:5.2944\n",
      "Average loss in Epoch 45:5.2934\n",
      "Average loss in Epoch 46:5.2924\n",
      "Average loss in Epoch 47:5.2909\n",
      "Average loss in Epoch 48:5.2901\n",
      "Average loss in Epoch 49:5.2885\n",
      "Average loss in Epoch 50:5.2876\n",
      "Average loss in Epoch 51:5.2892\n",
      "Average loss in Epoch 52:5.2889\n",
      "Average loss in Epoch 53:5.2862\n",
      "Average loss in Epoch 54:5.2884\n",
      "Average loss in Epoch 55:5.2892\n",
      "Average loss in Epoch 56:5.2871\n",
      "Average loss in Epoch 57:5.2807\n",
      "Average loss in Epoch 58:5.2816\n",
      "Average loss in Epoch 59:5.2782\n",
      "Average loss in Epoch 60:5.2768\n",
      "Average loss in Epoch 61:5.2756\n",
      "Average loss in Epoch 62:5.2743\n",
      "Average loss in Epoch 63:5.2729\n",
      "Average loss in Epoch 64:5.2742\n",
      "Average loss in Epoch 65:5.2703\n",
      "Average loss in Epoch 66:5.2688\n",
      "Average loss in Epoch 67:5.2681\n",
      "Average loss in Epoch 68:5.2669\n",
      "Average loss in Epoch 69:5.2679\n",
      "Average loss in Epoch 70:5.2638\n",
      "Average loss in Epoch 71:5.2626\n",
      "Average loss in Epoch 72:5.2614\n",
      "Average loss in Epoch 73:5.2602\n",
      "Average loss in Epoch 74:5.2578\n",
      "Average loss in Epoch 75:5.2545\n",
      "Average loss in Epoch 76:5.2556\n",
      "Average loss in Epoch 77:5.2546\n",
      "Average loss in Epoch 78:5.2607\n",
      "Average loss in Epoch 79:5.2585\n",
      "Average loss in Epoch 80:5.2567\n",
      "Average loss in Epoch 81:5.2524\n",
      "Average loss in Epoch 82:5.2496\n",
      "Average loss in Epoch 83:5.2470\n",
      "Average loss in Epoch 84:5.2441\n",
      "Average loss in Epoch 85:5.2370\n",
      "Average loss in Epoch 86:5.2273\n",
      "Average loss in Epoch 87:5.2186\n",
      "Average loss in Epoch 88:5.1888\n",
      "Average loss in Epoch 89:5.1350\n",
      "Average loss in Epoch 90:5.0720\n",
      "Average loss in Epoch 91:5.0340\n",
      "Average loss in Epoch 92:5.0070\n",
      "Average loss in Epoch 93:4.9824\n",
      "Average loss in Epoch 94:4.9765\n",
      "Average loss in Epoch 95:4.9814\n",
      "Average loss in Epoch 96:4.9462\n",
      "Average loss in Epoch 97:4.9168\n",
      "Average loss in Epoch 98:4.8740\n",
      "Average loss in Epoch 99:4.8471\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[7.4218243980407719,\n",
       " 7.2266994094848629,\n",
       " 6.2358335304260253,\n",
       " 5.8122639846801754,\n",
       " 5.6463211822509765,\n",
       " 5.5571738243103024,\n",
       " 5.5023233795166018,\n",
       " 5.4550895881652828,\n",
       " 5.4099793624877925,\n",
       " 5.3771359443664553,\n",
       " 5.3582220268249507,\n",
       " 5.3469111251831052,\n",
       " 5.3399650001525876,\n",
       " 5.3352075195312496,\n",
       " 5.3317015647888182,\n",
       " 5.3288762092590334,\n",
       " 5.3265036964416508,\n",
       " 5.3244793891906737,\n",
       " 5.322770862579346,\n",
       " 5.3211570167541504,\n",
       " 5.3197667503356936,\n",
       " 5.3184642791748047,\n",
       " 5.3171938705444335,\n",
       " 5.3159810829162595,\n",
       " 5.3148066711425779,\n",
       " 5.3136506080627441,\n",
       " 5.312477645874023,\n",
       " 5.3112623023986814,\n",
       " 5.3100501632690431,\n",
       " 5.3086326408386233,\n",
       " 5.3084370803833005,\n",
       " 5.3059710693359374,\n",
       " 5.3047337341308598,\n",
       " 5.3033027648925781,\n",
       " 5.301822872161865,\n",
       " 5.3020363235473633,\n",
       " 5.3008287620544436,\n",
       " 5.2983656501770016,\n",
       " 5.2973518753051758,\n",
       " 5.2961262893676757,\n",
       " 5.2947085952758792,\n",
       " 5.2979016304016113,\n",
       " 5.3007911109924315,\n",
       " 5.2987985801696773,\n",
       " 5.29440860748291,\n",
       " 5.2933845329284672,\n",
       " 5.2924264335632323,\n",
       " 5.2909031295776368,\n",
       " 5.290117931365967,\n",
       " 5.2885377502441404,\n",
       " 5.2876188278198244,\n",
       " 5.2891513824462892,\n",
       " 5.2888625144958494,\n",
       " 5.2861885261535644,\n",
       " 5.288418483734131,\n",
       " 5.2892060279846191,\n",
       " 5.2870829963684081,\n",
       " 5.2806863212585453,\n",
       " 5.2815645027160647,\n",
       " 5.2781661224365237,\n",
       " 5.2768358421325683,\n",
       " 5.2756052970886227,\n",
       " 5.2742650604248045,\n",
       " 5.2729265785217283,\n",
       " 5.2742222404479984,\n",
       " 5.2703168487548826,\n",
       " 5.2687563514709472,\n",
       " 5.268089008331299,\n",
       " 5.266890029907227,\n",
       " 5.2678919982910157,\n",
       " 5.2637723159790042,\n",
       " 5.2626383399963377,\n",
       " 5.2614375114440914,\n",
       " 5.2601715278625489,\n",
       " 5.25779993057251,\n",
       " 5.2545054817199706,\n",
       " 5.2555862045288082,\n",
       " 5.2545931053161619,\n",
       " 5.2606890106201174,\n",
       " 5.2584917831420901,\n",
       " 5.2567009735107426,\n",
       " 5.2524388504028323,\n",
       " 5.2496115112304684,\n",
       " 5.2469783401489254,\n",
       " 5.2441077995300294,\n",
       " 5.2370444869995119,\n",
       " 5.2273055839538571,\n",
       " 5.2185732078552247,\n",
       " 5.1887953186035158,\n",
       " 5.1350221633911133,\n",
       " 5.0719818496704105,\n",
       " 5.0339793205261234,\n",
       " 5.006996059417725,\n",
       " 4.9824479484558104,\n",
       " 4.976482124328613,\n",
       " 4.9814471626281742,\n",
       " 4.9462149047851565,\n",
       " 4.9167642021179203,\n",
       " 4.8739523124694824,\n",
       " 4.8471098709106446]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_graph(batch_size, num_steps, state_size, num_layers = 5,\n",
    "                num_classes = vocab_size, learning_rate=1e-4):\n",
    "    \n",
    "    # wipe out all previously built graphs\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # word ids, without one-hot encoding\n",
    "    x = tf.placeholder(tf.int32, [batch_size, num_steps], name='x')\n",
    "    y = tf.placeholder(tf.int32, [batch_size, num_steps], name='y')\n",
    "\n",
    "    # vector representation for each words(word2vec)\n",
    "    word_embeddings = tf.get_variable('embedding_matrix',\n",
    "                                 [num_classes, state_size])\n",
    "\n",
    "    # rnn_inputs is a tensor of dim [batch_size,num_steps,state_size]\n",
    "    rnn_inputs = tf.nn.embedding_lookup(word_embeddings, x)\n",
    "\n",
    "    cell = rnn.BasicLSTMCell(state_size, state_is_tuple=True)\n",
    "    cell = rnn.MultiRNNCell([cell] * num_layers, state_is_tuple=True)\n",
    "    init_state = cell.zero_state(batch_size, tf.float32)\n",
    "    rnn_outputs, final_state =\\\n",
    "    tf.nn.dynamic_rnn(cell,rnn_inputs,initial_state=init_state)\n",
    "\n",
    "    with tf.variable_scope('softmax'):\n",
    "        W = tf.get_variable('W', [state_size, num_classes])\n",
    "        b = tf.get_variable('b', [num_classes],\n",
    "                            initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "    #reshape rnn_outputs and y so we can get the logits in a single matmul\n",
    "    rnn_outputs = tf.reshape(rnn_outputs, [-1, state_size])\n",
    "    y_reshaped = tf.reshape(y, [-1])\n",
    "\n",
    "    logits = tf.matmul(rnn_outputs, W) + b\n",
    "\n",
    "    # Make predictions\n",
    "    predictions = tf.nn.softmax(logits)\n",
    "\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            logits=logits, labels=y_reshaped))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "    \n",
    "    return dict(\n",
    "        x=x,\n",
    "        y=y,\n",
    "        init_state=init_state,\n",
    "        final_state=final_state,\n",
    "        loss=loss,\n",
    "        optimizer=optimizer,\n",
    "        preds=predictions,\n",
    "        saver=tf.train.Saver()\n",
    "    )\n",
    "\n",
    "batch_size = 32\n",
    "num_steps = 10\n",
    "state_size = 500\n",
    "num_epochs = 100\n",
    "\n",
    "graph = build_graph(batch_size, num_steps, state_size)\n",
    "train_network(graph, num_epochs, batch_size, num_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Adding Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM with a single layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss in Epoch 0:7.4217\n",
      "Average loss in Epoch 1:7.4082\n",
      "Average loss in Epoch 2:7.3474\n",
      "Average loss in Epoch 3:6.7228\n",
      "Average loss in Epoch 4:6.1681\n",
      "Average loss in Epoch 5:5.9628\n",
      "Average loss in Epoch 6:5.8527\n",
      "Average loss in Epoch 7:5.7802\n",
      "Average loss in Epoch 8:5.7259\n",
      "Average loss in Epoch 9:5.6832\n",
      "Average loss in Epoch 10:5.6483\n",
      "Average loss in Epoch 11:5.6173\n",
      "Average loss in Epoch 12:5.5909\n",
      "Average loss in Epoch 13:5.5665\n",
      "Average loss in Epoch 14:5.5451\n",
      "Average loss in Epoch 15:5.5248\n",
      "Average loss in Epoch 16:5.5070\n",
      "Average loss in Epoch 17:5.4892\n",
      "Average loss in Epoch 18:5.4730\n",
      "Average loss in Epoch 19:5.4577\n",
      "Average loss in Epoch 20:5.4428\n",
      "Average loss in Epoch 21:5.4290\n",
      "Average loss in Epoch 22:5.4151\n",
      "Average loss in Epoch 23:5.4019\n",
      "Average loss in Epoch 24:5.3889\n",
      "Average loss in Epoch 25:5.3752\n",
      "Average loss in Epoch 26:5.3618\n",
      "Average loss in Epoch 27:5.3490\n",
      "Average loss in Epoch 28:5.3364\n",
      "Average loss in Epoch 29:5.3238\n",
      "Average loss in Epoch 30:5.3104\n",
      "Average loss in Epoch 31:5.2970\n",
      "Average loss in Epoch 32:5.2836\n",
      "Average loss in Epoch 33:5.2714\n",
      "Average loss in Epoch 34:5.2582\n",
      "Average loss in Epoch 35:5.2447\n",
      "Average loss in Epoch 36:5.2311\n",
      "Average loss in Epoch 37:5.2175\n",
      "Average loss in Epoch 38:5.2034\n",
      "Average loss in Epoch 39:5.1889\n",
      "Average loss in Epoch 40:5.1747\n",
      "Average loss in Epoch 41:5.1600\n",
      "Average loss in Epoch 42:5.1437\n",
      "Average loss in Epoch 43:5.1293\n",
      "Average loss in Epoch 44:5.1133\n",
      "Average loss in Epoch 45:5.0965\n",
      "Average loss in Epoch 46:5.0801\n",
      "Average loss in Epoch 47:5.0634\n",
      "Average loss in Epoch 48:5.0460\n",
      "Average loss in Epoch 49:5.0284\n",
      "Average loss in Epoch 50:5.0104\n",
      "Average loss in Epoch 51:4.9923\n",
      "Average loss in Epoch 52:4.9727\n",
      "Average loss in Epoch 53:4.9530\n",
      "Average loss in Epoch 54:4.9343\n",
      "Average loss in Epoch 55:4.9139\n",
      "Average loss in Epoch 56:4.8923\n",
      "Average loss in Epoch 57:4.8720\n",
      "Average loss in Epoch 58:4.8491\n",
      "Average loss in Epoch 59:4.8260\n",
      "Average loss in Epoch 60:4.8037\n",
      "Average loss in Epoch 61:4.7814\n",
      "Average loss in Epoch 62:4.7568\n",
      "Average loss in Epoch 63:4.7331\n",
      "Average loss in Epoch 64:4.7087\n",
      "Average loss in Epoch 65:4.6838\n",
      "Average loss in Epoch 66:4.6604\n",
      "Average loss in Epoch 67:4.6346\n",
      "Average loss in Epoch 68:4.6099\n",
      "Average loss in Epoch 69:4.5832\n",
      "Average loss in Epoch 70:4.5579\n",
      "Average loss in Epoch 71:4.5316\n",
      "Average loss in Epoch 72:4.5048\n",
      "Average loss in Epoch 73:4.4800\n",
      "Average loss in Epoch 74:4.4516\n",
      "Average loss in Epoch 75:4.4245\n",
      "Average loss in Epoch 76:4.3985\n",
      "Average loss in Epoch 77:4.3721\n",
      "Average loss in Epoch 78:4.3440\n",
      "Average loss in Epoch 79:4.3166\n",
      "Average loss in Epoch 80:4.2873\n",
      "Average loss in Epoch 81:4.2596\n",
      "Average loss in Epoch 82:4.2311\n",
      "Average loss in Epoch 83:4.2027\n",
      "Average loss in Epoch 84:4.1730\n",
      "Average loss in Epoch 85:4.1443\n",
      "Average loss in Epoch 86:4.1155\n",
      "Average loss in Epoch 87:4.0857\n",
      "Average loss in Epoch 88:4.0571\n",
      "Average loss in Epoch 89:4.0270\n",
      "Average loss in Epoch 90:3.9966\n",
      "Average loss in Epoch 91:3.9659\n",
      "Average loss in Epoch 92:3.9370\n",
      "Average loss in Epoch 93:3.9058\n",
      "Average loss in Epoch 94:3.8767\n",
      "Average loss in Epoch 95:3.8454\n",
      "Average loss in Epoch 96:3.8151\n",
      "Average loss in Epoch 97:3.7840\n",
      "Average loss in Epoch 98:3.7544\n",
      "Average loss in Epoch 99:3.7231\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[7.4217072296142579,\n",
       " 7.4082433509826657,\n",
       " 7.3474486923217777,\n",
       " 6.7227872276306151,\n",
       " 6.1680613708496095,\n",
       " 5.9627669906616214,\n",
       " 5.8527391052246092,\n",
       " 5.7802228355407719,\n",
       " 5.7259400939941409,\n",
       " 5.6832261085510254,\n",
       " 5.6482846450805662,\n",
       " 5.6172613906860356,\n",
       " 5.5909370422363285,\n",
       " 5.5665307807922364,\n",
       " 5.545142784118652,\n",
       " 5.5247776794433596,\n",
       " 5.5070427513122562,\n",
       " 5.4891741943359378,\n",
       " 5.4730441474914553,\n",
       " 5.4576987838745117,\n",
       " 5.4427736091613772,\n",
       " 5.4289922714233398,\n",
       " 5.4151290130615237,\n",
       " 5.4018707466125484,\n",
       " 5.3888525199890136,\n",
       " 5.3751937675476071,\n",
       " 5.3618081855773925,\n",
       " 5.3490494155883788,\n",
       " 5.3364063644409176,\n",
       " 5.3237851524353026,\n",
       " 5.3103896903991696,\n",
       " 5.2969864273071288,\n",
       " 5.2836200523376462,\n",
       " 5.2713753128051755,\n",
       " 5.2582091522216796,\n",
       " 5.2447212409973147,\n",
       " 5.2311424827575683,\n",
       " 5.2174740219116211,\n",
       " 5.2033799743652347,\n",
       " 5.1888844871520998,\n",
       " 5.1746632003784176,\n",
       " 5.1599514579772947,\n",
       " 5.1437390327453612,\n",
       " 5.1292967224121098,\n",
       " 5.1133275222778316,\n",
       " 5.0964964103698733,\n",
       " 5.0801027297973631,\n",
       " 5.0634106826782226,\n",
       " 5.0460233688354492,\n",
       " 5.028435344696045,\n",
       " 5.0103728485107419,\n",
       " 4.9922528457641597,\n",
       " 4.9726770210266116,\n",
       " 4.9530093955993655,\n",
       " 4.9343123435974121,\n",
       " 4.9138677024841311,\n",
       " 4.8923292541503907,\n",
       " 4.8719771003723142,\n",
       " 4.8491118812561034,\n",
       " 4.8260496520996092,\n",
       " 4.8037436866760252,\n",
       " 4.7813928031921389,\n",
       " 4.7567740058898922,\n",
       " 4.7330819702148439,\n",
       " 4.7086719894409184,\n",
       " 4.6838163757324223,\n",
       " 4.660447978973389,\n",
       " 4.6346054840087891,\n",
       " 4.6098520278930666,\n",
       " 4.5831666564941402,\n",
       " 4.557908306121826,\n",
       " 4.5316479873657229,\n",
       " 4.5047538948059085,\n",
       " 4.4799637031555175,\n",
       " 4.4515984725952151,\n",
       " 4.4244977378845212,\n",
       " 4.3985092258453369,\n",
       " 4.3720950222015382,\n",
       " 4.3439618396759032,\n",
       " 4.3166127681732176,\n",
       " 4.2873301410675051,\n",
       " 4.2596100616455077,\n",
       " 4.2311332511901858,\n",
       " 4.2026516056060794,\n",
       " 4.1729701328277589,\n",
       " 4.1442691326141361,\n",
       " 4.1155330467224118,\n",
       " 4.0857331848144529,\n",
       " 4.057094497680664,\n",
       " 4.0270054340362549,\n",
       " 3.9966187763214109,\n",
       " 3.9659019470214845,\n",
       " 3.9370237350463868,\n",
       " 3.9057974910736082,\n",
       " 3.8766773128509522,\n",
       " 3.8453764820098879,\n",
       " 3.8151459312438964,\n",
       " 3.7839702415466308,\n",
       " 3.754389820098877,\n",
       " 3.7230600547790527]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_graph(batch_size, num_steps, state_size, keep_prob=0.7,\n",
    "                num_classes = vocab_size, learning_rate=1e-4):\n",
    "    \n",
    "    # wipe out all previously built graphs\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # word ids, without one-hot encoding\n",
    "    x = tf.placeholder(tf.int32, [batch_size, num_steps], name='x')\n",
    "    y = tf.placeholder(tf.int32, [batch_size, num_steps], name='y')\n",
    "\n",
    "    # vector representation for each words(word2vec)\n",
    "    word_embeddings = tf.get_variable('embedding_matrix',\n",
    "                                 [num_classes, state_size])\n",
    "\n",
    "    # rnn_inputs is a tensor of dim [batch_size,num_steps,state_size]\n",
    "    rnn_inputs = tf.nn.embedding_lookup(word_embeddings, x)\n",
    "    \n",
    "    # Add a dropout to the input layer\n",
    "    rnn_inputs = tf.nn.dropout(rnn_inputs, keep_prob)\n",
    "\n",
    "    cell = rnn.BasicLSTMCell(state_size, state_is_tuple=True)\n",
    "    init_state = cell.zero_state(batch_size, tf.float32)\n",
    "    rnn_outputs, final_state =\\\n",
    "    tf.nn.dynamic_rnn(cell,rnn_inputs,initial_state=init_state)\n",
    "\n",
    "    with tf.variable_scope('softmax'):\n",
    "        W = tf.get_variable('W', [state_size, num_classes])\n",
    "        b = tf.get_variable('b', [num_classes],\n",
    "                            initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "    #reshape rnn_outputs and y so we can get the logits in a single matmul\n",
    "    rnn_outputs = tf.reshape(rnn_outputs, [-1, state_size])\n",
    "    \n",
    "    # Dropout layer\n",
    "    rnn_inputs = tf.nn.dropout(rnn_outputs, keep_prob)\n",
    "    \n",
    "    y_reshaped = tf.reshape(y, [-1])\n",
    "\n",
    "    logits = tf.matmul(rnn_outputs, W) + b\n",
    "\n",
    "    # Make predictions\n",
    "    predictions = tf.nn.softmax(logits)\n",
    "\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            logits=logits, labels=y_reshaped))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "    \n",
    "    return dict(\n",
    "        x=x,\n",
    "        y=y,\n",
    "        init_state=init_state,\n",
    "        final_state=final_state,\n",
    "        loss=loss,\n",
    "        optimizer=optimizer,\n",
    "        preds=predictions,\n",
    "        saver=tf.train.Saver()\n",
    "    )\n",
    "\n",
    "batch_size = 32\n",
    "num_steps = 10\n",
    "state_size = 500\n",
    "num_epochs = 100\n",
    "\n",
    "graph = build_graph(batch_size, num_steps, state_size)\n",
    "train_network(graph, num_epochs, batch_size, num_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding dropout for the layers in Stacked LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss in Epoch 0:7.4220\n",
      "Average loss in Epoch 1:7.2523\n",
      "Average loss in Epoch 2:6.3041\n",
      "Average loss in Epoch 3:5.8651\n",
      "Average loss in Epoch 4:5.7038\n",
      "Average loss in Epoch 5:5.6177\n",
      "Average loss in Epoch 6:5.5626\n",
      "Average loss in Epoch 7:5.5043\n",
      "Average loss in Epoch 8:5.4660\n",
      "Average loss in Epoch 9:5.4351\n",
      "Average loss in Epoch 10:5.4032\n",
      "Average loss in Epoch 11:5.3942\n",
      "Average loss in Epoch 12:5.3863\n",
      "Average loss in Epoch 13:5.3781\n",
      "Average loss in Epoch 14:5.3707\n",
      "Average loss in Epoch 15:5.3738\n",
      "Average loss in Epoch 16:5.3687\n",
      "Average loss in Epoch 17:5.3637\n",
      "Average loss in Epoch 18:5.3497\n",
      "Average loss in Epoch 19:5.3517\n",
      "Average loss in Epoch 20:5.3559\n",
      "Average loss in Epoch 21:5.3521\n",
      "Average loss in Epoch 22:5.3474\n",
      "Average loss in Epoch 23:5.3442\n",
      "Average loss in Epoch 24:5.3490\n",
      "Average loss in Epoch 25:5.3452\n",
      "Average loss in Epoch 26:5.3418\n",
      "Average loss in Epoch 27:5.3397\n",
      "Average loss in Epoch 28:5.3367\n",
      "Average loss in Epoch 29:5.3357\n",
      "Average loss in Epoch 30:5.3378\n",
      "Average loss in Epoch 31:5.3361\n",
      "Average loss in Epoch 32:5.3369\n",
      "Average loss in Epoch 33:5.3381\n",
      "Average loss in Epoch 34:5.3327\n",
      "Average loss in Epoch 35:5.3329\n",
      "Average loss in Epoch 36:5.3283\n",
      "Average loss in Epoch 37:5.3249\n",
      "Average loss in Epoch 38:5.3220\n",
      "Average loss in Epoch 39:5.3212\n",
      "Average loss in Epoch 40:5.3223\n",
      "Average loss in Epoch 41:5.3177\n",
      "Average loss in Epoch 42:5.3165\n",
      "Average loss in Epoch 43:5.3191\n",
      "Average loss in Epoch 44:5.3131\n",
      "Average loss in Epoch 45:5.3230\n",
      "Average loss in Epoch 46:5.3210\n",
      "Average loss in Epoch 47:5.3182\n",
      "Average loss in Epoch 48:5.3211\n",
      "Average loss in Epoch 49:5.3193\n",
      "Average loss in Epoch 50:5.3174\n",
      "Average loss in Epoch 51:5.3112\n",
      "Average loss in Epoch 52:5.3103\n",
      "Average loss in Epoch 53:5.3035\n",
      "Average loss in Epoch 54:5.3109\n",
      "Average loss in Epoch 55:5.3027\n",
      "Average loss in Epoch 56:5.2990\n",
      "Average loss in Epoch 57:5.3043\n",
      "Average loss in Epoch 58:5.2989\n",
      "Average loss in Epoch 59:5.2975\n",
      "Average loss in Epoch 60:5.2936\n",
      "Average loss in Epoch 61:5.3021\n",
      "Average loss in Epoch 62:5.2966\n",
      "Average loss in Epoch 63:5.2933\n",
      "Average loss in Epoch 64:5.2890\n",
      "Average loss in Epoch 65:5.2903\n",
      "Average loss in Epoch 66:5.2868\n",
      "Average loss in Epoch 67:5.2867\n",
      "Average loss in Epoch 68:5.2831\n",
      "Average loss in Epoch 69:5.2829\n",
      "Average loss in Epoch 70:5.2790\n",
      "Average loss in Epoch 71:5.2854\n",
      "Average loss in Epoch 72:5.2750\n",
      "Average loss in Epoch 73:5.2771\n",
      "Average loss in Epoch 74:5.2803\n",
      "Average loss in Epoch 75:5.2786\n",
      "Average loss in Epoch 76:5.2737\n",
      "Average loss in Epoch 77:5.2673\n",
      "Average loss in Epoch 78:5.2690\n",
      "Average loss in Epoch 79:5.2620\n",
      "Average loss in Epoch 80:5.2626\n",
      "Average loss in Epoch 81:5.2634\n",
      "Average loss in Epoch 82:5.2522\n",
      "Average loss in Epoch 83:5.2531\n",
      "Average loss in Epoch 84:5.2403\n",
      "Average loss in Epoch 85:5.2280\n",
      "Average loss in Epoch 86:5.2049\n",
      "Average loss in Epoch 87:5.1537\n",
      "Average loss in Epoch 88:5.1056\n",
      "Average loss in Epoch 89:5.0824\n",
      "Average loss in Epoch 90:5.0555\n",
      "Average loss in Epoch 91:5.0176\n",
      "Average loss in Epoch 92:4.9833\n",
      "Average loss in Epoch 93:4.9612\n",
      "Average loss in Epoch 94:4.9354\n",
      "Average loss in Epoch 95:4.9105\n",
      "Average loss in Epoch 96:4.8917\n",
      "Average loss in Epoch 97:4.8607\n",
      "Average loss in Epoch 98:4.8343\n",
      "Average loss in Epoch 99:4.8109\n",
      "Average loss in Epoch 100:4.7912\n",
      "Average loss in Epoch 101:4.7675\n",
      "Average loss in Epoch 102:4.7503\n",
      "Average loss in Epoch 103:4.7177\n",
      "Average loss in Epoch 104:4.6933\n",
      "Average loss in Epoch 105:4.6707\n",
      "Average loss in Epoch 106:4.6570\n",
      "Average loss in Epoch 107:4.6309\n",
      "Average loss in Epoch 108:4.6218\n",
      "Average loss in Epoch 109:4.6109\n",
      "Average loss in Epoch 110:4.5859\n",
      "Average loss in Epoch 111:4.5649\n",
      "Average loss in Epoch 112:4.5603\n",
      "Average loss in Epoch 113:4.5322\n",
      "Average loss in Epoch 114:4.5052\n",
      "Average loss in Epoch 115:4.4853\n",
      "Average loss in Epoch 116:4.4571\n",
      "Average loss in Epoch 117:4.4398\n",
      "Average loss in Epoch 118:4.4135\n",
      "Average loss in Epoch 119:4.3989\n",
      "Average loss in Epoch 120:4.3821\n",
      "Average loss in Epoch 121:4.3628\n",
      "Average loss in Epoch 122:4.3475\n",
      "Average loss in Epoch 123:4.3359\n",
      "Average loss in Epoch 124:4.3282\n",
      "Average loss in Epoch 125:4.3346\n",
      "Average loss in Epoch 126:4.3579\n",
      "Average loss in Epoch 127:4.3233\n",
      "Average loss in Epoch 128:4.2769\n",
      "Average loss in Epoch 129:4.2434\n",
      "Average loss in Epoch 130:4.2090\n",
      "Average loss in Epoch 131:4.1851\n",
      "Average loss in Epoch 132:4.1622\n",
      "Average loss in Epoch 133:4.1392\n",
      "Average loss in Epoch 134:4.1115\n",
      "Average loss in Epoch 135:4.0997\n",
      "Average loss in Epoch 136:4.0844\n",
      "Average loss in Epoch 137:4.0580\n",
      "Average loss in Epoch 138:4.0397\n",
      "Average loss in Epoch 139:4.0204\n",
      "Average loss in Epoch 140:4.0046\n",
      "Average loss in Epoch 141:3.9911\n",
      "Average loss in Epoch 142:3.9717\n",
      "Average loss in Epoch 143:3.9620\n",
      "Average loss in Epoch 144:3.9370\n",
      "Average loss in Epoch 145:3.9177\n",
      "Average loss in Epoch 146:3.9048\n",
      "Average loss in Epoch 147:3.8808\n",
      "Average loss in Epoch 148:3.8592\n",
      "Average loss in Epoch 149:3.8489\n",
      "Average loss in Epoch 150:3.8294\n",
      "Average loss in Epoch 151:3.8022\n",
      "Average loss in Epoch 152:3.7962\n",
      "Average loss in Epoch 153:3.7731\n",
      "Average loss in Epoch 154:3.7594\n",
      "Average loss in Epoch 155:3.7468\n",
      "Average loss in Epoch 156:3.7320\n",
      "Average loss in Epoch 157:3.7207\n",
      "Average loss in Epoch 158:3.7034\n",
      "Average loss in Epoch 159:3.6917\n",
      "Average loss in Epoch 160:3.6777\n",
      "Average loss in Epoch 161:3.6659\n",
      "Average loss in Epoch 162:3.6623\n",
      "Average loss in Epoch 163:3.6431\n",
      "Average loss in Epoch 164:3.6327\n",
      "Average loss in Epoch 165:3.6189\n",
      "Average loss in Epoch 166:3.6060\n",
      "Average loss in Epoch 167:3.6051\n",
      "Average loss in Epoch 168:3.5980\n",
      "Average loss in Epoch 169:3.5761\n",
      "Average loss in Epoch 170:3.5633\n",
      "Average loss in Epoch 171:3.5591\n",
      "Average loss in Epoch 172:3.5492\n",
      "Average loss in Epoch 173:3.5425\n",
      "Average loss in Epoch 174:3.5321\n",
      "Average loss in Epoch 175:3.5037\n",
      "Average loss in Epoch 176:3.4712\n",
      "Average loss in Epoch 177:3.4616\n",
      "Average loss in Epoch 178:3.4606\n",
      "Average loss in Epoch 179:3.4605\n",
      "Average loss in Epoch 180:3.4606\n",
      "Average loss in Epoch 181:3.4705\n",
      "Average loss in Epoch 182:3.4831\n",
      "Average loss in Epoch 183:3.4835\n",
      "Average loss in Epoch 184:3.4470\n",
      "Average loss in Epoch 185:3.3925\n",
      "Average loss in Epoch 186:3.3664\n",
      "Average loss in Epoch 187:3.3253\n",
      "Average loss in Epoch 188:3.2881\n",
      "Average loss in Epoch 189:3.2782\n",
      "Average loss in Epoch 190:3.2566\n",
      "Average loss in Epoch 191:3.2530\n",
      "Average loss in Epoch 192:3.2319\n",
      "Average loss in Epoch 193:3.2184\n",
      "Average loss in Epoch 194:3.2018\n",
      "Average loss in Epoch 195:3.1897\n",
      "Average loss in Epoch 196:3.1716\n",
      "Average loss in Epoch 197:3.1643\n",
      "Average loss in Epoch 198:3.1487\n",
      "Average loss in Epoch 199:3.1356\n",
      "Average loss in Epoch 200:3.1157\n",
      "Average loss in Epoch 201:3.0977\n",
      "Average loss in Epoch 202:3.0919\n",
      "Average loss in Epoch 203:3.0831\n",
      "Average loss in Epoch 204:3.0645\n",
      "Average loss in Epoch 205:3.0521\n",
      "Average loss in Epoch 206:3.0328\n",
      "Average loss in Epoch 207:3.0192\n",
      "Average loss in Epoch 208:3.0065\n",
      "Average loss in Epoch 209:3.0041\n",
      "Average loss in Epoch 210:2.9807\n",
      "Average loss in Epoch 211:2.9633\n",
      "Average loss in Epoch 212:2.9578\n",
      "Average loss in Epoch 213:2.9517\n",
      "Average loss in Epoch 214:2.9272\n",
      "Average loss in Epoch 215:2.9109\n",
      "Average loss in Epoch 216:2.9133\n",
      "Average loss in Epoch 217:2.8975\n",
      "Average loss in Epoch 218:2.8886\n",
      "Average loss in Epoch 219:2.8753\n",
      "Average loss in Epoch 220:2.8481\n",
      "Average loss in Epoch 221:2.8444\n",
      "Average loss in Epoch 222:2.8379\n",
      "Average loss in Epoch 223:2.8344\n",
      "Average loss in Epoch 224:2.8181\n",
      "Average loss in Epoch 225:2.8050\n",
      "Average loss in Epoch 226:2.7914\n",
      "Average loss in Epoch 227:2.7987\n",
      "Average loss in Epoch 228:2.7747\n",
      "Average loss in Epoch 229:2.7467\n",
      "Average loss in Epoch 230:2.7357\n",
      "Average loss in Epoch 231:2.7140\n",
      "Average loss in Epoch 232:2.7023\n",
      "Average loss in Epoch 233:2.6917\n",
      "Average loss in Epoch 234:2.6914\n",
      "Average loss in Epoch 235:2.6745\n",
      "Average loss in Epoch 236:2.6604\n",
      "Average loss in Epoch 237:2.6405\n",
      "Average loss in Epoch 238:2.6380\n",
      "Average loss in Epoch 239:2.6191\n",
      "Average loss in Epoch 240:2.6100\n",
      "Average loss in Epoch 241:2.6049\n",
      "Average loss in Epoch 242:2.5875\n",
      "Average loss in Epoch 243:2.5809\n",
      "Average loss in Epoch 244:2.5841\n",
      "Average loss in Epoch 245:2.5639\n",
      "Average loss in Epoch 246:2.5423\n",
      "Average loss in Epoch 247:2.5350\n",
      "Average loss in Epoch 248:2.5181\n",
      "Average loss in Epoch 249:2.5111\n",
      "Average loss in Epoch 250:2.5017\n",
      "Average loss in Epoch 251:2.5025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss in Epoch 252:2.4861\n",
      "Average loss in Epoch 253:2.4791\n",
      "Average loss in Epoch 254:2.4659\n",
      "Average loss in Epoch 255:2.4520\n",
      "Average loss in Epoch 256:2.4376\n",
      "Average loss in Epoch 257:2.4305\n",
      "Average loss in Epoch 258:2.4268\n",
      "Average loss in Epoch 259:2.4110\n",
      "Average loss in Epoch 260:2.4051\n",
      "Average loss in Epoch 261:2.3940\n",
      "Average loss in Epoch 262:2.3973\n",
      "Average loss in Epoch 263:2.3836\n",
      "Average loss in Epoch 264:2.3646\n",
      "Average loss in Epoch 265:2.3608\n",
      "Average loss in Epoch 266:2.3538\n",
      "Average loss in Epoch 267:2.3327\n",
      "Average loss in Epoch 268:2.3319\n",
      "Average loss in Epoch 269:2.3138\n",
      "Average loss in Epoch 270:2.2977\n",
      "Average loss in Epoch 271:2.2952\n",
      "Average loss in Epoch 272:2.2954\n",
      "Average loss in Epoch 273:2.2808\n",
      "Average loss in Epoch 274:2.2674\n",
      "Average loss in Epoch 275:2.2675\n",
      "Average loss in Epoch 276:2.2629\n",
      "Average loss in Epoch 277:2.2761\n",
      "Average loss in Epoch 278:2.2385\n",
      "Average loss in Epoch 279:2.2579\n",
      "Average loss in Epoch 280:2.2561\n",
      "Average loss in Epoch 281:2.2691\n",
      "Average loss in Epoch 282:2.3012\n",
      "Average loss in Epoch 283:2.3131\n",
      "Average loss in Epoch 284:2.3272\n",
      "Average loss in Epoch 285:2.2907\n",
      "Average loss in Epoch 286:2.2718\n",
      "Average loss in Epoch 287:2.2339\n",
      "Average loss in Epoch 288:2.1783\n",
      "Average loss in Epoch 289:2.1432\n",
      "Average loss in Epoch 290:2.1330\n",
      "Average loss in Epoch 291:2.1023\n",
      "Average loss in Epoch 292:2.0964\n",
      "Average loss in Epoch 293:2.0858\n",
      "Average loss in Epoch 294:2.0833\n",
      "Average loss in Epoch 295:2.0720\n",
      "Average loss in Epoch 296:2.0659\n",
      "Average loss in Epoch 297:2.0478\n",
      "Average loss in Epoch 298:2.0308\n",
      "Average loss in Epoch 299:2.0276\n",
      "Average loss in Epoch 300:2.0166\n",
      "Average loss in Epoch 301:1.9957\n",
      "Average loss in Epoch 302:1.9970\n",
      "Average loss in Epoch 303:1.9865\n",
      "Average loss in Epoch 304:1.9831\n",
      "Average loss in Epoch 305:1.9735\n",
      "Average loss in Epoch 306:1.9612\n",
      "Average loss in Epoch 307:1.9432\n",
      "Average loss in Epoch 308:1.9512\n",
      "Average loss in Epoch 309:1.9454\n",
      "Average loss in Epoch 310:1.9284\n",
      "Average loss in Epoch 311:1.9233\n",
      "Average loss in Epoch 312:1.9147\n",
      "Average loss in Epoch 313:1.8943\n",
      "Average loss in Epoch 314:1.8993\n",
      "Average loss in Epoch 315:1.8934\n",
      "Average loss in Epoch 316:1.8862\n",
      "Average loss in Epoch 317:1.8625\n",
      "Average loss in Epoch 318:1.8683\n",
      "Average loss in Epoch 319:1.8557\n",
      "Average loss in Epoch 320:1.8438\n",
      "Average loss in Epoch 321:1.8437\n",
      "Average loss in Epoch 322:1.8380\n",
      "Average loss in Epoch 323:1.8273\n",
      "Average loss in Epoch 324:1.8131\n",
      "Average loss in Epoch 325:1.8171\n",
      "Average loss in Epoch 326:1.8055\n",
      "Average loss in Epoch 327:1.7963\n",
      "Average loss in Epoch 328:1.7884\n",
      "Average loss in Epoch 329:1.7793\n",
      "Average loss in Epoch 330:1.7705\n",
      "Average loss in Epoch 331:1.7705\n",
      "Average loss in Epoch 332:1.7653\n",
      "Average loss in Epoch 333:1.7616\n",
      "Average loss in Epoch 334:1.7430\n",
      "Average loss in Epoch 335:1.7355\n",
      "Average loss in Epoch 336:1.7268\n",
      "Average loss in Epoch 337:1.7204\n",
      "Average loss in Epoch 338:1.7138\n",
      "Average loss in Epoch 339:1.7012\n",
      "Average loss in Epoch 340:1.6952\n",
      "Average loss in Epoch 341:1.6962\n",
      "Average loss in Epoch 342:1.6838\n",
      "Average loss in Epoch 343:1.6889\n",
      "Average loss in Epoch 344:1.6722\n",
      "Average loss in Epoch 345:1.6628\n",
      "Average loss in Epoch 346:1.6625\n",
      "Average loss in Epoch 347:1.6574\n",
      "Average loss in Epoch 348:1.6477\n",
      "Average loss in Epoch 349:1.6326\n",
      "Average loss in Epoch 350:1.6315\n",
      "Average loss in Epoch 351:1.6161\n",
      "Average loss in Epoch 352:1.6279\n",
      "Average loss in Epoch 353:1.6158\n",
      "Average loss in Epoch 354:1.6003\n",
      "Average loss in Epoch 355:1.6014\n",
      "Average loss in Epoch 356:1.5976\n",
      "Average loss in Epoch 357:1.5840\n",
      "Average loss in Epoch 358:1.5791\n",
      "Average loss in Epoch 359:1.5742\n",
      "Average loss in Epoch 360:1.5556\n",
      "Average loss in Epoch 361:1.5456\n",
      "Average loss in Epoch 362:1.5617\n",
      "Average loss in Epoch 363:1.5455\n",
      "Average loss in Epoch 364:1.5364\n",
      "Average loss in Epoch 365:1.5315\n",
      "Average loss in Epoch 366:1.5257\n",
      "Average loss in Epoch 367:1.5138\n",
      "Average loss in Epoch 368:1.5173\n",
      "Average loss in Epoch 369:1.5105\n",
      "Average loss in Epoch 370:1.4921\n",
      "Average loss in Epoch 371:1.4849\n",
      "Average loss in Epoch 372:1.4841\n",
      "Average loss in Epoch 373:1.4752\n",
      "Average loss in Epoch 374:1.4682\n",
      "Average loss in Epoch 375:1.4595\n",
      "Average loss in Epoch 376:1.4511\n",
      "Average loss in Epoch 377:1.4487\n",
      "Average loss in Epoch 378:1.4428\n",
      "Average loss in Epoch 379:1.4409\n",
      "Average loss in Epoch 380:1.4345\n",
      "Average loss in Epoch 381:1.4308\n",
      "Average loss in Epoch 382:1.4186\n",
      "Average loss in Epoch 383:1.4160\n",
      "Average loss in Epoch 384:1.4098\n",
      "Average loss in Epoch 385:1.4010\n",
      "Average loss in Epoch 386:1.3870\n",
      "Average loss in Epoch 387:1.3836\n",
      "Average loss in Epoch 388:1.3864\n",
      "Average loss in Epoch 389:1.3779\n",
      "Average loss in Epoch 390:1.3828\n",
      "Average loss in Epoch 391:1.3674\n",
      "Average loss in Epoch 392:1.3595\n",
      "Average loss in Epoch 393:1.3502\n",
      "Average loss in Epoch 394:1.3475\n",
      "Average loss in Epoch 395:1.3568\n",
      "Average loss in Epoch 396:1.3457\n",
      "Average loss in Epoch 397:1.3408\n",
      "Average loss in Epoch 398:1.3340\n",
      "Average loss in Epoch 399:1.3242\n",
      "Average loss in Epoch 400:1.3238\n",
      "Average loss in Epoch 401:1.3202\n",
      "Average loss in Epoch 402:1.3066\n",
      "Average loss in Epoch 403:1.2959\n",
      "Average loss in Epoch 404:1.2913\n",
      "Average loss in Epoch 405:1.2927\n",
      "Average loss in Epoch 406:1.2978\n",
      "Average loss in Epoch 407:1.2856\n",
      "Average loss in Epoch 408:1.2872\n",
      "Average loss in Epoch 409:1.2769\n",
      "Average loss in Epoch 410:1.2732\n",
      "Average loss in Epoch 411:1.2573\n",
      "Average loss in Epoch 412:1.2594\n",
      "Average loss in Epoch 413:1.2601\n",
      "Average loss in Epoch 414:1.2542\n",
      "Average loss in Epoch 415:1.2381\n",
      "Average loss in Epoch 416:1.2378\n",
      "Average loss in Epoch 417:1.2399\n",
      "Average loss in Epoch 418:1.2288\n",
      "Average loss in Epoch 419:1.2213\n",
      "Average loss in Epoch 420:1.2219\n",
      "Average loss in Epoch 421:1.2247\n",
      "Average loss in Epoch 422:1.2149\n",
      "Average loss in Epoch 423:1.2043\n",
      "Average loss in Epoch 424:1.2026\n",
      "Average loss in Epoch 425:1.1954\n",
      "Average loss in Epoch 426:1.1934\n",
      "Average loss in Epoch 427:1.1837\n",
      "Average loss in Epoch 428:1.1724\n",
      "Average loss in Epoch 429:1.1804\n",
      "Average loss in Epoch 430:1.1813\n",
      "Average loss in Epoch 431:1.1786\n",
      "Average loss in Epoch 432:1.1616\n",
      "Average loss in Epoch 433:1.1579\n",
      "Average loss in Epoch 434:1.1715\n",
      "Average loss in Epoch 435:1.1660\n",
      "Average loss in Epoch 436:1.1605\n",
      "Average loss in Epoch 437:1.1422\n",
      "Average loss in Epoch 438:1.1460\n",
      "Average loss in Epoch 439:1.1429\n",
      "Average loss in Epoch 440:1.1316\n",
      "Average loss in Epoch 441:1.1319\n",
      "Average loss in Epoch 442:1.1144\n",
      "Average loss in Epoch 443:1.1248\n",
      "Average loss in Epoch 444:1.1252\n",
      "Average loss in Epoch 445:1.1152\n",
      "Average loss in Epoch 446:1.1110\n",
      "Average loss in Epoch 447:1.1167\n",
      "Average loss in Epoch 448:1.1151\n",
      "Average loss in Epoch 449:1.1022\n",
      "Average loss in Epoch 450:1.0988\n",
      "Average loss in Epoch 451:1.0886\n",
      "Average loss in Epoch 452:1.0887\n",
      "Average loss in Epoch 453:1.0683\n",
      "Average loss in Epoch 454:1.0599\n",
      "Average loss in Epoch 455:1.0634\n",
      "Average loss in Epoch 456:1.0610\n",
      "Average loss in Epoch 457:1.0524\n",
      "Average loss in Epoch 458:1.0476\n",
      "Average loss in Epoch 459:1.0394\n",
      "Average loss in Epoch 460:1.0343\n",
      "Average loss in Epoch 461:1.0235\n",
      "Average loss in Epoch 462:1.0330\n",
      "Average loss in Epoch 463:1.0261\n",
      "Average loss in Epoch 464:1.0142\n",
      "Average loss in Epoch 465:1.0093\n",
      "Average loss in Epoch 466:1.0132\n",
      "Average loss in Epoch 467:1.0089\n",
      "Average loss in Epoch 468:1.0038\n",
      "Average loss in Epoch 469:1.0101\n",
      "Average loss in Epoch 470:1.0053\n",
      "Average loss in Epoch 471:0.9950\n",
      "Average loss in Epoch 472:0.9886\n",
      "Average loss in Epoch 473:0.9851\n",
      "Average loss in Epoch 474:0.9777\n",
      "Average loss in Epoch 475:0.9858\n",
      "Average loss in Epoch 476:0.9785\n",
      "Average loss in Epoch 477:0.9611\n",
      "Average loss in Epoch 478:0.9665\n",
      "Average loss in Epoch 479:0.9781\n",
      "Average loss in Epoch 480:0.9718\n",
      "Average loss in Epoch 481:0.9559\n",
      "Average loss in Epoch 482:0.9572\n",
      "Average loss in Epoch 483:0.9623\n",
      "Average loss in Epoch 484:0.9421\n",
      "Average loss in Epoch 485:0.9491\n",
      "Average loss in Epoch 486:0.9393\n",
      "Average loss in Epoch 487:0.9283\n",
      "Average loss in Epoch 488:0.9271\n",
      "Average loss in Epoch 489:0.9404\n",
      "Average loss in Epoch 490:0.9295\n",
      "Average loss in Epoch 491:0.9326\n",
      "Average loss in Epoch 492:0.9241\n",
      "Average loss in Epoch 493:0.9135\n",
      "Average loss in Epoch 494:0.9088\n",
      "Average loss in Epoch 495:0.9081\n",
      "Average loss in Epoch 496:0.9029\n",
      "Average loss in Epoch 497:0.9108\n",
      "Average loss in Epoch 498:0.9036\n",
      "Average loss in Epoch 499:0.8989\n",
      "Average loss in Epoch 500:0.8925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss in Epoch 501:0.8928\n",
      "Average loss in Epoch 502:0.8933\n",
      "Average loss in Epoch 503:0.8867\n",
      "Average loss in Epoch 504:0.8764\n",
      "Average loss in Epoch 505:0.8812\n",
      "Average loss in Epoch 506:0.8733\n",
      "Average loss in Epoch 507:0.8707\n",
      "Average loss in Epoch 508:0.8709\n",
      "Average loss in Epoch 509:0.8584\n",
      "Average loss in Epoch 510:0.8606\n",
      "Average loss in Epoch 511:0.8558\n",
      "Average loss in Epoch 512:0.8575\n",
      "Average loss in Epoch 513:0.8443\n",
      "Average loss in Epoch 514:0.8420\n",
      "Average loss in Epoch 515:0.8494\n",
      "Average loss in Epoch 516:0.8363\n",
      "Average loss in Epoch 517:0.8462\n",
      "Average loss in Epoch 518:0.8428\n",
      "Average loss in Epoch 519:0.8234\n",
      "Average loss in Epoch 520:0.8356\n",
      "Average loss in Epoch 521:0.8243\n",
      "Average loss in Epoch 522:0.8238\n",
      "Average loss in Epoch 523:0.8254\n",
      "Average loss in Epoch 524:0.8194\n",
      "Average loss in Epoch 525:0.8121\n",
      "Average loss in Epoch 526:0.8130\n",
      "Average loss in Epoch 527:0.8083\n",
      "Average loss in Epoch 528:0.8023\n",
      "Average loss in Epoch 529:0.8021\n",
      "Average loss in Epoch 530:0.8081\n",
      "Average loss in Epoch 531:0.7964\n",
      "Average loss in Epoch 532:0.7961\n",
      "Average loss in Epoch 533:0.8007\n",
      "Average loss in Epoch 534:0.7848\n",
      "Average loss in Epoch 535:0.7822\n",
      "Average loss in Epoch 536:0.7872\n",
      "Average loss in Epoch 537:0.7809\n",
      "Average loss in Epoch 538:0.7740\n",
      "Average loss in Epoch 539:0.7776\n",
      "Average loss in Epoch 540:0.7631\n",
      "Average loss in Epoch 541:0.7723\n",
      "Average loss in Epoch 542:0.7660\n",
      "Average loss in Epoch 543:0.7561\n",
      "Average loss in Epoch 544:0.7580\n",
      "Average loss in Epoch 545:0.7521\n",
      "Average loss in Epoch 546:0.7481\n",
      "Average loss in Epoch 547:0.7462\n",
      "Average loss in Epoch 548:0.7580\n",
      "Average loss in Epoch 549:0.7486\n",
      "Average loss in Epoch 550:0.7450\n",
      "Average loss in Epoch 551:0.7466\n",
      "Average loss in Epoch 552:0.7326\n",
      "Average loss in Epoch 553:0.7356\n",
      "Average loss in Epoch 554:0.7323\n",
      "Average loss in Epoch 555:0.7438\n",
      "Average loss in Epoch 556:0.7397\n",
      "Average loss in Epoch 557:0.7313\n",
      "Average loss in Epoch 558:0.7293\n",
      "Average loss in Epoch 559:0.7270\n",
      "Average loss in Epoch 560:0.7189\n",
      "Average loss in Epoch 561:0.7108\n",
      "Average loss in Epoch 562:0.7184\n",
      "Average loss in Epoch 563:0.7108\n",
      "Average loss in Epoch 564:0.7153\n",
      "Average loss in Epoch 565:0.7068\n",
      "Average loss in Epoch 566:0.7058\n",
      "Average loss in Epoch 567:0.7022\n",
      "Average loss in Epoch 568:0.6904\n",
      "Average loss in Epoch 569:0.7040\n",
      "Average loss in Epoch 570:0.6956\n",
      "Average loss in Epoch 571:0.6980\n",
      "Average loss in Epoch 572:0.6998\n",
      "Average loss in Epoch 573:0.6918\n",
      "Average loss in Epoch 574:0.6958\n",
      "Average loss in Epoch 575:0.6845\n",
      "Average loss in Epoch 576:0.6792\n",
      "Average loss in Epoch 577:0.6812\n",
      "Average loss in Epoch 578:0.6847\n",
      "Average loss in Epoch 579:0.6778\n",
      "Average loss in Epoch 580:0.6766\n",
      "Average loss in Epoch 581:0.6740\n",
      "Average loss in Epoch 582:0.6645\n",
      "Average loss in Epoch 583:0.6733\n",
      "Average loss in Epoch 584:0.6756\n",
      "Average loss in Epoch 585:0.6588\n",
      "Average loss in Epoch 586:0.6645\n",
      "Average loss in Epoch 587:0.6624\n",
      "Average loss in Epoch 588:0.6605\n",
      "Average loss in Epoch 589:0.6558\n",
      "Average loss in Epoch 590:0.6474\n",
      "Average loss in Epoch 591:0.6578\n",
      "Average loss in Epoch 592:0.6496\n",
      "Average loss in Epoch 593:0.6437\n",
      "Average loss in Epoch 594:0.6564\n",
      "Average loss in Epoch 595:0.6474\n",
      "Average loss in Epoch 596:0.6495\n",
      "Average loss in Epoch 597:0.6336\n",
      "Average loss in Epoch 598:0.6287\n",
      "Average loss in Epoch 599:0.6281\n",
      "Average loss in Epoch 600:0.6431\n",
      "Average loss in Epoch 601:0.6411\n",
      "Average loss in Epoch 602:0.6295\n",
      "Average loss in Epoch 603:0.6334\n",
      "Average loss in Epoch 604:0.6167\n",
      "Average loss in Epoch 605:0.6248\n",
      "Average loss in Epoch 606:0.6235\n",
      "Average loss in Epoch 607:0.6160\n",
      "Average loss in Epoch 608:0.6096\n",
      "Average loss in Epoch 609:0.6174\n",
      "Average loss in Epoch 610:0.6067\n",
      "Average loss in Epoch 611:0.6109\n",
      "Average loss in Epoch 612:0.6017\n",
      "Average loss in Epoch 613:0.6016\n",
      "Average loss in Epoch 614:0.6063\n",
      "Average loss in Epoch 615:0.6048\n",
      "Average loss in Epoch 616:0.5920\n",
      "Average loss in Epoch 617:0.6058\n",
      "Average loss in Epoch 618:0.5956\n",
      "Average loss in Epoch 619:0.5955\n",
      "Average loss in Epoch 620:0.5916\n",
      "Average loss in Epoch 621:0.5966\n",
      "Average loss in Epoch 622:0.5935\n",
      "Average loss in Epoch 623:0.5909\n",
      "Average loss in Epoch 624:0.5871\n",
      "Average loss in Epoch 625:0.5858\n",
      "Average loss in Epoch 626:0.5809\n",
      "Average loss in Epoch 627:0.5854\n",
      "Average loss in Epoch 628:0.5769\n",
      "Average loss in Epoch 629:0.5754\n",
      "Average loss in Epoch 630:0.5700\n",
      "Average loss in Epoch 631:0.5703\n",
      "Average loss in Epoch 632:0.5729\n",
      "Average loss in Epoch 633:0.5728\n",
      "Average loss in Epoch 634:0.5789\n",
      "Average loss in Epoch 635:0.5729\n",
      "Average loss in Epoch 636:0.5731\n",
      "Average loss in Epoch 637:0.5696\n",
      "Average loss in Epoch 638:0.5661\n",
      "Average loss in Epoch 639:0.5650\n",
      "Average loss in Epoch 640:0.5596\n",
      "Average loss in Epoch 641:0.5626\n",
      "Average loss in Epoch 642:0.5577\n",
      "Average loss in Epoch 643:0.5564\n",
      "Average loss in Epoch 644:0.5582\n",
      "Average loss in Epoch 645:0.5553\n",
      "Average loss in Epoch 646:0.5533\n",
      "Average loss in Epoch 647:0.5494\n",
      "Average loss in Epoch 648:0.5441\n",
      "Average loss in Epoch 649:0.5435\n",
      "Average loss in Epoch 650:0.5443\n",
      "Average loss in Epoch 651:0.5459\n",
      "Average loss in Epoch 652:0.5373\n",
      "Average loss in Epoch 653:0.5403\n",
      "Average loss in Epoch 654:0.5380\n",
      "Average loss in Epoch 655:0.5433\n",
      "Average loss in Epoch 656:0.5347\n",
      "Average loss in Epoch 657:0.5267\n",
      "Average loss in Epoch 658:0.5306\n",
      "Average loss in Epoch 659:0.5305\n",
      "Average loss in Epoch 660:0.5295\n",
      "Average loss in Epoch 661:0.5303\n",
      "Average loss in Epoch 662:0.5331\n",
      "Average loss in Epoch 663:0.5260\n",
      "Average loss in Epoch 664:0.5231\n",
      "Average loss in Epoch 665:0.5313\n",
      "Average loss in Epoch 666:0.5166\n",
      "Average loss in Epoch 667:0.5191\n",
      "Average loss in Epoch 668:0.5232\n",
      "Average loss in Epoch 669:0.5225\n",
      "Average loss in Epoch 670:0.5096\n",
      "Average loss in Epoch 671:0.5186\n",
      "Average loss in Epoch 672:0.5124\n",
      "Average loss in Epoch 673:0.5142\n",
      "Average loss in Epoch 674:0.5170\n",
      "Average loss in Epoch 675:0.5080\n",
      "Average loss in Epoch 676:0.5094\n",
      "Average loss in Epoch 677:0.5068\n",
      "Average loss in Epoch 678:0.5036\n",
      "Average loss in Epoch 679:0.5057\n",
      "Average loss in Epoch 680:0.5037\n",
      "Average loss in Epoch 681:0.5035\n",
      "Average loss in Epoch 682:0.5059\n",
      "Average loss in Epoch 683:0.5027\n",
      "Average loss in Epoch 684:0.4933\n",
      "Average loss in Epoch 685:0.4938\n",
      "Average loss in Epoch 686:0.4976\n",
      "Average loss in Epoch 687:0.4937\n",
      "Average loss in Epoch 688:0.4957\n",
      "Average loss in Epoch 689:0.4898\n",
      "Average loss in Epoch 690:0.4962\n",
      "Average loss in Epoch 691:0.4870\n",
      "Average loss in Epoch 692:0.4910\n",
      "Average loss in Epoch 693:0.4849\n",
      "Average loss in Epoch 694:0.4822\n",
      "Average loss in Epoch 695:0.4835\n",
      "Average loss in Epoch 696:0.4773\n",
      "Average loss in Epoch 697:0.4820\n",
      "Average loss in Epoch 698:0.4755\n",
      "Average loss in Epoch 699:0.4778\n",
      "Average loss in Epoch 700:0.4770\n",
      "Average loss in Epoch 701:0.4858\n",
      "Average loss in Epoch 702:0.4758\n",
      "Average loss in Epoch 703:0.4778\n",
      "Average loss in Epoch 704:0.4779\n",
      "Average loss in Epoch 705:0.4759\n",
      "Average loss in Epoch 706:0.4745\n",
      "Average loss in Epoch 707:0.4711\n",
      "Average loss in Epoch 708:0.4727\n",
      "Average loss in Epoch 709:0.4629\n",
      "Average loss in Epoch 710:0.4681\n",
      "Average loss in Epoch 711:0.4624\n",
      "Average loss in Epoch 712:0.4610\n",
      "Average loss in Epoch 713:0.4621\n",
      "Average loss in Epoch 714:0.4601\n",
      "Average loss in Epoch 715:0.4720\n",
      "Average loss in Epoch 716:0.4612\n",
      "Average loss in Epoch 717:0.4571\n",
      "Average loss in Epoch 718:0.4549\n",
      "Average loss in Epoch 719:0.4579\n",
      "Average loss in Epoch 720:0.4491\n",
      "Average loss in Epoch 721:0.4481\n",
      "Average loss in Epoch 722:0.4577\n",
      "Average loss in Epoch 723:0.4554\n",
      "Average loss in Epoch 724:0.4444\n",
      "Average loss in Epoch 725:0.4479\n",
      "Average loss in Epoch 726:0.4457\n",
      "Average loss in Epoch 727:0.4486\n",
      "Average loss in Epoch 728:0.4491\n",
      "Average loss in Epoch 729:0.4517\n",
      "Average loss in Epoch 730:0.4443\n",
      "Average loss in Epoch 731:0.4467\n",
      "Average loss in Epoch 732:0.4445\n",
      "Average loss in Epoch 733:0.4417\n",
      "Average loss in Epoch 734:0.4312\n",
      "Average loss in Epoch 735:0.4363\n",
      "Average loss in Epoch 736:0.4468\n",
      "Average loss in Epoch 737:0.4321\n",
      "Average loss in Epoch 738:0.4393\n",
      "Average loss in Epoch 739:0.4329\n",
      "Average loss in Epoch 740:0.4407\n",
      "Average loss in Epoch 741:0.4313\n",
      "Average loss in Epoch 742:0.4309\n",
      "Average loss in Epoch 743:0.4325\n",
      "Average loss in Epoch 744:0.4290\n",
      "Average loss in Epoch 745:0.4282\n",
      "Average loss in Epoch 746:0.4288\n",
      "Average loss in Epoch 747:0.4270\n",
      "Average loss in Epoch 748:0.4230\n",
      "Average loss in Epoch 749:0.4219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss in Epoch 750:0.4288\n",
      "Average loss in Epoch 751:0.4188\n",
      "Average loss in Epoch 752:0.4200\n",
      "Average loss in Epoch 753:0.4231\n",
      "Average loss in Epoch 754:0.4162\n",
      "Average loss in Epoch 755:0.4240\n",
      "Average loss in Epoch 756:0.4142\n",
      "Average loss in Epoch 757:0.4252\n",
      "Average loss in Epoch 758:0.4138\n",
      "Average loss in Epoch 759:0.4199\n",
      "Average loss in Epoch 760:0.4182\n",
      "Average loss in Epoch 761:0.4178\n",
      "Average loss in Epoch 762:0.4156\n",
      "Average loss in Epoch 763:0.4046\n",
      "Average loss in Epoch 764:0.4104\n",
      "Average loss in Epoch 765:0.4056\n",
      "Average loss in Epoch 766:0.4162\n",
      "Average loss in Epoch 767:0.4123\n",
      "Average loss in Epoch 768:0.4125\n",
      "Average loss in Epoch 769:0.4045\n",
      "Average loss in Epoch 770:0.3963\n",
      "Average loss in Epoch 771:0.4116\n",
      "Average loss in Epoch 772:0.3996\n",
      "Average loss in Epoch 773:0.4109\n",
      "Average loss in Epoch 774:0.4093\n",
      "Average loss in Epoch 775:0.4103\n",
      "Average loss in Epoch 776:0.3935\n",
      "Average loss in Epoch 777:0.4050\n",
      "Average loss in Epoch 778:0.3953\n",
      "Average loss in Epoch 779:0.4056\n",
      "Average loss in Epoch 780:0.3965\n",
      "Average loss in Epoch 781:0.3950\n",
      "Average loss in Epoch 782:0.3954\n",
      "Average loss in Epoch 783:0.3972\n",
      "Average loss in Epoch 784:0.3903\n",
      "Average loss in Epoch 785:0.3875\n",
      "Average loss in Epoch 786:0.3933\n",
      "Average loss in Epoch 787:0.3872\n",
      "Average loss in Epoch 788:0.3983\n",
      "Average loss in Epoch 789:0.3901\n",
      "Average loss in Epoch 790:0.3910\n",
      "Average loss in Epoch 791:0.3917\n",
      "Average loss in Epoch 792:0.3936\n",
      "Average loss in Epoch 793:0.3863\n",
      "Average loss in Epoch 794:0.3818\n",
      "Average loss in Epoch 795:0.3829\n",
      "Average loss in Epoch 796:0.3901\n",
      "Average loss in Epoch 797:0.3883\n",
      "Average loss in Epoch 798:0.3858\n",
      "Average loss in Epoch 799:0.3869\n",
      "Average loss in Epoch 800:0.3855\n",
      "Average loss in Epoch 801:0.3835\n",
      "Average loss in Epoch 802:0.3830\n",
      "Average loss in Epoch 803:0.3836\n",
      "Average loss in Epoch 804:0.3860\n",
      "Average loss in Epoch 805:0.3771\n",
      "Average loss in Epoch 806:0.3780\n",
      "Average loss in Epoch 807:0.3781\n",
      "Average loss in Epoch 808:0.3732\n",
      "Average loss in Epoch 809:0.3720\n",
      "Average loss in Epoch 810:0.3735\n",
      "Average loss in Epoch 811:0.3696\n",
      "Average loss in Epoch 812:0.3700\n",
      "Average loss in Epoch 813:0.3733\n",
      "Average loss in Epoch 814:0.3712\n",
      "Average loss in Epoch 815:0.3687\n",
      "Average loss in Epoch 816:0.3664\n",
      "Average loss in Epoch 817:0.3725\n",
      "Average loss in Epoch 818:0.3664\n",
      "Average loss in Epoch 819:0.3655\n",
      "Average loss in Epoch 820:0.3637\n",
      "Average loss in Epoch 821:0.3749\n",
      "Average loss in Epoch 822:0.3663\n",
      "Average loss in Epoch 823:0.3674\n",
      "Average loss in Epoch 824:0.3652\n",
      "Average loss in Epoch 825:0.3674\n",
      "Average loss in Epoch 826:0.3599\n",
      "Average loss in Epoch 827:0.3634\n",
      "Average loss in Epoch 828:0.3546\n",
      "Average loss in Epoch 829:0.3621\n",
      "Average loss in Epoch 830:0.3646\n",
      "Average loss in Epoch 831:0.3658\n",
      "Average loss in Epoch 832:0.3589\n",
      "Average loss in Epoch 833:0.3558\n",
      "Average loss in Epoch 834:0.3584\n",
      "Average loss in Epoch 835:0.3567\n",
      "Average loss in Epoch 836:0.3561\n",
      "Average loss in Epoch 837:0.3624\n",
      "Average loss in Epoch 838:0.3603\n",
      "Average loss in Epoch 839:0.3500\n",
      "Average loss in Epoch 840:0.3535\n",
      "Average loss in Epoch 841:0.3531\n",
      "Average loss in Epoch 842:0.3520\n",
      "Average loss in Epoch 843:0.3505\n",
      "Average loss in Epoch 844:0.3537\n",
      "Average loss in Epoch 845:0.3526\n",
      "Average loss in Epoch 846:0.3534\n",
      "Average loss in Epoch 847:0.3476\n",
      "Average loss in Epoch 848:0.3479\n",
      "Average loss in Epoch 849:0.3553\n",
      "Average loss in Epoch 850:0.3484\n",
      "Average loss in Epoch 851:0.3489\n",
      "Average loss in Epoch 852:0.3502\n",
      "Average loss in Epoch 853:0.3564\n",
      "Average loss in Epoch 854:0.3485\n",
      "Average loss in Epoch 855:0.3430\n",
      "Average loss in Epoch 856:0.3443\n",
      "Average loss in Epoch 857:0.3484\n",
      "Average loss in Epoch 858:0.3462\n",
      "Average loss in Epoch 859:0.3403\n",
      "Average loss in Epoch 860:0.3432\n",
      "Average loss in Epoch 861:0.3410\n",
      "Average loss in Epoch 862:0.3438\n",
      "Average loss in Epoch 863:0.3393\n",
      "Average loss in Epoch 864:0.3393\n",
      "Average loss in Epoch 865:0.3361\n",
      "Average loss in Epoch 866:0.3365\n",
      "Average loss in Epoch 867:0.3335\n",
      "Average loss in Epoch 868:0.3416\n",
      "Average loss in Epoch 869:0.3320\n",
      "Average loss in Epoch 870:0.3400\n",
      "Average loss in Epoch 871:0.3371\n",
      "Average loss in Epoch 872:0.3377\n",
      "Average loss in Epoch 873:0.3310\n",
      "Average loss in Epoch 874:0.3346\n",
      "Average loss in Epoch 875:0.3293\n",
      "Average loss in Epoch 876:0.3324\n",
      "Average loss in Epoch 877:0.3314\n",
      "Average loss in Epoch 878:0.3282\n",
      "Average loss in Epoch 879:0.3307\n",
      "Average loss in Epoch 880:0.3348\n",
      "Average loss in Epoch 881:0.3279\n",
      "Average loss in Epoch 882:0.3273\n",
      "Average loss in Epoch 883:0.3320\n",
      "Average loss in Epoch 884:0.3352\n",
      "Average loss in Epoch 885:0.3286\n",
      "Average loss in Epoch 886:0.3294\n",
      "Average loss in Epoch 887:0.3252\n",
      "Average loss in Epoch 888:0.3222\n",
      "Average loss in Epoch 889:0.3312\n",
      "Average loss in Epoch 890:0.3281\n",
      "Average loss in Epoch 891:0.3245\n",
      "Average loss in Epoch 892:0.3261\n",
      "Average loss in Epoch 893:0.3189\n",
      "Average loss in Epoch 894:0.3303\n",
      "Average loss in Epoch 895:0.3191\n",
      "Average loss in Epoch 896:0.3199\n",
      "Average loss in Epoch 897:0.3231\n",
      "Average loss in Epoch 898:0.3195\n",
      "Average loss in Epoch 899:0.3202\n",
      "Average loss in Epoch 900:0.3180\n",
      "Average loss in Epoch 901:0.3165\n",
      "Average loss in Epoch 902:0.3238\n",
      "Average loss in Epoch 903:0.3235\n",
      "Average loss in Epoch 904:0.3184\n",
      "Average loss in Epoch 905:0.3167\n",
      "Average loss in Epoch 906:0.3171\n",
      "Average loss in Epoch 907:0.3190\n",
      "Average loss in Epoch 908:0.3198\n",
      "Average loss in Epoch 909:0.3166\n",
      "Average loss in Epoch 910:0.3243\n",
      "Average loss in Epoch 911:0.3163\n",
      "Average loss in Epoch 912:0.3117\n",
      "Average loss in Epoch 913:0.3168\n",
      "Average loss in Epoch 914:0.3141\n",
      "Average loss in Epoch 915:0.3143\n",
      "Average loss in Epoch 916:0.3086\n",
      "Average loss in Epoch 917:0.3078\n",
      "Average loss in Epoch 918:0.3134\n",
      "Average loss in Epoch 919:0.3150\n",
      "Average loss in Epoch 920:0.3182\n",
      "Average loss in Epoch 921:0.3132\n",
      "Average loss in Epoch 922:0.3150\n",
      "Average loss in Epoch 923:0.3078\n",
      "Average loss in Epoch 924:0.3109\n",
      "Average loss in Epoch 925:0.3090\n",
      "Average loss in Epoch 926:0.3118\n",
      "Average loss in Epoch 927:0.3079\n",
      "Average loss in Epoch 928:0.3083\n",
      "Average loss in Epoch 929:0.3096\n",
      "Average loss in Epoch 930:0.3063\n",
      "Average loss in Epoch 931:0.3119\n",
      "Average loss in Epoch 932:0.3143\n",
      "Average loss in Epoch 933:0.3107\n",
      "Average loss in Epoch 934:0.2976\n",
      "Average loss in Epoch 935:0.3087\n",
      "Average loss in Epoch 936:0.3084\n",
      "Average loss in Epoch 937:0.3056\n",
      "Average loss in Epoch 938:0.3124\n",
      "Average loss in Epoch 939:0.3036\n",
      "Average loss in Epoch 940:0.3016\n",
      "Average loss in Epoch 941:0.3052\n",
      "Average loss in Epoch 942:0.3017\n",
      "Average loss in Epoch 943:0.3005\n",
      "Average loss in Epoch 944:0.3019\n",
      "Average loss in Epoch 945:0.3017\n",
      "Average loss in Epoch 946:0.3044\n",
      "Average loss in Epoch 947:0.2991\n",
      "Average loss in Epoch 948:0.2980\n",
      "Average loss in Epoch 949:0.3007\n",
      "Average loss in Epoch 950:0.3016\n",
      "Average loss in Epoch 951:0.2984\n",
      "Average loss in Epoch 952:0.2982\n",
      "Average loss in Epoch 953:0.2987\n",
      "Average loss in Epoch 954:0.3043\n",
      "Average loss in Epoch 955:0.2939\n",
      "Average loss in Epoch 956:0.2988\n",
      "Average loss in Epoch 957:0.2984\n",
      "Average loss in Epoch 958:0.2994\n",
      "Average loss in Epoch 959:0.2955\n",
      "Average loss in Epoch 960:0.2974\n",
      "Average loss in Epoch 961:0.2948\n",
      "Average loss in Epoch 962:0.2995\n",
      "Average loss in Epoch 963:0.2926\n",
      "Average loss in Epoch 964:0.3029\n",
      "Average loss in Epoch 965:0.2954\n",
      "Average loss in Epoch 966:0.2945\n",
      "Average loss in Epoch 967:0.2931\n",
      "Average loss in Epoch 968:0.2969\n",
      "Average loss in Epoch 969:0.2899\n",
      "Average loss in Epoch 970:0.2963\n",
      "Average loss in Epoch 971:0.2842\n",
      "Average loss in Epoch 972:0.2918\n",
      "Average loss in Epoch 973:0.2853\n",
      "Average loss in Epoch 974:0.2958\n",
      "Average loss in Epoch 975:0.2914\n",
      "Average loss in Epoch 976:0.2941\n",
      "Average loss in Epoch 977:0.2929\n",
      "Average loss in Epoch 978:0.2890\n",
      "Average loss in Epoch 979:0.2908\n",
      "Average loss in Epoch 980:0.2950\n",
      "Average loss in Epoch 981:0.2915\n",
      "Average loss in Epoch 982:0.2855\n",
      "Average loss in Epoch 983:0.2919\n",
      "Average loss in Epoch 984:0.2895\n",
      "Average loss in Epoch 985:0.2838\n",
      "Average loss in Epoch 986:0.2804\n",
      "Average loss in Epoch 987:0.2883\n",
      "Average loss in Epoch 988:0.2842\n",
      "Average loss in Epoch 989:0.2921\n",
      "Average loss in Epoch 990:0.2854\n",
      "Average loss in Epoch 991:0.2853\n",
      "Average loss in Epoch 992:0.2815\n",
      "Average loss in Epoch 993:0.2808\n",
      "Average loss in Epoch 994:0.2907\n",
      "Average loss in Epoch 995:0.2868\n",
      "Average loss in Epoch 996:0.2788\n",
      "Average loss in Epoch 997:0.2790\n",
      "Average loss in Epoch 998:0.2859\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss in Epoch 999:0.2878\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[7.4219515228271487,\n",
       " 7.2522882080078128,\n",
       " 6.3040896224975587,\n",
       " 5.8651046371459961,\n",
       " 5.7037962150573733,\n",
       " 5.6177187156677242,\n",
       " 5.5626495552062991,\n",
       " 5.5042684936523436,\n",
       " 5.4660033988952641,\n",
       " 5.4350634002685547,\n",
       " 5.4031640243530275,\n",
       " 5.3941654968261723,\n",
       " 5.3863291168212895,\n",
       " 5.3781476402282715,\n",
       " 5.3706502914428711,\n",
       " 5.3738175201416016,\n",
       " 5.3687067985534664,\n",
       " 5.363705978393555,\n",
       " 5.3496961021423344,\n",
       " 5.3517421722412113,\n",
       " 5.3559330368041991,\n",
       " 5.3520964050292967,\n",
       " 5.3474118423461912,\n",
       " 5.3442337226867673,\n",
       " 5.348981075286865,\n",
       " 5.3451544761657717,\n",
       " 5.3417949867248531,\n",
       " 5.3397066879272463,\n",
       " 5.3367014312744141,\n",
       " 5.3357400131225585,\n",
       " 5.3378387641906739,\n",
       " 5.3361295890808105,\n",
       " 5.3369355201721191,\n",
       " 5.3381499671936039,\n",
       " 5.3326733398437502,\n",
       " 5.3328661346435551,\n",
       " 5.3283170890808105,\n",
       " 5.3248631286621091,\n",
       " 5.3219636726379393,\n",
       " 5.3212178421020511,\n",
       " 5.3222626304626468,\n",
       " 5.3177423667907711,\n",
       " 5.3164740753173829,\n",
       " 5.3191093254089354,\n",
       " 5.313106460571289,\n",
       " 5.3230368423461911,\n",
       " 5.321018161773682,\n",
       " 5.3181517219543455,\n",
       " 5.3211429595947264,\n",
       " 5.319275531768799,\n",
       " 5.3173830032348635,\n",
       " 5.31116678237915,\n",
       " 5.3103356742858887,\n",
       " 5.3034815406799316,\n",
       " 5.3108867454528808,\n",
       " 5.302696571350098,\n",
       " 5.2989589118957516,\n",
       " 5.3043462944030759,\n",
       " 5.2988730621337892,\n",
       " 5.2974835586547853,\n",
       " 5.2936427879333499,\n",
       " 5.3021378707885738,\n",
       " 5.2966189002990722,\n",
       " 5.293299789428711,\n",
       " 5.2890108108520506,\n",
       " 5.2902630424499515,\n",
       " 5.286819648742676,\n",
       " 5.2867111778259277,\n",
       " 5.2830756759643558,\n",
       " 5.28293098449707,\n",
       " 5.2790303421020504,\n",
       " 5.2853621101379398,\n",
       " 5.2750251770019529,\n",
       " 5.2771180915832518,\n",
       " 5.2803454208374028,\n",
       " 5.2785556793212889,\n",
       " 5.2737047004699704,\n",
       " 5.2672673416137696,\n",
       " 5.268965358734131,\n",
       " 5.2619970130920413,\n",
       " 5.2626320838928224,\n",
       " 5.2634485435485843,\n",
       " 5.2522424316406253,\n",
       " 5.2531357574462891,\n",
       " 5.2403148078918456,\n",
       " 5.2280401802062988,\n",
       " 5.2049080848693849,\n",
       " 5.1537035369873045,\n",
       " 5.1055706596374515,\n",
       " 5.0824388694763183,\n",
       " 5.0555272674560543,\n",
       " 5.0175543212890625,\n",
       " 4.9833305168151858,\n",
       " 4.9611962890625003,\n",
       " 4.9354489898681644,\n",
       " 4.9104783630371092,\n",
       " 4.8916697311401371,\n",
       " 4.8607025718688961,\n",
       " 4.8342938041687011,\n",
       " 4.8108529853820805,\n",
       " 4.7911985778808592,\n",
       " 4.7674670028686528,\n",
       " 4.7503491973876955,\n",
       " 4.7176898193359378,\n",
       " 4.6933253669738768,\n",
       " 4.6707031059265134,\n",
       " 4.6570101356506344,\n",
       " 4.6308870124816899,\n",
       " 4.6217588424682621,\n",
       " 4.6108838272094728,\n",
       " 4.5859325790405272,\n",
       " 4.5648503112792973,\n",
       " 4.5602586174011233,\n",
       " 4.5321692657470702,\n",
       " 4.5051661872863766,\n",
       " 4.4853240776062009,\n",
       " 4.4570777320861819,\n",
       " 4.4398205947875979,\n",
       " 4.4135316658020018,\n",
       " 4.3989411163330079,\n",
       " 4.3821423149108885,\n",
       " 4.3628328895568851,\n",
       " 4.3475307083129886,\n",
       " 4.3358647346496584,\n",
       " 4.3282217025756839,\n",
       " 4.3345611381530764,\n",
       " 4.3579463195800781,\n",
       " 4.3232715415954592,\n",
       " 4.2768715381622311,\n",
       " 4.2433920669555665,\n",
       " 4.208962898254395,\n",
       " 4.1851071643829343,\n",
       " 4.1621840953826901,\n",
       " 4.1391857624053952,\n",
       " 4.1114749431610109,\n",
       " 4.0997066402435305,\n",
       " 4.0843956565856931,\n",
       " 4.0579500293731687,\n",
       " 4.0396678352355959,\n",
       " 4.020431957244873,\n",
       " 4.0046152877807621,\n",
       " 3.9910600185394287,\n",
       " 3.9716691398620605,\n",
       " 3.96199613571167,\n",
       " 3.9369805622100831,\n",
       " 3.9177248191833498,\n",
       " 3.90478967666626,\n",
       " 3.8808175468444825,\n",
       " 3.8591954994201658,\n",
       " 3.8489428138732911,\n",
       " 3.8293534469604493,\n",
       " 3.8021510314941405,\n",
       " 3.7961954689025879,\n",
       " 3.7731093502044679,\n",
       " 3.7594190597534181,\n",
       " 3.7468434715270997,\n",
       " 3.7319717693328855,\n",
       " 3.7207266235351564,\n",
       " 3.7034007835388185,\n",
       " 3.6917348098754883,\n",
       " 3.6776899719238281,\n",
       " 3.6658599853515623,\n",
       " 3.6622518634796144,\n",
       " 3.6431365776062012,\n",
       " 3.6326937961578367,\n",
       " 3.6188787460327148,\n",
       " 3.6060315895080568,\n",
       " 3.6050550651550295,\n",
       " 3.598004493713379,\n",
       " 3.5761384010314941,\n",
       " 3.5632599639892577,\n",
       " 3.5591049194335938,\n",
       " 3.5492375564575194,\n",
       " 3.5424580669403074,\n",
       " 3.5321385478973388,\n",
       " 3.5036656856536865,\n",
       " 3.471201572418213,\n",
       " 3.4616332721710203,\n",
       " 3.4605866527557372,\n",
       " 3.460476589202881,\n",
       " 3.4606024551391603,\n",
       " 3.4705208873748781,\n",
       " 3.4831344795227053,\n",
       " 3.4834627437591554,\n",
       " 3.44702374458313,\n",
       " 3.3925464153289795,\n",
       " 3.3664345932006836,\n",
       " 3.3253046894073486,\n",
       " 3.2881128787994385,\n",
       " 3.2781845188140868,\n",
       " 3.2565576648712158,\n",
       " 3.2529921150207519,\n",
       " 3.2319355392456055,\n",
       " 3.2183564567565917,\n",
       " 3.2018092918395995,\n",
       " 3.1896564388275146,\n",
       " 3.1716005611419678,\n",
       " 3.1643027687072753,\n",
       " 3.1487274074554445,\n",
       " 3.1356486701965331,\n",
       " 3.1157417201995852,\n",
       " 3.0977402877807618,\n",
       " 3.0919242668151856,\n",
       " 3.0831052970886232,\n",
       " 3.0645174884796145,\n",
       " 3.0521459102630617,\n",
       " 3.0327746677398681,\n",
       " 3.0192034912109373,\n",
       " 3.0064555454254149,\n",
       " 3.0040524482727049,\n",
       " 2.9807499504089354,\n",
       " 2.9632817363739012,\n",
       " 2.9578182029724123,\n",
       " 2.9517002964019774,\n",
       " 2.9271765327453614,\n",
       " 2.9108579730987549,\n",
       " 2.9133221817016604,\n",
       " 2.8975312328338623,\n",
       " 2.8885503101348875,\n",
       " 2.8753111648559568,\n",
       " 2.8480674839019775,\n",
       " 2.8444209289550781,\n",
       " 2.8379280757904053,\n",
       " 2.8344068908691407,\n",
       " 2.8181409740448,\n",
       " 2.8049793148040774,\n",
       " 2.7914345932006834,\n",
       " 2.7987214756011962,\n",
       " 2.7746553707122801,\n",
       " 2.746670503616333,\n",
       " 2.7356838417053222,\n",
       " 2.7139835453033445,\n",
       " 2.7023203563690186,\n",
       " 2.6916589641571047,\n",
       " 2.6913886642456055,\n",
       " 2.6745100784301759,\n",
       " 2.6604130268096924,\n",
       " 2.6405217170715334,\n",
       " 2.6379867458343504,\n",
       " 2.6191496276855468,\n",
       " 2.6100486946105956,\n",
       " 2.6049034404754638,\n",
       " 2.5874997043609618,\n",
       " 2.5809096240997316,\n",
       " 2.5841145324707031,\n",
       " 2.5639254760742189,\n",
       " 2.5422860908508302,\n",
       " 2.5350008201599121,\n",
       " 2.5180700397491456,\n",
       " 2.511121835708618,\n",
       " 2.5017394161224367,\n",
       " 2.5025019168853762,\n",
       " 2.4860934543609621,\n",
       " 2.4790948772430421,\n",
       " 2.465897550582886,\n",
       " 2.451962947845459,\n",
       " 2.4376078033447266,\n",
       " 2.4305433177947999,\n",
       " 2.4267593288421629,\n",
       " 2.4110361766815185,\n",
       " 2.4051386451721193,\n",
       " 2.393981809616089,\n",
       " 2.3972995185852053,\n",
       " 2.383589506149292,\n",
       " 2.3646013641357424,\n",
       " 2.360757579803467,\n",
       " 2.3538169574737551,\n",
       " 2.3326984882354735,\n",
       " 2.3318836975097654,\n",
       " 2.3138468933105467,\n",
       " 2.2976692390441893,\n",
       " 2.2952007579803468,\n",
       " 2.2953885841369628,\n",
       " 2.2807683372497558,\n",
       " 2.2674162101745607,\n",
       " 2.2675118589401246,\n",
       " 2.2629179763793945,\n",
       " 2.2761137866973877,\n",
       " 2.2385297679901122,\n",
       " 2.2578846836090087,\n",
       " 2.2560619831085207,\n",
       " 2.2691495656967162,\n",
       " 2.301176223754883,\n",
       " 2.3131343650817873,\n",
       " 2.327207145690918,\n",
       " 2.290729913711548,\n",
       " 2.2718424415588379,\n",
       " 2.2339024925231934,\n",
       " 2.17832745552063,\n",
       " 2.143226776123047,\n",
       " 2.1329704713821411,\n",
       " 2.1023360824584962,\n",
       " 2.0963795757293702,\n",
       " 2.0857515907287598,\n",
       " 2.0832984256744385,\n",
       " 2.0720099544525148,\n",
       " 2.0658586359024049,\n",
       " 2.0478213930130007,\n",
       " 2.0307605028152467,\n",
       " 2.0275553369522097,\n",
       " 2.0166083717346193,\n",
       " 1.9957348108291626,\n",
       " 1.9970235824584961,\n",
       " 1.9865092945098877,\n",
       " 1.9830555772781373,\n",
       " 1.9734526681900024,\n",
       " 1.9611696577072144,\n",
       " 1.9431803369522094,\n",
       " 1.9512471866607666,\n",
       " 1.9454397869110107,\n",
       " 1.9284217834472657,\n",
       " 1.923285083770752,\n",
       " 1.9147024536132813,\n",
       " 1.8942676305770874,\n",
       " 1.8992830038070678,\n",
       " 1.8933827686309814,\n",
       " 1.8861792850494385,\n",
       " 1.8624504375457764,\n",
       " 1.8682604789733888,\n",
       " 1.8556955385208129,\n",
       " 1.84384117603302,\n",
       " 1.8437128019332887,\n",
       " 1.8379846954345702,\n",
       " 1.8273275327682494,\n",
       " 1.8131462478637694,\n",
       " 1.8170683240890504,\n",
       " 1.8055346250534057,\n",
       " 1.796305718421936,\n",
       " 1.7884499549865722,\n",
       " 1.7793174314498901,\n",
       " 1.7704725122451783,\n",
       " 1.770495252609253,\n",
       " 1.7652506828308105,\n",
       " 1.7616181421279906,\n",
       " 1.7430472278594971,\n",
       " 1.7354702377319335,\n",
       " 1.7267734384536744,\n",
       " 1.7204158449172973,\n",
       " 1.7137876272201538,\n",
       " 1.7012407445907594,\n",
       " 1.6952057075500488,\n",
       " 1.6961644172668457,\n",
       " 1.683809289932251,\n",
       " 1.6889434957504272,\n",
       " 1.6722156667709351,\n",
       " 1.6627859687805175,\n",
       " 1.6625272560119628,\n",
       " 1.6574387788772582,\n",
       " 1.6477398347854615,\n",
       " 1.6325722742080688,\n",
       " 1.6314889812469482,\n",
       " 1.6161377286911012,\n",
       " 1.627946262359619,\n",
       " 1.6158058404922486,\n",
       " 1.6003277206420898,\n",
       " 1.6014070892333985,\n",
       " 1.5975619125366212,\n",
       " 1.5840102100372315,\n",
       " 1.5790524148941041,\n",
       " 1.5741683673858642,\n",
       " 1.5555965280532837,\n",
       " 1.5456484603881835,\n",
       " 1.5617170906066895,\n",
       " 1.5454832458496093,\n",
       " 1.5364281082153319,\n",
       " 1.5315227508544922,\n",
       " 1.5257085657119751,\n",
       " 1.5137524890899658,\n",
       " 1.5172899866104126,\n",
       " 1.5104884147644042,\n",
       " 1.492146716117859,\n",
       " 1.4848684549331665,\n",
       " 1.4840792083740235,\n",
       " 1.4751900100708009,\n",
       " 1.4681717777252197,\n",
       " 1.4595151281356811,\n",
       " 1.4510895395278931,\n",
       " 1.4486556196212768,\n",
       " 1.4428212022781373,\n",
       " 1.4408878946304322,\n",
       " 1.4344950056076049,\n",
       " 1.4308190965652465,\n",
       " 1.4185532999038697,\n",
       " 1.4160201168060302,\n",
       " 1.4098495483398437,\n",
       " 1.4010228157043456,\n",
       " 1.3869899749755858,\n",
       " 1.3835951757431031,\n",
       " 1.3863823413848877,\n",
       " 1.3779488277435303,\n",
       " 1.382753472328186,\n",
       " 1.3673871755599976,\n",
       " 1.3595058536529541,\n",
       " 1.3502098131179809,\n",
       " 1.3475465106964111,\n",
       " 1.3567670965194703,\n",
       " 1.3456511163711549,\n",
       " 1.3407988548278809,\n",
       " 1.3339957094192505,\n",
       " 1.3242017745971679,\n",
       " 1.3238155031204224,\n",
       " 1.320154480934143,\n",
       " 1.3066350269317626,\n",
       " 1.2958979558944703,\n",
       " 1.2912633228302002,\n",
       " 1.2926551580429078,\n",
       " 1.2977661752700806,\n",
       " 1.2855907154083253,\n",
       " 1.2871527242660523,\n",
       " 1.276865200996399,\n",
       " 1.2732306051254272,\n",
       " 1.2573279857635498,\n",
       " 1.2593873596191407,\n",
       " 1.260097360610962,\n",
       " 1.2542322587966919,\n",
       " 1.2380731391906739,\n",
       " 1.237799472808838,\n",
       " 1.2399390602111817,\n",
       " 1.2288123512268065,\n",
       " 1.221318621635437,\n",
       " 1.2219255828857423,\n",
       " 1.2246672821044922,\n",
       " 1.2149463844299317,\n",
       " 1.2043276882171632,\n",
       " 1.2025563955307006,\n",
       " 1.1953547286987305,\n",
       " 1.1934175729751586,\n",
       " 1.1836629390716553,\n",
       " 1.1724191164970399,\n",
       " 1.1804025292396545,\n",
       " 1.1812584924697875,\n",
       " 1.1785736465454102,\n",
       " 1.1616081237792968,\n",
       " 1.1578515696525573,\n",
       " 1.1714923930168153,\n",
       " 1.1660025143623352,\n",
       " 1.1605455160140992,\n",
       " 1.1421966218948365,\n",
       " 1.1459559154510499,\n",
       " 1.1428781962394714,\n",
       " 1.1315576529502869,\n",
       " 1.1318923878669738,\n",
       " 1.1144346165657044,\n",
       " 1.1248148322105407,\n",
       " 1.1252214765548707,\n",
       " 1.1151555848121644,\n",
       " 1.1109768700599671,\n",
       " 1.1166911649703979,\n",
       " 1.1150764203071595,\n",
       " 1.1022034573554993,\n",
       " 1.0988321948051452,\n",
       " 1.0885774278640747,\n",
       " 1.0887252616882324,\n",
       " 1.068272840976715,\n",
       " 1.0599267315864562,\n",
       " 1.0633691549301147,\n",
       " 1.0609713840484618,\n",
       " 1.0524227261543273,\n",
       " 1.0476019525527953,\n",
       " 1.0394144868850708,\n",
       " 1.0343011808395386,\n",
       " 1.0235146737098695,\n",
       " 1.0330226159095763,\n",
       " 1.0261493849754333,\n",
       " 1.0141966247558594,\n",
       " 1.009307427406311,\n",
       " 1.013236997127533,\n",
       " 1.0088721513748169,\n",
       " 1.0037623834609986,\n",
       " 1.0100591349601746,\n",
       " 1.0052636551856995,\n",
       " 0.99500544071197505,\n",
       " 0.98860935926437377,\n",
       " 0.98510908603668212,\n",
       " 0.9777367115020752,\n",
       " 0.98583867073059084,\n",
       " 0.97847071170806887,\n",
       " 0.9610671472549438,\n",
       " 0.96647029161453246,\n",
       " 0.9781242728233337,\n",
       " 0.97183494806289672,\n",
       " 0.95589634656906131,\n",
       " 0.95718480587005617,\n",
       " 0.9622929382324219,\n",
       " 0.94209756135940548,\n",
       " 0.94910790205001827,\n",
       " 0.939266471862793,\n",
       " 0.92825340270996093,\n",
       " 0.92706851243972777,\n",
       " 0.94035269498825069,\n",
       " 0.92950174331665036,\n",
       " 0.93259513139724737,\n",
       " 0.92407561540603633,\n",
       " 0.9135090041160584,\n",
       " 0.90877328634262089,\n",
       " 0.90810402393341061,\n",
       " 0.90292660951614379,\n",
       " 0.91083798885345457,\n",
       " 0.90355105876922603,\n",
       " 0.89891088247299189,\n",
       " 0.8924912285804748,\n",
       " 0.89276284217834467,\n",
       " 0.89329318523406986,\n",
       " 0.88674533367156982,\n",
       " 0.87637861013412477,\n",
       " 0.88118101119995118,\n",
       " 0.87330506086349491,\n",
       " 0.87073082923889156,\n",
       " 0.87092423915863038,\n",
       " 0.85838617086410518,\n",
       " 0.8606434416770935,\n",
       " 0.8557652568817139,\n",
       " 0.85752359867095951,\n",
       " 0.84432161808013917,\n",
       " 0.84200684309005736,\n",
       " 0.84938927650451657,\n",
       " 0.8363108110427856,\n",
       " 0.84615985870361332,\n",
       " 0.84276828050613406,\n",
       " 0.8234273433685303,\n",
       " 0.83562259912490844,\n",
       " 0.82426998376846317,\n",
       " 0.82378208875656123,\n",
       " 0.82539154767990108,\n",
       " 0.81935541629791264,\n",
       " 0.8121254754066467,\n",
       " 0.81302978038787843,\n",
       " 0.80825975894927982,\n",
       " 0.8022838664054871,\n",
       " 0.80207050323486329,\n",
       " 0.80807299852371217,\n",
       " 0.79642227411270139,\n",
       " 0.79605785608291624,\n",
       " 0.80069423198699952,\n",
       " 0.78478551626205439,\n",
       " 0.78222316980361939,\n",
       " 0.78718792676925664,\n",
       " 0.78086067914962765,\n",
       " 0.77401515245437624,\n",
       " 0.77759436607360843,\n",
       " 0.76314457654953005,\n",
       " 0.77228223323822021,\n",
       " 0.76597029209136958,\n",
       " 0.75608797073364253,\n",
       " 0.75802426815032964,\n",
       " 0.75209473609924316,\n",
       " 0.74813612222671511,\n",
       " 0.74622123479843139,\n",
       " 0.75803104639053343,\n",
       " 0.74860252857208254,\n",
       " 0.74503091573715208,\n",
       " 0.74660508394241332,\n",
       " 0.73260982990264889,\n",
       " 0.73559648752212525,\n",
       " 0.7323309874534607,\n",
       " 0.74376672029495239,\n",
       " 0.73971845388412472,\n",
       " 0.73134471893310549,\n",
       " 0.72930617570877077,\n",
       " 0.7270460367202759,\n",
       " 0.71891463041305537,\n",
       " 0.71076095342636103,\n",
       " 0.71839912414550777,\n",
       " 0.71081809282302855,\n",
       " 0.71526608467102049,\n",
       " 0.7068062925338745,\n",
       " 0.70583611726760864,\n",
       " 0.70217855691909792,\n",
       " 0.69041930675506591,\n",
       " 0.70395322084426881,\n",
       " 0.69558129787445067,\n",
       " 0.69796739339828495,\n",
       " 0.69977432250976568,\n",
       " 0.69183090925216673,\n",
       " 0.69581838846206667,\n",
       " 0.68449193239212036,\n",
       " 0.67917671442031857,\n",
       " 0.68120036959648134,\n",
       " 0.68469090461730953,\n",
       " 0.67784913063049312,\n",
       " 0.67655237674713131,\n",
       " 0.67398857593536377,\n",
       " 0.66451896429061885,\n",
       " 0.67328289031982425,\n",
       " 0.67557228565216065,\n",
       " 0.65884732723236084,\n",
       " 0.66450116634368894,\n",
       " 0.66239506244659418,\n",
       " 0.66054852008819576,\n",
       " 0.65575845479965211,\n",
       " 0.64735613346099852,\n",
       " 0.65782318115234373,\n",
       " 0.64955160140991208,\n",
       " 0.64366023063659672,\n",
       " 0.65636202931404108,\n",
       " 0.64742997169494632,\n",
       " 0.64945281028747559,\n",
       " 0.63358018159866336,\n",
       " 0.62873627662658693,\n",
       " 0.62808829784393305,\n",
       " 0.64312839508056641,\n",
       " 0.6410548067092896,\n",
       " 0.62945190429687503,\n",
       " 0.63340895175933842,\n",
       " 0.61667513489723202,\n",
       " 0.62478491544723513,\n",
       " 0.62346933245658875,\n",
       " 0.61596763372421259,\n",
       " 0.60962569117546084,\n",
       " 0.61736426115036014,\n",
       " 0.60673957705497739,\n",
       " 0.61092541694641112,\n",
       " 0.60171033859252931,\n",
       " 0.6015780138969421,\n",
       " 0.60627941250801087,\n",
       " 0.60478862166404723,\n",
       " 0.59197670340538022,\n",
       " 0.6058127355575561,\n",
       " 0.5956192541122437,\n",
       " 0.59551262855529785,\n",
       " 0.59162885069847104,\n",
       " 0.59657577514648441,\n",
       " 0.59352148532867433,\n",
       " 0.59089796781539916,\n",
       " 0.58713758707046504,\n",
       " 0.5857683062553406,\n",
       " 0.58093133091926574,\n",
       " 0.58535124063491817,\n",
       " 0.57689205646514896,\n",
       " 0.57541659951210022,\n",
       " 0.5699586403369904,\n",
       " 0.57028636217117312,\n",
       " 0.57285765647888187,\n",
       " 0.572829681634903,\n",
       " 0.57890319943428037,\n",
       " 0.57287475824356082,\n",
       " 0.57307530879974367,\n",
       " 0.5695696640014648,\n",
       " 0.56609238862991329,\n",
       " 0.56496208906173706,\n",
       " 0.55955876231193546,\n",
       " 0.5626219785213471,\n",
       " 0.55771576642990117,\n",
       " 0.55635962963104246,\n",
       " 0.55824140787124632,\n",
       " 0.55527480959892273,\n",
       " 0.55327431321144105,\n",
       " 0.54942339062690737,\n",
       " 0.54406103372573855,\n",
       " 0.54348094344139097,\n",
       " 0.54434431791305538,\n",
       " 0.54586414337158207,\n",
       " 0.53727825880050661,\n",
       " 0.54027465462684632,\n",
       " 0.53799893736839299,\n",
       " 0.54333845734596253,\n",
       " 0.53468313097953801,\n",
       " 0.52669731736183167,\n",
       " 0.53059078693389894,\n",
       " 0.53047087669372561,\n",
       " 0.52948845624923702,\n",
       " 0.53032839417457578,\n",
       " 0.53314277291297918,\n",
       " 0.52601232528686526,\n",
       " 0.52306069135665889,\n",
       " 0.53134114384651188,\n",
       " 0.51659521579742429,\n",
       " 0.51914559841156005,\n",
       " 0.52324060201644895,\n",
       " 0.52253338456153875,\n",
       " 0.50956474542617802,\n",
       " 0.51863300919532773,\n",
       " 0.51236906051635744,\n",
       " 0.51420641064643857,\n",
       " 0.51701482057571413,\n",
       " 0.50799142122268681,\n",
       " 0.50936991810798649,\n",
       " 0.50680015802383427,\n",
       " 0.5036480057239533,\n",
       " 0.50569715023040773,\n",
       " 0.50370100378990168,\n",
       " 0.50353024721145634,\n",
       " 0.50593645811080934,\n",
       " 0.50269015550613405,\n",
       " 0.49325912117958071,\n",
       " 0.49381914973258972,\n",
       " 0.49760818481445312,\n",
       " 0.4937126886844635,\n",
       " 0.495666024684906,\n",
       " 0.48975211024284365,\n",
       " 0.49618904232978822,\n",
       " 0.48698121309280396,\n",
       " 0.49104478240013122,\n",
       " 0.48485744357109067,\n",
       " 0.48219933629035949,\n",
       " 0.48348530888557434,\n",
       " 0.47731225848197939,\n",
       " 0.48200986504554749,\n",
       " 0.47551507592201231,\n",
       " 0.47780215740203857,\n",
       " 0.4769594669342041,\n",
       " 0.48584260582923888,\n",
       " 0.47583621025085449,\n",
       " 0.47781380534172058,\n",
       " 0.47785638928413393,\n",
       " 0.47592651844024658,\n",
       " 0.47445284724235537,\n",
       " 0.47105420947074889,\n",
       " 0.4727302825450897,\n",
       " 0.46288458108901975,\n",
       " 0.46810448765754697,\n",
       " 0.46242117404937744,\n",
       " 0.46097525119781496,\n",
       " 0.46212382078170777,\n",
       " 0.4600576448440552,\n",
       " 0.47195282220840457,\n",
       " 0.46117424845695498,\n",
       " 0.45714986324310303,\n",
       " 0.45491627335548401,\n",
       " 0.45787426829338074,\n",
       " 0.44913861513137815,\n",
       " 0.44810721278190613,\n",
       " 0.4576784098148346,\n",
       " 0.45538465142250062,\n",
       " 0.44437823653221131,\n",
       " 0.44794299244880675,\n",
       " 0.44571430563926695,\n",
       " 0.44862668156623842,\n",
       " 0.4490914583206177,\n",
       " 0.45169626951217651,\n",
       " 0.44430491447448728,\n",
       " 0.44668920516967775,\n",
       " 0.44447203397750856,\n",
       " 0.441713011264801,\n",
       " 0.43123057246208191,\n",
       " 0.43632525920867921,\n",
       " 0.44682800889015195,\n",
       " 0.43208814382553101,\n",
       " 0.4392553675174713,\n",
       " 0.43294962525367736,\n",
       " 0.44072498440742491,\n",
       " 0.43130160212516783,\n",
       " 0.43087424635887145,\n",
       " 0.43252875804901125,\n",
       " 0.4289685130119324,\n",
       " 0.42822872281074525,\n",
       " 0.42883300542831421,\n",
       " 0.42700481414794922,\n",
       " 0.42299041271209714,\n",
       " 0.42186846971511843,\n",
       " 0.42882912755012514,\n",
       " 0.418831844329834,\n",
       " 0.41996823430061342,\n",
       " 0.42311989784240722,\n",
       " 0.41617899537086489,\n",
       " 0.42397665977478027,\n",
       " 0.41419937372207644,\n",
       " 0.42519647002220151,\n",
       " 0.41377121567726133,\n",
       " 0.41992248415946959,\n",
       " 0.41821545362472534,\n",
       " 0.41781262159347532,\n",
       " 0.4155846405029297,\n",
       " 0.40457671642303467,\n",
       " 0.41036531567573548,\n",
       " 0.40561799168586732,\n",
       " 0.41617562770843508,\n",
       " 0.41226464629173276,\n",
       " 0.41249915361404421,\n",
       " 0.40449358224868776,\n",
       " 0.39632556796073914,\n",
       " 0.41155970096588135,\n",
       " 0.3996471428871155,\n",
       " 0.41088940024375914,\n",
       " 0.40931903600692748,\n",
       " 0.41030484080314639,\n",
       " 0.39345692515373232,\n",
       " 0.40502332448959349,\n",
       " 0.39526234745979311,\n",
       " 0.405583393573761,\n",
       " 0.39651758193969727,\n",
       " 0.39495316505432126,\n",
       " 0.39538639783859253,\n",
       " 0.39716552972793578,\n",
       " 0.39028545975685119,\n",
       " 0.38747275829315186,\n",
       " 0.39333545207977294,\n",
       " 0.38715177178382876,\n",
       " 0.39825791120529175,\n",
       " 0.39011371970176695,\n",
       " 0.39102605581283567,\n",
       " 0.39170565247535705,\n",
       " 0.39360315442085264,\n",
       " 0.38627756357192994,\n",
       " 0.38175989389419557,\n",
       " 0.3828715479373932,\n",
       " 0.39006661653518676,\n",
       " 0.38825702428817749,\n",
       " 0.38577268719673158,\n",
       " 0.38693460702896121,\n",
       " 0.38549631834030151,\n",
       " 0.38346871972084046,\n",
       " 0.38298741579055784,\n",
       " 0.38363482356071471,\n",
       " 0.38597287774085998,\n",
       " 0.37714791893959043,\n",
       " 0.3779896640777588,\n",
       " 0.37814281582832338,\n",
       " 0.37318850755691529,\n",
       " 0.37201482892036436,\n",
       " 0.37350002884864808,\n",
       " 0.36959264397621155,\n",
       " 0.37000056505203249,\n",
       " 0.37331257939338686,\n",
       " 0.37121444225311279,\n",
       " 0.36865851879119871,\n",
       " 0.36639626264572145,\n",
       " 0.37250348448753356,\n",
       " 0.36641597986221314,\n",
       " 0.36547119021415708,\n",
       " 0.36372120141983033,\n",
       " 0.37492124915122987,\n",
       " 0.36634538412094114,\n",
       " 0.3673838746547699,\n",
       " 0.36521789789199827,\n",
       " 0.36744545578956606,\n",
       " 0.35990729808807376,\n",
       " 0.36337105631828309,\n",
       " 0.35463275551795959,\n",
       " 0.36207019329071044,\n",
       " 0.36462214350700378,\n",
       " 0.36577852487564089,\n",
       " 0.35894358515739438,\n",
       " 0.35581348061561585,\n",
       " 0.35838083267211912,\n",
       " 0.35668859720230101,\n",
       " 0.35609265446662902,\n",
       " 0.36235743403434756,\n",
       " 0.36031755328178405,\n",
       " 0.35004822731018065,\n",
       " 0.35353829979896545,\n",
       " 0.35305827379226684,\n",
       " 0.3519893276691437,\n",
       " 0.35051903724670408,\n",
       " 0.35369748234748838,\n",
       " 0.35258563041687013,\n",
       " 0.35341843485832214,\n",
       " 0.34763603568077089,\n",
       " 0.34790427207946778,\n",
       " 0.35530734896659849,\n",
       " 0.34844981908798217,\n",
       " 0.34888953685760499,\n",
       " 0.35019400477409363,\n",
       " 0.35638108730316165,\n",
       " 0.34850851058959958,\n",
       " 0.3430016541481018,\n",
       " 0.34429917573928831,\n",
       " 0.34839229106903075,\n",
       " 0.34618136405944822,\n",
       " 0.34025304794311523,\n",
       " 0.34321557879447939,\n",
       " 0.34104695200920104,\n",
       " 0.34380707144737244,\n",
       " 0.33925656318664549,\n",
       " 0.33933401942253111,\n",
       " 0.33608831644058229,\n",
       " 0.33645145058631898,\n",
       " 0.3335223877429962,\n",
       " 0.34159946441650391,\n",
       " 0.33200102448463442,\n",
       " 0.34003828048706053,\n",
       " 0.33707877635955813,\n",
       " 0.33774928092956541,\n",
       " 0.33099783062934873,\n",
       " 0.33459571242332459,\n",
       " 0.32934548139572145,\n",
       " 0.33239071607589721,\n",
       " 0.33140477657318113,\n",
       " 0.32821807742118836,\n",
       " 0.33072003245353698,\n",
       " 0.33483367681503295,\n",
       " 0.32785825490951537,\n",
       " 0.32729344129562377,\n",
       " 0.33197643041610719,\n",
       " 0.3351511263847351,\n",
       " 0.32857720613479613,\n",
       " 0.32936781644821167,\n",
       " 0.32517676353454589,\n",
       " 0.3221706646680832,\n",
       " 0.33124397993087767,\n",
       " 0.32809146881103518,\n",
       " 0.3245466089248657,\n",
       " 0.32605356216430664,\n",
       " 0.31885577797889708,\n",
       " 0.33028153181076048,\n",
       " 0.31909110903739929,\n",
       " 0.31990429520606994,\n",
       " 0.32311504602432251,\n",
       " 0.31954428434371951,\n",
       " 0.32018218398094178,\n",
       " 0.3180496072769165,\n",
       " 0.31653825283050535,\n",
       " 0.32383361697196961,\n",
       " 0.32349042057991029,\n",
       " 0.31836441159248352,\n",
       " 0.31671656012535093,\n",
       " 0.31708787322044374,\n",
       " 0.31901486396789552,\n",
       " 0.31981924533843992,\n",
       " 0.31655101358890536,\n",
       " 0.32433730065822602,\n",
       " 0.31634546875953673,\n",
       " 0.31169135510921481,\n",
       " 0.31684062838554383,\n",
       " 0.31412043929100036,\n",
       " 0.31433035910129548,\n",
       " 0.30860760629177092,\n",
       " 0.30784825026988982,\n",
       " 0.31338614821434019,\n",
       " 0.3150202775001526,\n",
       " 0.31822783827781675,\n",
       " 0.3132243573665619,\n",
       " 0.31500555932521818,\n",
       " 0.30782075107097628,\n",
       " 0.31085639476776122,\n",
       " 0.30903275012969972,\n",
       " 0.31184944152832034,\n",
       " 0.30790594518184661,\n",
       " 0.30830725133419035,\n",
       " 0.30962224066257477,\n",
       " 0.30634788870811464,\n",
       " 0.31185716032981875,\n",
       " 0.31432107925415037,\n",
       " 0.31069511950016021,\n",
       " 0.29755411505699159,\n",
       " 0.3087071907520294,\n",
       " 0.30841364264488219,\n",
       " 0.30560738801956178,\n",
       " 0.31244913458824158,\n",
       " 0.30355819821357727,\n",
       " 0.30161695599555971,\n",
       " 0.3052328312397003,\n",
       " 0.30174139022827151,\n",
       " 0.30050127565860746,\n",
       " 0.30191180706024168,\n",
       " 0.30166376590728761,\n",
       " 0.30440668225288392,\n",
       " 0.2991350728273392,\n",
       " 0.29796561062335969,\n",
       " 0.30074142932891845,\n",
       " 0.30157941699028012,\n",
       " 0.29840412914752962,\n",
       " 0.29823067724704744,\n",
       " 0.29867162942886355,\n",
       " 0.30430795133113864,\n",
       " 0.29389730989933016,\n",
       " 0.29876458108425141,\n",
       " 0.29835896193981171,\n",
       " 0.2993541580438614,\n",
       " 0.2954566878080368,\n",
       " 0.29737601697444915,\n",
       " 0.29477859020233155,\n",
       " 0.29948892652988435,\n",
       " 0.29261611521244046,\n",
       " 0.30285340309143066,\n",
       " 0.2953979641199112,\n",
       " 0.29450792312622071,\n",
       " 0.29307100653648377,\n",
       " 0.2968878948688507,\n",
       " 0.2898713558912277,\n",
       " 0.29630994379520414,\n",
       " 0.28418197154998781,\n",
       " 0.29180834054946897,\n",
       " 0.28532852411270143,\n",
       " 0.29584286749362948,\n",
       " 0.29144230246543884,\n",
       " 0.2941197741031647,\n",
       " 0.29286643087863923,\n",
       " 0.28900264024734496,\n",
       " 0.29083058178424837,\n",
       " 0.29499121546745299,\n",
       " 0.29149675488471982,\n",
       " 0.28549716174602507,\n",
       " 0.29189629316329957,\n",
       " 0.28949561119079592,\n",
       " 0.28384849965572356,\n",
       " 0.28040130019187925,\n",
       " 0.28833348155021665,\n",
       " 0.28419097661972048,\n",
       " 0.2920536011457443,\n",
       " 0.2854418408870697,\n",
       " 0.28531766057014463,\n",
       " 0.28154073238372801,\n",
       " 0.28077622056007384,\n",
       " 0.29066248953342438,\n",
       " 0.2867798322439194,\n",
       " 0.27884670495986941,\n",
       " 0.27904107511043547,\n",
       " 0.28589035868644713,\n",
       " 0.28778985202312468]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_graph(batch_size, num_steps, state_size,\n",
    "                num_classes = vocab_size, keep_prob = 0.7,\n",
    "                learning_rate=1e-4):\n",
    "    \n",
    "    # wipe out all previously built graphs\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # word ids, without one-hot encoding\n",
    "    x = tf.placeholder(tf.int32, [batch_size, num_steps], name='x')\n",
    "    y = tf.placeholder(tf.int32, [batch_size, num_steps], name='y')\n",
    "\n",
    "    # vector representation for each words(word2vec)\n",
    "    word_embeddings = tf.get_variable('embedding_matrix',\n",
    "                                 [num_classes, state_size])\n",
    "\n",
    "    # rnn_inputs is a tensor of dim [batch_size,num_steps,state_size]\n",
    "    rnn_inputs = tf.nn.embedding_lookup(word_embeddings, x)\n",
    "\n",
    "    cell = rnn.BasicLSTMCell(state_size, state_is_tuple=True)\n",
    "    # Adding Dropout along to each stacked LSTM layers\n",
    "    cell = rnn.DropoutWrapper(cell,\n",
    "                              input_keep_prob = keep_prob,\n",
    "                              output_keep_prob = keep_prob)\n",
    "    cell = rnn.MultiRNNCell([cell] * 5, state_is_tuple=True)\n",
    "    init_state = cell.zero_state(batch_size, tf.float32)\n",
    "    rnn_outputs, final_state =\\\n",
    "    tf.nn.dynamic_rnn(cell,rnn_inputs,initial_state=init_state)\n",
    "\n",
    "    with tf.variable_scope('softmax'):\n",
    "        W = tf.get_variable('W', [state_size, num_classes])\n",
    "        b = tf.get_variable('b', [num_classes],\n",
    "                            initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "    #reshape rnn_outputs and y so we can get the logits in a single matmul\n",
    "    rnn_outputs = tf.reshape(rnn_outputs, [-1, state_size])\n",
    "    y_reshaped = tf.reshape(y, [-1])\n",
    "\n",
    "    logits = tf.matmul(rnn_outputs, W) + b\n",
    "\n",
    "    # Make predictions\n",
    "    predictions = tf.nn.softmax(logits)\n",
    "\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            logits=logits, labels=y_reshaped))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "    \n",
    "    return dict(\n",
    "        x=x,\n",
    "        y=y,\n",
    "        init_state=init_state,\n",
    "        final_state=final_state,\n",
    "        loss=loss,\n",
    "        optimizer=optimizer,\n",
    "        preds=predictions,\n",
    "        saver=tf.train.Saver()\n",
    "    )\n",
    "\n",
    "batch_size = 32\n",
    "num_steps = 10\n",
    "state_size = 500\n",
    "num_epochs = 1000\n",
    "\n",
    "graph = build_graph(batch_size,num_steps,state_size)\n",
    "train_network(graph, num_epochs, batch_size, num_steps,\n",
    "              save='saves/lstm_lm_1000epochs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Using `GRUCell`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss in Epoch 0:7.4222\n",
      "Average loss in Epoch 1:7.3584\n",
      "Average loss in Epoch 2:6.5257\n",
      "Average loss in Epoch 3:5.9450\n",
      "Average loss in Epoch 4:5.7495\n",
      "Average loss in Epoch 5:5.6482\n",
      "Average loss in Epoch 6:5.5778\n",
      "Average loss in Epoch 7:5.5332\n",
      "Average loss in Epoch 8:5.5026\n",
      "Average loss in Epoch 9:5.4657\n",
      "Average loss in Epoch 10:5.4389\n",
      "Average loss in Epoch 11:5.4214\n",
      "Average loss in Epoch 12:5.4063\n",
      "Average loss in Epoch 13:5.3938\n",
      "Average loss in Epoch 14:5.3959\n",
      "Average loss in Epoch 15:5.3843\n",
      "Average loss in Epoch 16:5.3792\n",
      "Average loss in Epoch 17:5.3875\n",
      "Average loss in Epoch 18:5.3808\n",
      "Average loss in Epoch 19:5.3778\n",
      "Average loss in Epoch 20:5.3757\n",
      "Average loss in Epoch 21:5.3726\n",
      "Average loss in Epoch 22:5.3691\n",
      "Average loss in Epoch 23:5.3694\n",
      "Average loss in Epoch 24:5.3714\n",
      "Average loss in Epoch 25:5.3639\n",
      "Average loss in Epoch 26:5.3600\n",
      "Average loss in Epoch 27:5.3609\n",
      "Average loss in Epoch 28:5.3581\n",
      "Average loss in Epoch 29:5.3542\n",
      "Average loss in Epoch 30:5.3578\n",
      "Average loss in Epoch 31:5.3499\n",
      "Average loss in Epoch 32:5.3572\n",
      "Average loss in Epoch 33:5.3598\n",
      "Average loss in Epoch 34:5.3549\n",
      "Average loss in Epoch 35:5.3567\n",
      "Average loss in Epoch 36:5.3582\n",
      "Average loss in Epoch 37:5.3561\n",
      "Average loss in Epoch 38:5.3449\n",
      "Average loss in Epoch 39:5.3538\n",
      "Average loss in Epoch 40:5.3508\n",
      "Average loss in Epoch 41:5.3434\n",
      "Average loss in Epoch 42:5.3526\n",
      "Average loss in Epoch 43:5.3465\n",
      "Average loss in Epoch 44:5.3442\n",
      "Average loss in Epoch 45:5.3484\n",
      "Average loss in Epoch 46:5.3488\n",
      "Average loss in Epoch 47:5.3454\n",
      "Average loss in Epoch 48:5.3390\n",
      "Average loss in Epoch 49:5.3374\n",
      "Average loss in Epoch 50:5.3370\n",
      "Average loss in Epoch 51:5.3350\n",
      "Average loss in Epoch 52:5.3323\n",
      "Average loss in Epoch 53:5.3360\n",
      "Average loss in Epoch 54:5.3354\n",
      "Average loss in Epoch 55:5.3305\n",
      "Average loss in Epoch 56:5.3295\n",
      "Average loss in Epoch 57:5.3283\n",
      "Average loss in Epoch 58:5.3249\n",
      "Average loss in Epoch 59:5.3344\n",
      "Average loss in Epoch 60:5.3310\n",
      "Average loss in Epoch 61:5.3278\n",
      "Average loss in Epoch 62:5.3194\n",
      "Average loss in Epoch 63:5.3228\n",
      "Average loss in Epoch 64:5.3253\n",
      "Average loss in Epoch 65:5.3232\n",
      "Average loss in Epoch 66:5.3238\n",
      "Average loss in Epoch 67:5.3235\n",
      "Average loss in Epoch 68:5.3180\n",
      "Average loss in Epoch 69:5.3125\n",
      "Average loss in Epoch 70:5.3045\n",
      "Average loss in Epoch 71:5.3096\n",
      "Average loss in Epoch 72:5.3122\n",
      "Average loss in Epoch 73:5.3068\n",
      "Average loss in Epoch 74:5.3196\n",
      "Average loss in Epoch 75:5.3155\n",
      "Average loss in Epoch 76:5.3018\n",
      "Average loss in Epoch 77:5.3044\n",
      "Average loss in Epoch 78:5.3028\n",
      "Average loss in Epoch 79:5.3017\n",
      "Average loss in Epoch 80:5.3008\n",
      "Average loss in Epoch 81:5.3001\n",
      "Average loss in Epoch 82:5.2912\n",
      "Average loss in Epoch 83:5.2910\n",
      "Average loss in Epoch 84:5.2911\n",
      "Average loss in Epoch 85:5.2894\n",
      "Average loss in Epoch 86:5.2902\n",
      "Average loss in Epoch 87:5.2840\n",
      "Average loss in Epoch 88:5.2878\n",
      "Average loss in Epoch 89:5.2884\n",
      "Average loss in Epoch 90:5.2793\n",
      "Average loss in Epoch 91:5.2800\n",
      "Average loss in Epoch 92:5.2736\n",
      "Average loss in Epoch 93:5.2769\n",
      "Average loss in Epoch 94:5.2643\n",
      "Average loss in Epoch 95:5.2665\n",
      "Average loss in Epoch 96:5.2685\n",
      "Average loss in Epoch 97:5.2639\n",
      "Average loss in Epoch 98:5.2580\n",
      "Average loss in Epoch 99:5.2514\n",
      "Average loss in Epoch 100:5.2547\n",
      "Average loss in Epoch 101:5.2413\n",
      "Average loss in Epoch 102:5.2338\n",
      "Average loss in Epoch 103:5.2320\n",
      "Average loss in Epoch 104:5.2151\n",
      "Average loss in Epoch 105:5.2056\n",
      "Average loss in Epoch 106:5.2122\n",
      "Average loss in Epoch 107:5.2100\n",
      "Average loss in Epoch 108:5.1877\n",
      "Average loss in Epoch 109:5.1760\n",
      "Average loss in Epoch 110:5.1639\n",
      "Average loss in Epoch 111:5.1480\n",
      "Average loss in Epoch 112:5.1355\n",
      "Average loss in Epoch 113:5.1208\n",
      "Average loss in Epoch 114:5.1204\n",
      "Average loss in Epoch 115:5.1070\n",
      "Average loss in Epoch 116:5.0779\n",
      "Average loss in Epoch 117:5.0600\n",
      "Average loss in Epoch 118:5.0373\n",
      "Average loss in Epoch 119:5.0182\n",
      "Average loss in Epoch 120:4.9894\n",
      "Average loss in Epoch 121:4.9761\n",
      "Average loss in Epoch 122:4.9571\n",
      "Average loss in Epoch 123:4.9327\n",
      "Average loss in Epoch 124:4.9243\n",
      "Average loss in Epoch 125:4.8958\n",
      "Average loss in Epoch 126:4.8731\n",
      "Average loss in Epoch 127:4.8418\n",
      "Average loss in Epoch 128:4.8170\n",
      "Average loss in Epoch 129:4.7915\n",
      "Average loss in Epoch 130:4.7639\n",
      "Average loss in Epoch 131:4.7496\n",
      "Average loss in Epoch 132:4.7383\n",
      "Average loss in Epoch 133:4.7099\n",
      "Average loss in Epoch 134:4.6972\n",
      "Average loss in Epoch 135:4.6762\n",
      "Average loss in Epoch 136:4.6450\n",
      "Average loss in Epoch 137:4.6203\n",
      "Average loss in Epoch 138:4.5967\n",
      "Average loss in Epoch 139:4.5722\n",
      "Average loss in Epoch 140:4.5561\n",
      "Average loss in Epoch 141:4.5422\n",
      "Average loss in Epoch 142:4.5216\n",
      "Average loss in Epoch 143:4.5057\n",
      "Average loss in Epoch 144:4.4776\n",
      "Average loss in Epoch 145:4.4505\n",
      "Average loss in Epoch 146:4.4305\n",
      "Average loss in Epoch 147:4.4026\n",
      "Average loss in Epoch 148:4.3860\n",
      "Average loss in Epoch 149:4.3532\n",
      "Average loss in Epoch 150:4.3440\n",
      "Average loss in Epoch 151:4.3191\n",
      "Average loss in Epoch 152:4.3044\n",
      "Average loss in Epoch 153:4.2836\n",
      "Average loss in Epoch 154:4.2713\n",
      "Average loss in Epoch 155:4.2405\n",
      "Average loss in Epoch 156:4.2200\n",
      "Average loss in Epoch 157:4.2107\n",
      "Average loss in Epoch 158:4.1778\n",
      "Average loss in Epoch 159:4.1535\n",
      "Average loss in Epoch 160:4.1309\n",
      "Average loss in Epoch 161:4.1229\n",
      "Average loss in Epoch 162:4.0964\n",
      "Average loss in Epoch 163:4.0835\n",
      "Average loss in Epoch 164:4.0680\n",
      "Average loss in Epoch 165:4.0385\n",
      "Average loss in Epoch 166:4.0206\n",
      "Average loss in Epoch 167:3.9951\n",
      "Average loss in Epoch 168:3.9754\n",
      "Average loss in Epoch 169:3.9588\n",
      "Average loss in Epoch 170:3.9413\n",
      "Average loss in Epoch 171:3.9209\n",
      "Average loss in Epoch 172:3.9059\n",
      "Average loss in Epoch 173:3.8844\n",
      "Average loss in Epoch 174:3.8622\n",
      "Average loss in Epoch 175:3.8471\n",
      "Average loss in Epoch 176:3.8396\n",
      "Average loss in Epoch 177:3.8129\n",
      "Average loss in Epoch 178:3.8039\n",
      "Average loss in Epoch 179:3.7695\n",
      "Average loss in Epoch 180:3.7573\n",
      "Average loss in Epoch 181:3.7359\n",
      "Average loss in Epoch 182:3.7194\n",
      "Average loss in Epoch 183:3.7025\n",
      "Average loss in Epoch 184:3.6874\n",
      "Average loss in Epoch 185:3.6641\n",
      "Average loss in Epoch 186:3.6556\n",
      "Average loss in Epoch 187:3.6231\n",
      "Average loss in Epoch 188:3.6191\n",
      "Average loss in Epoch 189:3.5950\n",
      "Average loss in Epoch 190:3.5835\n",
      "Average loss in Epoch 191:3.5642\n",
      "Average loss in Epoch 192:3.5366\n",
      "Average loss in Epoch 193:3.5342\n",
      "Average loss in Epoch 194:3.5184\n",
      "Average loss in Epoch 195:3.4954\n",
      "Average loss in Epoch 196:3.4841\n",
      "Average loss in Epoch 197:3.4676\n",
      "Average loss in Epoch 198:3.4543\n",
      "Average loss in Epoch 199:3.4476\n",
      "Average loss in Epoch 200:3.4410\n",
      "Average loss in Epoch 201:3.4081\n",
      "Average loss in Epoch 202:3.4010\n",
      "Average loss in Epoch 203:3.3706\n",
      "Average loss in Epoch 204:3.3708\n",
      "Average loss in Epoch 205:3.3556\n",
      "Average loss in Epoch 206:3.3443\n",
      "Average loss in Epoch 207:3.3287\n",
      "Average loss in Epoch 208:3.3005\n",
      "Average loss in Epoch 209:3.2906\n",
      "Average loss in Epoch 210:3.2787\n",
      "Average loss in Epoch 211:3.2662\n",
      "Average loss in Epoch 212:3.2729\n",
      "Average loss in Epoch 213:3.2375\n",
      "Average loss in Epoch 214:3.2205\n",
      "Average loss in Epoch 215:3.2045\n",
      "Average loss in Epoch 216:3.1876\n",
      "Average loss in Epoch 217:3.1751\n",
      "Average loss in Epoch 218:3.1558\n",
      "Average loss in Epoch 219:3.1430\n",
      "Average loss in Epoch 220:3.1461\n",
      "Average loss in Epoch 221:3.1184\n",
      "Average loss in Epoch 222:3.1199\n",
      "Average loss in Epoch 223:3.0943\n",
      "Average loss in Epoch 224:3.0820\n",
      "Average loss in Epoch 225:3.0825\n",
      "Average loss in Epoch 226:3.0600\n",
      "Average loss in Epoch 227:3.0329\n",
      "Average loss in Epoch 228:3.0264\n",
      "Average loss in Epoch 229:3.0360\n",
      "Average loss in Epoch 230:3.0246\n",
      "Average loss in Epoch 231:3.0235\n",
      "Average loss in Epoch 232:3.0223\n",
      "Average loss in Epoch 233:3.0265\n",
      "Average loss in Epoch 234:3.0085\n",
      "Average loss in Epoch 235:2.9807\n",
      "Average loss in Epoch 236:2.9422\n",
      "Average loss in Epoch 237:2.9320\n",
      "Average loss in Epoch 238:2.9009\n",
      "Average loss in Epoch 239:2.8777\n",
      "Average loss in Epoch 240:2.8652\n",
      "Average loss in Epoch 241:2.8441\n",
      "Average loss in Epoch 242:2.8323\n",
      "Average loss in Epoch 243:2.8133\n",
      "Average loss in Epoch 244:2.7987\n",
      "Average loss in Epoch 245:2.7889\n",
      "Average loss in Epoch 246:2.7779\n",
      "Average loss in Epoch 247:2.7599\n",
      "Average loss in Epoch 248:2.7578\n",
      "Average loss in Epoch 249:2.7369\n",
      "Average loss in Epoch 250:2.7216\n",
      "Average loss in Epoch 251:2.7179\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss in Epoch 252:2.7055\n",
      "Average loss in Epoch 253:2.6915\n",
      "Average loss in Epoch 254:2.6676\n",
      "Average loss in Epoch 255:2.6699\n",
      "Average loss in Epoch 256:2.6517\n",
      "Average loss in Epoch 257:2.6301\n",
      "Average loss in Epoch 258:2.6171\n",
      "Average loss in Epoch 259:2.6259\n",
      "Average loss in Epoch 260:2.6084\n",
      "Average loss in Epoch 261:2.5913\n",
      "Average loss in Epoch 262:2.5907\n",
      "Average loss in Epoch 263:2.5806\n",
      "Average loss in Epoch 264:2.5720\n",
      "Average loss in Epoch 265:2.5594\n",
      "Average loss in Epoch 266:2.5328\n",
      "Average loss in Epoch 267:2.5255\n",
      "Average loss in Epoch 268:2.5099\n",
      "Average loss in Epoch 269:2.5010\n",
      "Average loss in Epoch 270:2.5050\n",
      "Average loss in Epoch 271:2.4778\n",
      "Average loss in Epoch 272:2.4631\n",
      "Average loss in Epoch 273:2.4520\n",
      "Average loss in Epoch 274:2.4428\n",
      "Average loss in Epoch 275:2.4298\n",
      "Average loss in Epoch 276:2.4275\n",
      "Average loss in Epoch 277:2.4222\n",
      "Average loss in Epoch 278:2.3966\n",
      "Average loss in Epoch 279:2.4005\n",
      "Average loss in Epoch 280:2.3861\n",
      "Average loss in Epoch 281:2.3685\n",
      "Average loss in Epoch 282:2.3585\n",
      "Average loss in Epoch 283:2.3736\n",
      "Average loss in Epoch 284:2.3601\n",
      "Average loss in Epoch 285:2.3462\n",
      "Average loss in Epoch 286:2.3255\n",
      "Average loss in Epoch 287:2.3250\n",
      "Average loss in Epoch 288:2.3083\n",
      "Average loss in Epoch 289:2.2867\n",
      "Average loss in Epoch 290:2.2878\n",
      "Average loss in Epoch 291:2.2789\n",
      "Average loss in Epoch 292:2.2728\n",
      "Average loss in Epoch 293:2.2529\n",
      "Average loss in Epoch 294:2.2418\n",
      "Average loss in Epoch 295:2.2483\n",
      "Average loss in Epoch 296:2.2310\n",
      "Average loss in Epoch 297:2.2337\n",
      "Average loss in Epoch 298:2.2129\n",
      "Average loss in Epoch 299:2.1953\n",
      "Average loss in Epoch 300:2.1856\n",
      "Average loss in Epoch 301:2.1725\n",
      "Average loss in Epoch 302:2.1669\n",
      "Average loss in Epoch 303:2.1714\n",
      "Average loss in Epoch 304:2.1559\n",
      "Average loss in Epoch 305:2.1495\n",
      "Average loss in Epoch 306:2.1527\n",
      "Average loss in Epoch 307:2.1317\n",
      "Average loss in Epoch 308:2.1229\n",
      "Average loss in Epoch 309:2.1244\n",
      "Average loss in Epoch 310:2.1123\n",
      "Average loss in Epoch 311:2.1130\n",
      "Average loss in Epoch 312:2.1136\n",
      "Average loss in Epoch 313:2.1120\n",
      "Average loss in Epoch 314:2.1136\n",
      "Average loss in Epoch 315:2.1033\n",
      "Average loss in Epoch 316:2.1093\n",
      "Average loss in Epoch 317:2.0900\n",
      "Average loss in Epoch 318:2.0744\n",
      "Average loss in Epoch 319:2.0825\n",
      "Average loss in Epoch 320:2.0509\n",
      "Average loss in Epoch 321:2.0401\n",
      "Average loss in Epoch 322:2.0152\n",
      "Average loss in Epoch 323:1.9945\n",
      "Average loss in Epoch 324:1.9915\n",
      "Average loss in Epoch 325:1.9683\n",
      "Average loss in Epoch 326:1.9574\n",
      "Average loss in Epoch 327:1.9457\n",
      "Average loss in Epoch 328:1.9414\n",
      "Average loss in Epoch 329:1.9273\n",
      "Average loss in Epoch 330:1.9289\n",
      "Average loss in Epoch 331:1.9093\n",
      "Average loss in Epoch 332:1.9040\n",
      "Average loss in Epoch 333:1.8990\n",
      "Average loss in Epoch 334:1.8883\n",
      "Average loss in Epoch 335:1.8744\n",
      "Average loss in Epoch 336:1.8887\n",
      "Average loss in Epoch 337:1.8566\n",
      "Average loss in Epoch 338:1.8490\n",
      "Average loss in Epoch 339:1.8460\n",
      "Average loss in Epoch 340:1.8511\n",
      "Average loss in Epoch 341:1.8321\n",
      "Average loss in Epoch 342:1.8275\n",
      "Average loss in Epoch 343:1.8206\n",
      "Average loss in Epoch 344:1.7993\n",
      "Average loss in Epoch 345:1.8005\n",
      "Average loss in Epoch 346:1.7847\n",
      "Average loss in Epoch 347:1.7895\n",
      "Average loss in Epoch 348:1.7722\n",
      "Average loss in Epoch 349:1.7779\n",
      "Average loss in Epoch 350:1.7622\n",
      "Average loss in Epoch 351:1.7524\n",
      "Average loss in Epoch 352:1.7431\n",
      "Average loss in Epoch 353:1.7476\n",
      "Average loss in Epoch 354:1.7290\n",
      "Average loss in Epoch 355:1.7335\n",
      "Average loss in Epoch 356:1.7243\n",
      "Average loss in Epoch 357:1.7238\n",
      "Average loss in Epoch 358:1.7010\n",
      "Average loss in Epoch 359:1.7088\n",
      "Average loss in Epoch 360:1.7029\n",
      "Average loss in Epoch 361:1.6970\n",
      "Average loss in Epoch 362:1.6749\n",
      "Average loss in Epoch 363:1.6744\n",
      "Average loss in Epoch 364:1.6664\n",
      "Average loss in Epoch 365:1.6584\n",
      "Average loss in Epoch 366:1.6596\n",
      "Average loss in Epoch 367:1.6378\n",
      "Average loss in Epoch 368:1.6484\n",
      "Average loss in Epoch 369:1.6184\n",
      "Average loss in Epoch 370:1.6075\n",
      "Average loss in Epoch 371:1.6171\n",
      "Average loss in Epoch 372:1.6161\n",
      "Average loss in Epoch 373:1.6120\n",
      "Average loss in Epoch 374:1.5948\n",
      "Average loss in Epoch 375:1.5928\n",
      "Average loss in Epoch 376:1.5917\n",
      "Average loss in Epoch 377:1.5740\n",
      "Average loss in Epoch 378:1.5636\n",
      "Average loss in Epoch 379:1.5734\n",
      "Average loss in Epoch 380:1.5708\n",
      "Average loss in Epoch 381:1.5561\n",
      "Average loss in Epoch 382:1.5430\n",
      "Average loss in Epoch 383:1.5423\n",
      "Average loss in Epoch 384:1.5330\n",
      "Average loss in Epoch 385:1.5288\n",
      "Average loss in Epoch 386:1.5218\n",
      "Average loss in Epoch 387:1.5188\n",
      "Average loss in Epoch 388:1.5154\n",
      "Average loss in Epoch 389:1.4999\n",
      "Average loss in Epoch 390:1.5032\n",
      "Average loss in Epoch 391:1.5016\n",
      "Average loss in Epoch 392:1.4908\n",
      "Average loss in Epoch 393:1.4910\n",
      "Average loss in Epoch 394:1.4791\n",
      "Average loss in Epoch 395:1.4633\n",
      "Average loss in Epoch 396:1.4684\n",
      "Average loss in Epoch 397:1.4522\n",
      "Average loss in Epoch 398:1.4511\n",
      "Average loss in Epoch 399:1.4496\n",
      "Average loss in Epoch 400:1.4265\n",
      "Average loss in Epoch 401:1.4322\n",
      "Average loss in Epoch 402:1.4386\n",
      "Average loss in Epoch 403:1.4262\n",
      "Average loss in Epoch 404:1.4263\n",
      "Average loss in Epoch 405:1.4165\n",
      "Average loss in Epoch 406:1.4062\n",
      "Average loss in Epoch 407:1.3952\n",
      "Average loss in Epoch 408:1.3987\n",
      "Average loss in Epoch 409:1.3982\n",
      "Average loss in Epoch 410:1.3963\n",
      "Average loss in Epoch 411:1.3768\n",
      "Average loss in Epoch 412:1.3766\n",
      "Average loss in Epoch 413:1.3635\n",
      "Average loss in Epoch 414:1.3695\n",
      "Average loss in Epoch 415:1.3581\n",
      "Average loss in Epoch 416:1.3560\n",
      "Average loss in Epoch 417:1.3458\n",
      "Average loss in Epoch 418:1.3335\n",
      "Average loss in Epoch 419:1.3387\n",
      "Average loss in Epoch 420:1.3407\n",
      "Average loss in Epoch 421:1.3300\n",
      "Average loss in Epoch 422:1.3242\n",
      "Average loss in Epoch 423:1.3245\n",
      "Average loss in Epoch 424:1.3185\n",
      "Average loss in Epoch 425:1.3053\n",
      "Average loss in Epoch 426:1.3007\n",
      "Average loss in Epoch 427:1.2990\n",
      "Average loss in Epoch 428:1.3065\n",
      "Average loss in Epoch 429:1.2992\n",
      "Average loss in Epoch 430:1.2936\n",
      "Average loss in Epoch 431:1.2856\n",
      "Average loss in Epoch 432:1.2725\n",
      "Average loss in Epoch 433:1.2746\n",
      "Average loss in Epoch 434:1.2730\n",
      "Average loss in Epoch 435:1.2644\n",
      "Average loss in Epoch 436:1.2602\n",
      "Average loss in Epoch 437:1.2644\n",
      "Average loss in Epoch 438:1.2679\n",
      "Average loss in Epoch 439:1.2600\n",
      "Average loss in Epoch 440:1.2489\n",
      "Average loss in Epoch 441:1.2625\n",
      "Average loss in Epoch 442:1.2588\n",
      "Average loss in Epoch 443:1.2487\n",
      "Average loss in Epoch 444:1.2333\n",
      "Average loss in Epoch 445:1.2446\n",
      "Average loss in Epoch 446:1.2283\n",
      "Average loss in Epoch 447:1.2256\n",
      "Average loss in Epoch 448:1.2243\n",
      "Average loss in Epoch 449:1.2222\n",
      "Average loss in Epoch 450:1.2061\n",
      "Average loss in Epoch 451:1.2035\n",
      "Average loss in Epoch 452:1.2014\n",
      "Average loss in Epoch 453:1.1988\n",
      "Average loss in Epoch 454:1.1808\n",
      "Average loss in Epoch 455:1.1770\n",
      "Average loss in Epoch 456:1.1683\n",
      "Average loss in Epoch 457:1.1641\n",
      "Average loss in Epoch 458:1.1621\n",
      "Average loss in Epoch 459:1.1567\n",
      "Average loss in Epoch 460:1.1535\n",
      "Average loss in Epoch 461:1.1369\n",
      "Average loss in Epoch 462:1.1397\n",
      "Average loss in Epoch 463:1.1314\n",
      "Average loss in Epoch 464:1.1376\n",
      "Average loss in Epoch 465:1.1264\n",
      "Average loss in Epoch 466:1.1161\n",
      "Average loss in Epoch 467:1.1246\n",
      "Average loss in Epoch 468:1.1132\n",
      "Average loss in Epoch 469:1.1161\n",
      "Average loss in Epoch 470:1.1012\n",
      "Average loss in Epoch 471:1.1074\n",
      "Average loss in Epoch 472:1.1065\n",
      "Average loss in Epoch 473:1.0902\n",
      "Average loss in Epoch 474:1.0940\n",
      "Average loss in Epoch 475:1.0976\n",
      "Average loss in Epoch 476:1.0921\n",
      "Average loss in Epoch 477:1.0740\n",
      "Average loss in Epoch 478:1.0851\n",
      "Average loss in Epoch 479:1.0718\n",
      "Average loss in Epoch 480:1.0665\n",
      "Average loss in Epoch 481:1.0748\n",
      "Average loss in Epoch 482:1.0597\n",
      "Average loss in Epoch 483:1.0586\n",
      "Average loss in Epoch 484:1.0561\n",
      "Average loss in Epoch 485:1.0522\n",
      "Average loss in Epoch 486:1.0496\n",
      "Average loss in Epoch 487:1.0452\n",
      "Average loss in Epoch 488:1.0338\n",
      "Average loss in Epoch 489:1.0323\n",
      "Average loss in Epoch 490:1.0305\n",
      "Average loss in Epoch 491:1.0341\n",
      "Average loss in Epoch 492:1.0274\n",
      "Average loss in Epoch 493:1.0186\n",
      "Average loss in Epoch 494:1.0240\n",
      "Average loss in Epoch 495:1.0158\n",
      "Average loss in Epoch 496:1.0113\n",
      "Average loss in Epoch 497:1.0104\n",
      "Average loss in Epoch 498:1.0076\n",
      "Average loss in Epoch 499:1.0065\n",
      "Average loss in Epoch 500:1.0013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss in Epoch 501:0.9884\n",
      "Average loss in Epoch 502:0.9963\n",
      "Average loss in Epoch 503:0.9879\n",
      "Average loss in Epoch 504:0.9861\n",
      "Average loss in Epoch 505:0.9824\n",
      "Average loss in Epoch 506:0.9811\n",
      "Average loss in Epoch 507:0.9816\n",
      "Average loss in Epoch 508:0.9830\n",
      "Average loss in Epoch 509:0.9668\n",
      "Average loss in Epoch 510:0.9684\n",
      "Average loss in Epoch 511:0.9658\n",
      "Average loss in Epoch 512:0.9630\n",
      "Average loss in Epoch 513:0.9555\n",
      "Average loss in Epoch 514:0.9584\n",
      "Average loss in Epoch 515:0.9410\n",
      "Average loss in Epoch 516:0.9437\n",
      "Average loss in Epoch 517:0.9436\n",
      "Average loss in Epoch 518:0.9517\n",
      "Average loss in Epoch 519:0.9469\n",
      "Average loss in Epoch 520:0.9323\n",
      "Average loss in Epoch 521:0.9354\n",
      "Average loss in Epoch 522:0.9282\n",
      "Average loss in Epoch 523:0.9356\n",
      "Average loss in Epoch 524:0.9240\n",
      "Average loss in Epoch 525:0.9203\n",
      "Average loss in Epoch 526:0.9200\n",
      "Average loss in Epoch 527:0.9253\n",
      "Average loss in Epoch 528:0.9155\n",
      "Average loss in Epoch 529:0.9259\n",
      "Average loss in Epoch 530:0.9042\n",
      "Average loss in Epoch 531:0.9045\n",
      "Average loss in Epoch 532:0.8990\n",
      "Average loss in Epoch 533:0.8956\n",
      "Average loss in Epoch 534:0.8921\n",
      "Average loss in Epoch 535:0.8843\n",
      "Average loss in Epoch 536:0.8885\n",
      "Average loss in Epoch 537:0.8942\n",
      "Average loss in Epoch 538:0.8687\n",
      "Average loss in Epoch 539:0.8656\n",
      "Average loss in Epoch 540:0.8705\n",
      "Average loss in Epoch 541:0.8888\n",
      "Average loss in Epoch 542:0.8657\n",
      "Average loss in Epoch 543:0.8773\n",
      "Average loss in Epoch 544:0.8657\n",
      "Average loss in Epoch 545:0.8617\n",
      "Average loss in Epoch 546:0.8540\n",
      "Average loss in Epoch 547:0.8542\n",
      "Average loss in Epoch 548:0.8542\n",
      "Average loss in Epoch 549:0.8609\n",
      "Average loss in Epoch 550:0.8441\n",
      "Average loss in Epoch 551:0.8555\n",
      "Average loss in Epoch 552:0.8413\n",
      "Average loss in Epoch 553:0.8464\n",
      "Average loss in Epoch 554:0.8467\n",
      "Average loss in Epoch 555:0.8412\n",
      "Average loss in Epoch 556:0.8363\n",
      "Average loss in Epoch 557:0.8442\n",
      "Average loss in Epoch 558:0.8363\n",
      "Average loss in Epoch 559:0.8223\n",
      "Average loss in Epoch 560:0.8195\n",
      "Average loss in Epoch 561:0.8349\n",
      "Average loss in Epoch 562:0.8336\n",
      "Average loss in Epoch 563:0.8202\n",
      "Average loss in Epoch 564:0.8209\n",
      "Average loss in Epoch 565:0.8181\n",
      "Average loss in Epoch 566:0.8147\n",
      "Average loss in Epoch 567:0.8224\n",
      "Average loss in Epoch 568:0.8079\n",
      "Average loss in Epoch 569:0.8123\n",
      "Average loss in Epoch 570:0.8097\n",
      "Average loss in Epoch 571:0.8010\n",
      "Average loss in Epoch 572:0.7943\n",
      "Average loss in Epoch 573:0.8006\n",
      "Average loss in Epoch 574:0.7977\n",
      "Average loss in Epoch 575:0.7970\n",
      "Average loss in Epoch 576:0.7822\n",
      "Average loss in Epoch 577:0.7846\n",
      "Average loss in Epoch 578:0.7838\n",
      "Average loss in Epoch 579:0.7832\n",
      "Average loss in Epoch 580:0.7768\n",
      "Average loss in Epoch 581:0.7768\n",
      "Average loss in Epoch 582:0.7720\n",
      "Average loss in Epoch 583:0.7666\n",
      "Average loss in Epoch 584:0.7617\n",
      "Average loss in Epoch 585:0.7718\n",
      "Average loss in Epoch 586:0.7660\n",
      "Average loss in Epoch 587:0.7639\n",
      "Average loss in Epoch 588:0.7667\n",
      "Average loss in Epoch 589:0.7591\n",
      "Average loss in Epoch 590:0.7572\n",
      "Average loss in Epoch 591:0.7471\n",
      "Average loss in Epoch 592:0.7439\n",
      "Average loss in Epoch 593:0.7564\n",
      "Average loss in Epoch 594:0.7553\n",
      "Average loss in Epoch 595:0.7490\n",
      "Average loss in Epoch 596:0.7484\n",
      "Average loss in Epoch 597:0.7472\n",
      "Average loss in Epoch 598:0.7363\n",
      "Average loss in Epoch 599:0.7355\n",
      "Average loss in Epoch 600:0.7378\n",
      "Average loss in Epoch 601:0.7372\n",
      "Average loss in Epoch 602:0.7342\n",
      "Average loss in Epoch 603:0.7323\n",
      "Average loss in Epoch 604:0.7346\n",
      "Average loss in Epoch 605:0.7228\n",
      "Average loss in Epoch 606:0.7200\n",
      "Average loss in Epoch 607:0.7266\n",
      "Average loss in Epoch 608:0.7211\n",
      "Average loss in Epoch 609:0.7217\n",
      "Average loss in Epoch 610:0.7259\n",
      "Average loss in Epoch 611:0.7246\n",
      "Average loss in Epoch 612:0.7100\n",
      "Average loss in Epoch 613:0.7070\n",
      "Average loss in Epoch 614:0.7065\n",
      "Average loss in Epoch 615:0.6993\n",
      "Average loss in Epoch 616:0.7090\n",
      "Average loss in Epoch 617:0.6963\n",
      "Average loss in Epoch 618:0.6970\n",
      "Average loss in Epoch 619:0.6986\n",
      "Average loss in Epoch 620:0.6969\n",
      "Average loss in Epoch 621:0.6993\n",
      "Average loss in Epoch 622:0.6926\n",
      "Average loss in Epoch 623:0.6957\n",
      "Average loss in Epoch 624:0.6876\n",
      "Average loss in Epoch 625:0.6885\n",
      "Average loss in Epoch 626:0.6877\n",
      "Average loss in Epoch 627:0.6912\n",
      "Average loss in Epoch 628:0.6812\n",
      "Average loss in Epoch 629:0.6760\n",
      "Average loss in Epoch 630:0.6861\n",
      "Average loss in Epoch 631:0.6828\n",
      "Average loss in Epoch 632:0.6802\n",
      "Average loss in Epoch 633:0.6750\n",
      "Average loss in Epoch 634:0.6800\n",
      "Average loss in Epoch 635:0.6711\n",
      "Average loss in Epoch 636:0.6663\n",
      "Average loss in Epoch 637:0.6661\n",
      "Average loss in Epoch 638:0.6709\n",
      "Average loss in Epoch 639:0.6555\n",
      "Average loss in Epoch 640:0.6730\n",
      "Average loss in Epoch 641:0.6552\n",
      "Average loss in Epoch 642:0.6584\n",
      "Average loss in Epoch 643:0.6581\n",
      "Average loss in Epoch 644:0.6651\n",
      "Average loss in Epoch 645:0.6505\n",
      "Average loss in Epoch 646:0.6518\n",
      "Average loss in Epoch 647:0.6510\n",
      "Average loss in Epoch 648:0.6592\n",
      "Average loss in Epoch 649:0.6507\n",
      "Average loss in Epoch 650:0.6460\n",
      "Average loss in Epoch 651:0.6447\n",
      "Average loss in Epoch 652:0.6564\n",
      "Average loss in Epoch 653:0.6440\n",
      "Average loss in Epoch 654:0.6282\n",
      "Average loss in Epoch 655:0.6425\n",
      "Average loss in Epoch 656:0.6380\n",
      "Average loss in Epoch 657:0.6397\n",
      "Average loss in Epoch 658:0.6401\n",
      "Average loss in Epoch 659:0.6347\n",
      "Average loss in Epoch 660:0.6350\n",
      "Average loss in Epoch 661:0.6326\n",
      "Average loss in Epoch 662:0.6401\n",
      "Average loss in Epoch 663:0.6228\n",
      "Average loss in Epoch 664:0.6302\n",
      "Average loss in Epoch 665:0.6302\n",
      "Average loss in Epoch 666:0.6182\n",
      "Average loss in Epoch 667:0.6152\n",
      "Average loss in Epoch 668:0.6152\n",
      "Average loss in Epoch 669:0.6168\n",
      "Average loss in Epoch 670:0.6139\n",
      "Average loss in Epoch 671:0.6166\n",
      "Average loss in Epoch 672:0.6081\n",
      "Average loss in Epoch 673:0.6104\n",
      "Average loss in Epoch 674:0.6131\n",
      "Average loss in Epoch 675:0.5997\n",
      "Average loss in Epoch 676:0.6087\n",
      "Average loss in Epoch 677:0.6021\n",
      "Average loss in Epoch 678:0.6140\n",
      "Average loss in Epoch 679:0.6060\n",
      "Average loss in Epoch 680:0.6071\n",
      "Average loss in Epoch 681:0.6035\n",
      "Average loss in Epoch 682:0.6045\n",
      "Average loss in Epoch 683:0.6006\n",
      "Average loss in Epoch 684:0.6008\n",
      "Average loss in Epoch 685:0.5965\n",
      "Average loss in Epoch 686:0.5844\n",
      "Average loss in Epoch 687:0.5859\n",
      "Average loss in Epoch 688:0.5958\n",
      "Average loss in Epoch 689:0.5899\n",
      "Average loss in Epoch 690:0.5861\n",
      "Average loss in Epoch 691:0.5821\n",
      "Average loss in Epoch 692:0.5829\n",
      "Average loss in Epoch 693:0.5838\n",
      "Average loss in Epoch 694:0.5828\n",
      "Average loss in Epoch 695:0.5784\n",
      "Average loss in Epoch 696:0.5863\n",
      "Average loss in Epoch 697:0.5785\n",
      "Average loss in Epoch 698:0.5725\n",
      "Average loss in Epoch 699:0.5778\n",
      "Average loss in Epoch 700:0.5683\n",
      "Average loss in Epoch 701:0.5691\n",
      "Average loss in Epoch 702:0.5716\n",
      "Average loss in Epoch 703:0.5728\n",
      "Average loss in Epoch 704:0.5706\n",
      "Average loss in Epoch 705:0.5718\n",
      "Average loss in Epoch 706:0.5663\n",
      "Average loss in Epoch 707:0.5602\n",
      "Average loss in Epoch 708:0.5722\n",
      "Average loss in Epoch 709:0.5634\n",
      "Average loss in Epoch 710:0.5554\n",
      "Average loss in Epoch 711:0.5565\n",
      "Average loss in Epoch 712:0.5596\n",
      "Average loss in Epoch 713:0.5561\n",
      "Average loss in Epoch 714:0.5538\n",
      "Average loss in Epoch 715:0.5474\n",
      "Average loss in Epoch 716:0.5551\n",
      "Average loss in Epoch 717:0.5610\n",
      "Average loss in Epoch 718:0.5517\n",
      "Average loss in Epoch 719:0.5441\n",
      "Average loss in Epoch 720:0.5379\n",
      "Average loss in Epoch 721:0.5474\n",
      "Average loss in Epoch 722:0.5456\n",
      "Average loss in Epoch 723:0.5445\n",
      "Average loss in Epoch 724:0.5409\n",
      "Average loss in Epoch 725:0.5438\n",
      "Average loss in Epoch 726:0.5466\n",
      "Average loss in Epoch 727:0.5440\n",
      "Average loss in Epoch 728:0.5457\n",
      "Average loss in Epoch 729:0.5488\n",
      "Average loss in Epoch 730:0.5418\n",
      "Average loss in Epoch 731:0.5372\n",
      "Average loss in Epoch 732:0.5259\n",
      "Average loss in Epoch 733:0.5350\n",
      "Average loss in Epoch 734:0.5381\n",
      "Average loss in Epoch 735:0.5273\n",
      "Average loss in Epoch 736:0.5342\n",
      "Average loss in Epoch 737:0.5323\n",
      "Average loss in Epoch 738:0.5365\n",
      "Average loss in Epoch 739:0.5224\n",
      "Average loss in Epoch 740:0.5272\n",
      "Average loss in Epoch 741:0.5220\n",
      "Average loss in Epoch 742:0.5246\n",
      "Average loss in Epoch 743:0.5202\n",
      "Average loss in Epoch 744:0.5259\n",
      "Average loss in Epoch 745:0.5234\n",
      "Average loss in Epoch 746:0.5137\n",
      "Average loss in Epoch 747:0.5243\n",
      "Average loss in Epoch 748:0.5142\n",
      "Average loss in Epoch 749:0.5182\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss in Epoch 750:0.5135\n",
      "Average loss in Epoch 751:0.5155\n",
      "Average loss in Epoch 752:0.5102\n",
      "Average loss in Epoch 753:0.5111\n",
      "Average loss in Epoch 754:0.5090\n",
      "Average loss in Epoch 755:0.5063\n",
      "Average loss in Epoch 756:0.5114\n",
      "Average loss in Epoch 757:0.5108\n",
      "Average loss in Epoch 758:0.5009\n",
      "Average loss in Epoch 759:0.5182\n",
      "Average loss in Epoch 760:0.5107\n",
      "Average loss in Epoch 761:0.5022\n",
      "Average loss in Epoch 762:0.5092\n",
      "Average loss in Epoch 763:0.5035\n",
      "Average loss in Epoch 764:0.5039\n",
      "Average loss in Epoch 765:0.4927\n",
      "Average loss in Epoch 766:0.4982\n",
      "Average loss in Epoch 767:0.4929\n",
      "Average loss in Epoch 768:0.4926\n",
      "Average loss in Epoch 769:0.5032\n",
      "Average loss in Epoch 770:0.4929\n",
      "Average loss in Epoch 771:0.4967\n",
      "Average loss in Epoch 772:0.4943\n",
      "Average loss in Epoch 773:0.4880\n",
      "Average loss in Epoch 774:0.4905\n",
      "Average loss in Epoch 775:0.4933\n",
      "Average loss in Epoch 776:0.4972\n",
      "Average loss in Epoch 777:0.4823\n",
      "Average loss in Epoch 778:0.4854\n",
      "Average loss in Epoch 779:0.4899\n",
      "Average loss in Epoch 780:0.4850\n",
      "Average loss in Epoch 781:0.4959\n",
      "Average loss in Epoch 782:0.4817\n",
      "Average loss in Epoch 783:0.4878\n",
      "Average loss in Epoch 784:0.4826\n",
      "Average loss in Epoch 785:0.4859\n",
      "Average loss in Epoch 786:0.4766\n",
      "Average loss in Epoch 787:0.4792\n",
      "Average loss in Epoch 788:0.4812\n",
      "Average loss in Epoch 789:0.4770\n",
      "Average loss in Epoch 790:0.4726\n",
      "Average loss in Epoch 791:0.4775\n",
      "Average loss in Epoch 792:0.4697\n",
      "Average loss in Epoch 793:0.4719\n",
      "Average loss in Epoch 794:0.4708\n",
      "Average loss in Epoch 795:0.4664\n",
      "Average loss in Epoch 796:0.4642\n",
      "Average loss in Epoch 797:0.4656\n",
      "Average loss in Epoch 798:0.4652\n",
      "Average loss in Epoch 799:0.4633\n",
      "Average loss in Epoch 800:0.4672\n",
      "Average loss in Epoch 801:0.4678\n",
      "Average loss in Epoch 802:0.4671\n",
      "Average loss in Epoch 803:0.4611\n",
      "Average loss in Epoch 804:0.4637\n",
      "Average loss in Epoch 805:0.4638\n",
      "Average loss in Epoch 806:0.4647\n",
      "Average loss in Epoch 807:0.4553\n",
      "Average loss in Epoch 808:0.4607\n",
      "Average loss in Epoch 809:0.4627\n",
      "Average loss in Epoch 810:0.4555\n",
      "Average loss in Epoch 811:0.4550\n",
      "Average loss in Epoch 812:0.4531\n",
      "Average loss in Epoch 813:0.4519\n",
      "Average loss in Epoch 814:0.4536\n",
      "Average loss in Epoch 815:0.4533\n",
      "Average loss in Epoch 816:0.4483\n",
      "Average loss in Epoch 817:0.4558\n",
      "Average loss in Epoch 818:0.4556\n",
      "Average loss in Epoch 819:0.4487\n",
      "Average loss in Epoch 820:0.4500\n",
      "Average loss in Epoch 821:0.4429\n",
      "Average loss in Epoch 822:0.4512\n",
      "Average loss in Epoch 823:0.4437\n",
      "Average loss in Epoch 824:0.4479\n",
      "Average loss in Epoch 825:0.4469\n",
      "Average loss in Epoch 826:0.4414\n",
      "Average loss in Epoch 827:0.4421\n",
      "Average loss in Epoch 828:0.4368\n",
      "Average loss in Epoch 829:0.4460\n",
      "Average loss in Epoch 830:0.4475\n",
      "Average loss in Epoch 831:0.4408\n",
      "Average loss in Epoch 832:0.4398\n",
      "Average loss in Epoch 833:0.4304\n",
      "Average loss in Epoch 834:0.4369\n",
      "Average loss in Epoch 835:0.4306\n",
      "Average loss in Epoch 836:0.4377\n",
      "Average loss in Epoch 837:0.4280\n",
      "Average loss in Epoch 838:0.4403\n",
      "Average loss in Epoch 839:0.4387\n",
      "Average loss in Epoch 840:0.4310\n",
      "Average loss in Epoch 841:0.4386\n",
      "Average loss in Epoch 842:0.4379\n",
      "Average loss in Epoch 843:0.4328\n",
      "Average loss in Epoch 844:0.4298\n",
      "Average loss in Epoch 845:0.4276\n",
      "Average loss in Epoch 846:0.4258\n",
      "Average loss in Epoch 847:0.4391\n",
      "Average loss in Epoch 848:0.4312\n",
      "Average loss in Epoch 849:0.4197\n",
      "Average loss in Epoch 850:0.4296\n",
      "Average loss in Epoch 851:0.4186\n",
      "Average loss in Epoch 852:0.4206\n",
      "Average loss in Epoch 853:0.4258\n",
      "Average loss in Epoch 854:0.4199\n",
      "Average loss in Epoch 855:0.4299\n",
      "Average loss in Epoch 856:0.4201\n",
      "Average loss in Epoch 857:0.4243\n",
      "Average loss in Epoch 858:0.4222\n",
      "Average loss in Epoch 859:0.4157\n",
      "Average loss in Epoch 860:0.4164\n",
      "Average loss in Epoch 861:0.4196\n",
      "Average loss in Epoch 862:0.4137\n",
      "Average loss in Epoch 863:0.4095\n",
      "Average loss in Epoch 864:0.4143\n",
      "Average loss in Epoch 865:0.4166\n",
      "Average loss in Epoch 866:0.4140\n",
      "Average loss in Epoch 867:0.4146\n",
      "Average loss in Epoch 868:0.4148\n",
      "Average loss in Epoch 869:0.4066\n",
      "Average loss in Epoch 870:0.4109\n",
      "Average loss in Epoch 871:0.4045\n",
      "Average loss in Epoch 872:0.4135\n",
      "Average loss in Epoch 873:0.4121\n",
      "Average loss in Epoch 874:0.4117\n",
      "Average loss in Epoch 875:0.4046\n",
      "Average loss in Epoch 876:0.4052\n",
      "Average loss in Epoch 877:0.4044\n",
      "Average loss in Epoch 878:0.4060\n",
      "Average loss in Epoch 879:0.4077\n",
      "Average loss in Epoch 880:0.4045\n",
      "Average loss in Epoch 881:0.4027\n",
      "Average loss in Epoch 882:0.4049\n",
      "Average loss in Epoch 883:0.4053\n",
      "Average loss in Epoch 884:0.4062\n",
      "Average loss in Epoch 885:0.4023\n",
      "Average loss in Epoch 886:0.3951\n",
      "Average loss in Epoch 887:0.4009\n",
      "Average loss in Epoch 888:0.3942\n",
      "Average loss in Epoch 889:0.4057\n",
      "Average loss in Epoch 890:0.4026\n",
      "Average loss in Epoch 891:0.3963\n",
      "Average loss in Epoch 892:0.4003\n",
      "Average loss in Epoch 893:0.3952\n",
      "Average loss in Epoch 894:0.4007\n",
      "Average loss in Epoch 895:0.3942\n",
      "Average loss in Epoch 896:0.3978\n",
      "Average loss in Epoch 897:0.4005\n",
      "Average loss in Epoch 898:0.3890\n",
      "Average loss in Epoch 899:0.3936\n",
      "Average loss in Epoch 900:0.3990\n",
      "Average loss in Epoch 901:0.3914\n",
      "Average loss in Epoch 902:0.3944\n",
      "Average loss in Epoch 903:0.3927\n",
      "Average loss in Epoch 904:0.3867\n",
      "Average loss in Epoch 905:0.3917\n",
      "Average loss in Epoch 906:0.3848\n",
      "Average loss in Epoch 907:0.3944\n",
      "Average loss in Epoch 908:0.3887\n",
      "Average loss in Epoch 909:0.3885\n",
      "Average loss in Epoch 910:0.3925\n",
      "Average loss in Epoch 911:0.3806\n",
      "Average loss in Epoch 912:0.3920\n",
      "Average loss in Epoch 913:0.3915\n",
      "Average loss in Epoch 914:0.3926\n",
      "Average loss in Epoch 915:0.3885\n",
      "Average loss in Epoch 916:0.3872\n",
      "Average loss in Epoch 917:0.3851\n",
      "Average loss in Epoch 918:0.3806\n",
      "Average loss in Epoch 919:0.3858\n",
      "Average loss in Epoch 920:0.3840\n",
      "Average loss in Epoch 921:0.3790\n",
      "Average loss in Epoch 922:0.3888\n",
      "Average loss in Epoch 923:0.3776\n",
      "Average loss in Epoch 924:0.3772\n",
      "Average loss in Epoch 925:0.3779\n",
      "Average loss in Epoch 926:0.3768\n",
      "Average loss in Epoch 927:0.3765\n",
      "Average loss in Epoch 928:0.3733\n",
      "Average loss in Epoch 929:0.3729\n",
      "Average loss in Epoch 930:0.3803\n",
      "Average loss in Epoch 931:0.3762\n",
      "Average loss in Epoch 932:0.3777\n",
      "Average loss in Epoch 933:0.3729\n",
      "Average loss in Epoch 934:0.3737\n",
      "Average loss in Epoch 935:0.3775\n",
      "Average loss in Epoch 936:0.3774\n",
      "Average loss in Epoch 937:0.3772\n",
      "Average loss in Epoch 938:0.3682\n",
      "Average loss in Epoch 939:0.3720\n",
      "Average loss in Epoch 940:0.3728\n",
      "Average loss in Epoch 941:0.3671\n",
      "Average loss in Epoch 942:0.3721\n",
      "Average loss in Epoch 943:0.3692\n",
      "Average loss in Epoch 944:0.3664\n",
      "Average loss in Epoch 945:0.3655\n",
      "Average loss in Epoch 946:0.3666\n",
      "Average loss in Epoch 947:0.3719\n",
      "Average loss in Epoch 948:0.3648\n",
      "Average loss in Epoch 949:0.3597\n",
      "Average loss in Epoch 950:0.3647\n",
      "Average loss in Epoch 951:0.3743\n",
      "Average loss in Epoch 952:0.3629\n",
      "Average loss in Epoch 953:0.3601\n",
      "Average loss in Epoch 954:0.3607\n",
      "Average loss in Epoch 955:0.3644\n",
      "Average loss in Epoch 956:0.3612\n",
      "Average loss in Epoch 957:0.3588\n",
      "Average loss in Epoch 958:0.3545\n",
      "Average loss in Epoch 959:0.3596\n",
      "Average loss in Epoch 960:0.3556\n",
      "Average loss in Epoch 961:0.3609\n",
      "Average loss in Epoch 962:0.3634\n",
      "Average loss in Epoch 963:0.3572\n",
      "Average loss in Epoch 964:0.3540\n",
      "Average loss in Epoch 965:0.3531\n",
      "Average loss in Epoch 966:0.3587\n",
      "Average loss in Epoch 967:0.3607\n",
      "Average loss in Epoch 968:0.3534\n",
      "Average loss in Epoch 969:0.3595\n",
      "Average loss in Epoch 970:0.3590\n",
      "Average loss in Epoch 971:0.3555\n",
      "Average loss in Epoch 972:0.3528\n",
      "Average loss in Epoch 973:0.3604\n",
      "Average loss in Epoch 974:0.3514\n",
      "Average loss in Epoch 975:0.3538\n",
      "Average loss in Epoch 976:0.3598\n",
      "Average loss in Epoch 977:0.3543\n",
      "Average loss in Epoch 978:0.3516\n",
      "Average loss in Epoch 979:0.3565\n",
      "Average loss in Epoch 980:0.3607\n",
      "Average loss in Epoch 981:0.3573\n",
      "Average loss in Epoch 982:0.3543\n",
      "Average loss in Epoch 983:0.3483\n",
      "Average loss in Epoch 984:0.3535\n",
      "Average loss in Epoch 985:0.3489\n",
      "Average loss in Epoch 986:0.3475\n",
      "Average loss in Epoch 987:0.3476\n",
      "Average loss in Epoch 988:0.3441\n",
      "Average loss in Epoch 989:0.3441\n",
      "Average loss in Epoch 990:0.3452\n",
      "Average loss in Epoch 991:0.3493\n",
      "Average loss in Epoch 992:0.3483\n",
      "Average loss in Epoch 993:0.3494\n",
      "Average loss in Epoch 994:0.3448\n",
      "Average loss in Epoch 995:0.3461\n",
      "Average loss in Epoch 996:0.3504\n",
      "Average loss in Epoch 997:0.3412\n",
      "Average loss in Epoch 998:0.3415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss in Epoch 999:0.3411\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[7.4221527862548831,\n",
       " 7.3584401702880857,\n",
       " 6.5256946182250974,\n",
       " 5.94500732421875,\n",
       " 5.7494622611999509,\n",
       " 5.6481802558898924,\n",
       " 5.577843055725098,\n",
       " 5.5332091140747073,\n",
       " 5.5026279830932614,\n",
       " 5.4657371330261233,\n",
       " 5.4388532066345219,\n",
       " 5.4214054489135739,\n",
       " 5.4063381958007817,\n",
       " 5.3938037872314455,\n",
       " 5.395850791931152,\n",
       " 5.3842817878723146,\n",
       " 5.3792078781127932,\n",
       " 5.3875311279296874,\n",
       " 5.3808303070068355,\n",
       " 5.3778426170349123,\n",
       " 5.3757103919982914,\n",
       " 5.3725952911376957,\n",
       " 5.369075698852539,\n",
       " 5.3693513107299804,\n",
       " 5.3713754653930668,\n",
       " 5.3639495468139646,\n",
       " 5.35998836517334,\n",
       " 5.3608768272399905,\n",
       " 5.3581439208984376,\n",
       " 5.3542014694213869,\n",
       " 5.3577730751037596,\n",
       " 5.3498951339721676,\n",
       " 5.3571703529357908,\n",
       " 5.3597930145263675,\n",
       " 5.3548560714721676,\n",
       " 5.3567337989807129,\n",
       " 5.3582329177856449,\n",
       " 5.3560940361022951,\n",
       " 5.3449330902099605,\n",
       " 5.3538310623168943,\n",
       " 5.3507927131652835,\n",
       " 5.3434312820434569,\n",
       " 5.352578620910645,\n",
       " 5.3464652633666994,\n",
       " 5.3442092323303223,\n",
       " 5.3483645439147951,\n",
       " 5.3487653350830078,\n",
       " 5.3453985023498536,\n",
       " 5.3390080070495607,\n",
       " 5.3373727226257328,\n",
       " 5.3369811630249027,\n",
       " 5.3349668312072751,\n",
       " 5.3322968864440918,\n",
       " 5.3360118865966797,\n",
       " 5.3354014015197757,\n",
       " 5.3304531669616697,\n",
       " 5.3295004463195799,\n",
       " 5.328268547058105,\n",
       " 5.3249209594726565,\n",
       " 5.3344186401367191,\n",
       " 5.3310429763793943,\n",
       " 5.3278154945373535,\n",
       " 5.319438552856445,\n",
       " 5.3228172302246097,\n",
       " 5.3252602005004883,\n",
       " 5.323227500915527,\n",
       " 5.3238192176818844,\n",
       " 5.3234881210327147,\n",
       " 5.3180141448974609,\n",
       " 5.3124984169006346,\n",
       " 5.3044797515869142,\n",
       " 5.3095548057556154,\n",
       " 5.3122085762023925,\n",
       " 5.306826248168945,\n",
       " 5.3196092414855958,\n",
       " 5.3154852485656736,\n",
       " 5.3018037605285642,\n",
       " 5.304441108703613,\n",
       " 5.3028498268127437,\n",
       " 5.3017132759094237,\n",
       " 5.3007546043395992,\n",
       " 5.3000554275512695,\n",
       " 5.2911691284179687,\n",
       " 5.2909509658813478,\n",
       " 5.2910783767700194,\n",
       " 5.2893686294555664,\n",
       " 5.2901719665527347,\n",
       " 5.284028472900391,\n",
       " 5.287772350311279,\n",
       " 5.2883786392211913,\n",
       " 5.2792867279052738,\n",
       " 5.2799600219726566,\n",
       " 5.2736141395568845,\n",
       " 5.276854457855225,\n",
       " 5.2642880630493165,\n",
       " 5.2664666366577144,\n",
       " 5.2685158729553221,\n",
       " 5.2639389801025391,\n",
       " 5.258008918762207,\n",
       " 5.2513730430603029,\n",
       " 5.254687156677246,\n",
       " 5.2412561988830566,\n",
       " 5.2337784194946293,\n",
       " 5.2320367813110353,\n",
       " 5.2150799560546872,\n",
       " 5.2056175613403317,\n",
       " 5.2121714401245116,\n",
       " 5.2100466537475585,\n",
       " 5.1876848983764647,\n",
       " 5.1759997367858883,\n",
       " 5.1639389801025395,\n",
       " 5.1479952239990237,\n",
       " 5.1354500961303708,\n",
       " 5.1207750892639163,\n",
       " 5.1203507804870609,\n",
       " 5.1069667243957522,\n",
       " 5.0779377365112301,\n",
       " 5.0600122642517089,\n",
       " 5.0373014450073246,\n",
       " 5.0182001113891603,\n",
       " 4.9894074249267577,\n",
       " 4.9760993766784667,\n",
       " 4.9571084213256835,\n",
       " 4.9326874923706052,\n",
       " 4.9242755699157712,\n",
       " 4.8958223152160647,\n",
       " 4.8731480598449703,\n",
       " 4.8417672157287601,\n",
       " 4.8169982528686521,\n",
       " 4.7914687156677243,\n",
       " 4.7639156150817872,\n",
       " 4.749567852020264,\n",
       " 4.7382732200622559,\n",
       " 4.7098608207702632,\n",
       " 4.6971924018859861,\n",
       " 4.6762416648864749,\n",
       " 4.6450051498413085,\n",
       " 4.6203490829467775,\n",
       " 4.5966883277893062,\n",
       " 4.5722203636169434,\n",
       " 4.5561060523986816,\n",
       " 4.5421742057800296,\n",
       " 4.5215505218505863,\n",
       " 4.5057265663146975,\n",
       " 4.4776207733154294,\n",
       " 4.4504791831970216,\n",
       " 4.4305458450317383,\n",
       " 4.4026102256774902,\n",
       " 4.3860347557067874,\n",
       " 4.3531902217864991,\n",
       " 4.3440137672424317,\n",
       " 4.3191051387786867,\n",
       " 4.3043946075439452,\n",
       " 4.2835513305664064,\n",
       " 4.2713252830505368,\n",
       " 4.2404575061798093,\n",
       " 4.2200159740447996,\n",
       " 4.2107444667816161,\n",
       " 4.1778400611877444,\n",
       " 4.1534749412536618,\n",
       " 4.1309356594085695,\n",
       " 4.1228660869598386,\n",
       " 4.0964454555511471,\n",
       " 4.083541297912598,\n",
       " 4.0679991531372073,\n",
       " 4.0385119915008545,\n",
       " 4.0206375217437742,\n",
       " 3.9950512886047362,\n",
       " 3.9754491996765138,\n",
       " 3.9588378620147706,\n",
       " 3.9412553119659424,\n",
       " 3.9209317970275879,\n",
       " 3.9059398555755616,\n",
       " 3.8844316005706787,\n",
       " 3.8621837329864501,\n",
       " 3.8471499061584473,\n",
       " 3.8396465396881103,\n",
       " 3.8129374885559084,\n",
       " 3.8038812732696532,\n",
       " 3.7694919681549073,\n",
       " 3.7573170471191406,\n",
       " 3.7359448623657228,\n",
       " 3.7194306564331057,\n",
       " 3.7024948215484619,\n",
       " 3.6873917865753172,\n",
       " 3.6640634536743164,\n",
       " 3.6556120777130126,\n",
       " 3.6230634307861327,\n",
       " 3.6191232872009276,\n",
       " 3.5949981594085694,\n",
       " 3.5834912681579589,\n",
       " 3.5642182445526123,\n",
       " 3.5366007709503173,\n",
       " 3.5342006397247316,\n",
       " 3.5183969879150392,\n",
       " 3.495391721725464,\n",
       " 3.4841281986236572,\n",
       " 3.4675726604461672,\n",
       " 3.4543111515045166,\n",
       " 3.4476155948638918,\n",
       " 3.4409816455841065,\n",
       " 3.4080895042419432,\n",
       " 3.4010074138641357,\n",
       " 3.3705863285064699,\n",
       " 3.3707588005065916,\n",
       " 3.3555877017974853,\n",
       " 3.3443354320526124,\n",
       " 3.328657865524292,\n",
       " 3.3004907131195069,\n",
       " 3.2905894279479981,\n",
       " 3.2786728763580322,\n",
       " 3.2661810016632078,\n",
       " 3.2728812599182131,\n",
       " 3.2375195598602295,\n",
       " 3.2204583168029783,\n",
       " 3.2044960689544677,\n",
       " 3.1875540924072268,\n",
       " 3.175116767883301,\n",
       " 3.1558472347259521,\n",
       " 3.1430498218536376,\n",
       " 3.1460931873321534,\n",
       " 3.1183770084381104,\n",
       " 3.1199313831329345,\n",
       " 3.094294662475586,\n",
       " 3.0820454883575441,\n",
       " 3.0825450611114502,\n",
       " 3.0599743938446045,\n",
       " 3.0328910064697268,\n",
       " 3.026412467956543,\n",
       " 3.0359638881683351,\n",
       " 3.0245752143859863,\n",
       " 3.0235145854949952,\n",
       " 3.022322664260864,\n",
       " 3.0265175247192384,\n",
       " 3.0084733772277832,\n",
       " 2.9807452011108397,\n",
       " 2.9422200870513917,\n",
       " 2.9320441341400145,\n",
       " 2.900922040939331,\n",
       " 2.8776548767089842,\n",
       " 2.865242156982422,\n",
       " 2.8440982246398927,\n",
       " 2.8322889614105224,\n",
       " 2.8132569694519045,\n",
       " 2.7986627769470216,\n",
       " 2.7888525676727296,\n",
       " 2.7779275608062743,\n",
       " 2.7599366188049315,\n",
       " 2.7577967834472656,\n",
       " 2.7368610858917237,\n",
       " 2.7216266155242921,\n",
       " 2.717864923477173,\n",
       " 2.7055380821228026,\n",
       " 2.6914629077911378,\n",
       " 2.6675943756103515,\n",
       " 2.6698675537109375,\n",
       " 2.6517435836791994,\n",
       " 2.6301273250579835,\n",
       " 2.6170792293548586,\n",
       " 2.6259119701385498,\n",
       " 2.6084274864196777,\n",
       " 2.5913057613372801,\n",
       " 2.5906893157958986,\n",
       " 2.580577459335327,\n",
       " 2.571992416381836,\n",
       " 2.5593613052368163,\n",
       " 2.5328475666046142,\n",
       " 2.5255048561096189,\n",
       " 2.5099263381958008,\n",
       " 2.5009623050689695,\n",
       " 2.504997091293335,\n",
       " 2.4777734565734861,\n",
       " 2.4631173515319826,\n",
       " 2.4519758510589598,\n",
       " 2.4428028678894043,\n",
       " 2.4298303222656248,\n",
       " 2.4274981784820557,\n",
       " 2.4221528816223143,\n",
       " 2.3966419982910154,\n",
       " 2.4005223274230958,\n",
       " 2.3860564613342286,\n",
       " 2.3685415840148925,\n",
       " 2.3584506702423096,\n",
       " 2.3735942840576172,\n",
       " 2.3601341629028321,\n",
       " 2.3462058830261232,\n",
       " 2.3255319786071778,\n",
       " 2.3250380897521974,\n",
       " 2.30826735496521,\n",
       " 2.2867103004455567,\n",
       " 2.2878445434570311,\n",
       " 2.2789268589019773,\n",
       " 2.2727872943878173,\n",
       " 2.252914056777954,\n",
       " 2.2418074321746828,\n",
       " 2.2483014249801636,\n",
       " 2.2309655237197874,\n",
       " 2.2336803245544434,\n",
       " 2.2129192876815797,\n",
       " 2.1952978754043579,\n",
       " 2.1856361865997314,\n",
       " 2.1724921083450317,\n",
       " 2.1668675756454467,\n",
       " 2.1714047002792358,\n",
       " 2.1559402942657471,\n",
       " 2.1495268774032592,\n",
       " 2.1526759386062624,\n",
       " 2.1316635131835939,\n",
       " 2.1229087591171263,\n",
       " 2.124406042098999,\n",
       " 2.1122565031051637,\n",
       " 2.112983989715576,\n",
       " 2.1135977840423585,\n",
       " 2.111970558166504,\n",
       " 2.1135575485229494,\n",
       " 2.1033457708358765,\n",
       " 2.1093337726593018,\n",
       " 2.090025029182434,\n",
       " 2.0743687582015991,\n",
       " 2.0825299263000487,\n",
       " 2.0509177684783935,\n",
       " 2.0400637531280519,\n",
       " 2.0151655578613283,\n",
       " 1.9944683265686036,\n",
       " 1.9915280103683473,\n",
       " 1.9683261632919311,\n",
       " 1.9573550510406494,\n",
       " 1.94571120262146,\n",
       " 1.9413749694824218,\n",
       " 1.9273350572586059,\n",
       " 1.9289210987091066,\n",
       " 1.9093281650543212,\n",
       " 1.9040094900131226,\n",
       " 1.8990198612213134,\n",
       " 1.8882763862609864,\n",
       " 1.8744382047653199,\n",
       " 1.8886899280548095,\n",
       " 1.856628451347351,\n",
       " 1.8489511346817016,\n",
       " 1.8459548377990722,\n",
       " 1.8510665798187256,\n",
       " 1.8321240377426147,\n",
       " 1.8275108337402344,\n",
       " 1.820609393119812,\n",
       " 1.7993226194381713,\n",
       " 1.8005377292633056,\n",
       " 1.7847217035293579,\n",
       " 1.7894541120529175,\n",
       " 1.7721840095520021,\n",
       " 1.7778845262527465,\n",
       " 1.7622228240966797,\n",
       " 1.7523553657531739,\n",
       " 1.7431347179412842,\n",
       " 1.7476109170913696,\n",
       " 1.7289967679977416,\n",
       " 1.7334780311584472,\n",
       " 1.7242984104156494,\n",
       " 1.7238156604766846,\n",
       " 1.7009933853149415,\n",
       " 1.7088205575942994,\n",
       " 1.7029066038131715,\n",
       " 1.6969611263275146,\n",
       " 1.6748800992965698,\n",
       " 1.6743900203704833,\n",
       " 1.6663874340057374,\n",
       " 1.6583948564529418,\n",
       " 1.6596222400665284,\n",
       " 1.6378013038635253,\n",
       " 1.6483747529983521,\n",
       " 1.6183521938323975,\n",
       " 1.6074901580810548,\n",
       " 1.6171087360382079,\n",
       " 1.6160859918594361,\n",
       " 1.6120185232162476,\n",
       " 1.5947991037368774,\n",
       " 1.5927999114990234,\n",
       " 1.5917306327819825,\n",
       " 1.5740431022644044,\n",
       " 1.563594765663147,\n",
       " 1.573449878692627,\n",
       " 1.5707668209075927,\n",
       " 1.5561226797103882,\n",
       " 1.5430484914779663,\n",
       " 1.5423024129867553,\n",
       " 1.5330065059661866,\n",
       " 1.5287693405151368,\n",
       " 1.521756772994995,\n",
       " 1.5188290119171142,\n",
       " 1.5153705453872681,\n",
       " 1.4998920488357543,\n",
       " 1.5031626510620117,\n",
       " 1.5015753889083863,\n",
       " 1.490829997062683,\n",
       " 1.4910142183303834,\n",
       " 1.4790805959701538,\n",
       " 1.4632936859130858,\n",
       " 1.4684159660339355,\n",
       " 1.4521570587158203,\n",
       " 1.4510538578033447,\n",
       " 1.4496478366851806,\n",
       " 1.4264796161651612,\n",
       " 1.4322465085983276,\n",
       " 1.4386480760574341,\n",
       " 1.4261789989471436,\n",
       " 1.4262738752365112,\n",
       " 1.4164854907989501,\n",
       " 1.4062297391891478,\n",
       " 1.3952089214324952,\n",
       " 1.3987441396713256,\n",
       " 1.3982387256622315,\n",
       " 1.3963437747955323,\n",
       " 1.3768026208877564,\n",
       " 1.3765675306320191,\n",
       " 1.3634706163406372,\n",
       " 1.3694638919830322,\n",
       " 1.3580503273010254,\n",
       " 1.3559977293014527,\n",
       " 1.3457520532608032,\n",
       " 1.3334577178955078,\n",
       " 1.3386881923675538,\n",
       " 1.3407175540924072,\n",
       " 1.329975209236145,\n",
       " 1.3241577863693237,\n",
       " 1.3245466709136964,\n",
       " 1.3185136032104492,\n",
       " 1.3053386592864991,\n",
       " 1.3007046270370484,\n",
       " 1.2990344047546387,\n",
       " 1.3065052366256713,\n",
       " 1.2992383241653442,\n",
       " 1.2936356687545776,\n",
       " 1.2856349468231201,\n",
       " 1.2725045680999756,\n",
       " 1.2745724201202393,\n",
       " 1.2730027198791505,\n",
       " 1.2644436168670654,\n",
       " 1.2602161359786987,\n",
       " 1.2643581819534302,\n",
       " 1.2679004764556885,\n",
       " 1.2599554729461671,\n",
       " 1.2489209985733032,\n",
       " 1.2624960136413574,\n",
       " 1.2588480710983276,\n",
       " 1.2486552095413208,\n",
       " 1.2332740020751953,\n",
       " 1.2446456813812257,\n",
       " 1.2283463811874389,\n",
       " 1.225635485649109,\n",
       " 1.2243191385269165,\n",
       " 1.2221767997741699,\n",
       " 1.2060820007324218,\n",
       " 1.2034627532958984,\n",
       " 1.2013841629028321,\n",
       " 1.1988494682312012,\n",
       " 1.1807673048973084,\n",
       " 1.1769843435287475,\n",
       " 1.1682907032966614,\n",
       " 1.1640955924987793,\n",
       " 1.1620778560638427,\n",
       " 1.1567160415649413,\n",
       " 1.1535183715820312,\n",
       " 1.1369384503364564,\n",
       " 1.1396801877021789,\n",
       " 1.1313538289070129,\n",
       " 1.137551612854004,\n",
       " 1.1264463710784911,\n",
       " 1.1161316108703614,\n",
       " 1.1245843482017517,\n",
       " 1.1131592011451721,\n",
       " 1.116110999584198,\n",
       " 1.1012220335006715,\n",
       " 1.107376606464386,\n",
       " 1.1065175867080688,\n",
       " 1.0902182078361511,\n",
       " 1.0940165781974793,\n",
       " 1.0975674557685853,\n",
       " 1.0920979118347167,\n",
       " 1.0739957761764527,\n",
       " 1.0850919485092163,\n",
       " 1.0718385291099548,\n",
       " 1.0665110969543456,\n",
       " 1.0748207569122314,\n",
       " 1.0596967649459839,\n",
       " 1.0586482858657837,\n",
       " 1.0560842490196227,\n",
       " 1.0522365140914918,\n",
       " 1.0496052956581117,\n",
       " 1.0451574206352234,\n",
       " 1.0337936353683472,\n",
       " 1.0323214554786682,\n",
       " 1.0305109930038452,\n",
       " 1.0340882182121276,\n",
       " 1.0273756098747253,\n",
       " 1.0185643458366394,\n",
       " 1.0240372967720033,\n",
       " 1.0157731652259827,\n",
       " 1.0112653088569641,\n",
       " 1.0103992342948913,\n",
       " 1.0075526928901672,\n",
       " 1.0064872527122497,\n",
       " 1.0013346195220947,\n",
       " 0.98837607383728032,\n",
       " 0.99626519441604611,\n",
       " 0.98793343544006351,\n",
       " 0.98606950998306275,\n",
       " 0.98244174242019655,\n",
       " 0.98114828824996947,\n",
       " 0.9816048455238342,\n",
       " 0.98296910285949712,\n",
       " 0.96680465698242191,\n",
       " 0.96839428424835206,\n",
       " 0.96579719066619873,\n",
       " 0.96299297809600826,\n",
       " 0.95554527759551999,\n",
       " 0.95838735818862919,\n",
       " 0.94098804235458378,\n",
       " 0.94370928049087521,\n",
       " 0.94361306667327882,\n",
       " 0.95169960021972655,\n",
       " 0.94685839414596562,\n",
       " 0.93229031562805176,\n",
       " 0.9353649497032166,\n",
       " 0.92820117235183719,\n",
       " 0.9356484818458557,\n",
       " 0.92395933151245113,\n",
       " 0.92026741266250611,\n",
       " 0.91995228052139277,\n",
       " 0.92530780792236333,\n",
       " 0.91553754329681392,\n",
       " 0.92586846113204957,\n",
       " 0.90421419143676762,\n",
       " 0.90452504873275752,\n",
       " 0.89900999307632445,\n",
       " 0.89557102918624876,\n",
       " 0.89205873250961298,\n",
       " 0.88428402662277217,\n",
       " 0.8885353493690491,\n",
       " 0.89424808025360103,\n",
       " 0.86866426229476934,\n",
       " 0.86561300277709963,\n",
       " 0.87046073198318485,\n",
       " 0.88883929014205931,\n",
       " 0.8657151293754578,\n",
       " 0.87726442098617552,\n",
       " 0.86571649074554446,\n",
       " 0.86167377471923823,\n",
       " 0.85397690057754516,\n",
       " 0.85421156167984014,\n",
       " 0.85418877124786374,\n",
       " 0.86092531919479365,\n",
       " 0.84413165569305415,\n",
       " 0.85545010089874263,\n",
       " 0.84129242658615111,\n",
       " 0.84638462305068973,\n",
       " 0.84668935775756837,\n",
       " 0.84115512371063228,\n",
       " 0.83627895593643187,\n",
       " 0.84416791677474978,\n",
       " 0.83627270460128789,\n",
       " 0.82232749938964844,\n",
       " 0.81949842691421504,\n",
       " 0.83491580009460453,\n",
       " 0.83362903833389279,\n",
       " 0.82020900011062625,\n",
       " 0.82091032743453984,\n",
       " 0.81811532258987429,\n",
       " 0.81471896648406983,\n",
       " 0.82244230508804317,\n",
       " 0.8078960967063904,\n",
       " 0.81226448059082035,\n",
       " 0.80972301959991455,\n",
       " 0.80095920801162723,\n",
       " 0.79428700685501097,\n",
       " 0.80059219360351563,\n",
       " 0.7977139282226563,\n",
       " 0.79697233200073248,\n",
       " 0.78221699237823483,\n",
       " 0.78463250637054438,\n",
       " 0.78378710746765134,\n",
       " 0.78317522525787353,\n",
       " 0.7768153572082519,\n",
       " 0.77684392929077151,\n",
       " 0.77197131872177127,\n",
       " 0.76662331104278569,\n",
       " 0.76167272329330449,\n",
       " 0.77183893918991087,\n",
       " 0.76603260755538938,\n",
       " 0.76393938064575195,\n",
       " 0.76674957513809205,\n",
       " 0.75906402826309205,\n",
       " 0.75719867229461668,\n",
       " 0.74714977025985718,\n",
       " 0.74393297433853145,\n",
       " 0.75635899782180782,\n",
       " 0.75534586668014525,\n",
       " 0.748995053768158,\n",
       " 0.74840411186218259,\n",
       " 0.74717927694320674,\n",
       " 0.73630857706069941,\n",
       " 0.73547175407409671,\n",
       " 0.737761697769165,\n",
       " 0.73723613739013671,\n",
       " 0.73417364120483397,\n",
       " 0.73232413530349727,\n",
       " 0.73462144851684574,\n",
       " 0.72275485038757326,\n",
       " 0.71995236396789553,\n",
       " 0.72660975456237797,\n",
       " 0.72106163978576665,\n",
       " 0.72171727895736693,\n",
       " 0.72585580348968504,\n",
       " 0.72455552577972415,\n",
       " 0.71000982999801632,\n",
       " 0.70698132038116457,\n",
       " 0.70645978927612307,\n",
       " 0.69926494836807251,\n",
       " 0.70896960496902461,\n",
       " 0.69633321762084965,\n",
       " 0.6969953560829163,\n",
       " 0.69856063365936283,\n",
       " 0.696941454410553,\n",
       " 0.69926490068435665,\n",
       " 0.69261682748794551,\n",
       " 0.69570698976516721,\n",
       " 0.68762930631637575,\n",
       " 0.68851426601409915,\n",
       " 0.68768620967864991,\n",
       " 0.69118556261062625,\n",
       " 0.68116757869720457,\n",
       " 0.67603794574737552,\n",
       " 0.68612311601638798,\n",
       " 0.68278720378875735,\n",
       " 0.68015176057815552,\n",
       " 0.67498749732971186,\n",
       " 0.68000818490982051,\n",
       " 0.67106569051742548,\n",
       " 0.66634290695190435,\n",
       " 0.66608397483825688,\n",
       " 0.67085654020309449,\n",
       " 0.65546329259872438,\n",
       " 0.67300706624984741,\n",
       " 0.65517548799514769,\n",
       " 0.65836801528930666,\n",
       " 0.6580670428276062,\n",
       " 0.66505139350891118,\n",
       " 0.65047745704650883,\n",
       " 0.65178886175155637,\n",
       " 0.65095915079116817,\n",
       " 0.65916964054107663,\n",
       " 0.6506626892089844,\n",
       " 0.64603032588958742,\n",
       " 0.64471914768218996,\n",
       " 0.65642111778259282,\n",
       " 0.64402953624725345,\n",
       " 0.62820030450820918,\n",
       " 0.64246270418167117,\n",
       " 0.63803270816802982,\n",
       " 0.63971353054046631,\n",
       " 0.64014691352844233,\n",
       " 0.63470341205596925,\n",
       " 0.63500574588775638,\n",
       " 0.6325565481185913,\n",
       " 0.64006600141525272,\n",
       " 0.62282624483108517,\n",
       " 0.63020710229873655,\n",
       " 0.63019421339035031,\n",
       " 0.61817544579505923,\n",
       " 0.61517661333084106,\n",
       " 0.61524699091911317,\n",
       " 0.61684478282928468,\n",
       " 0.61394827604293822,\n",
       " 0.61657165646553036,\n",
       " 0.60807955980300898,\n",
       " 0.61044516324996945,\n",
       " 0.6131342434883118,\n",
       " 0.59965988278388982,\n",
       " 0.60865044116973877,\n",
       " 0.60205279827117919,\n",
       " 0.61399762511253353,\n",
       " 0.60600274682044986,\n",
       " 0.6070827841758728,\n",
       " 0.603512909412384,\n",
       " 0.60454003453254701,\n",
       " 0.60057376861572265,\n",
       " 0.60080674290657043,\n",
       " 0.59647508740425115,\n",
       " 0.58441615939140323,\n",
       " 0.58588311553001404,\n",
       " 0.59580207347869873,\n",
       " 0.58990004301071164,\n",
       " 0.58606461167335511,\n",
       " 0.58214105606079103,\n",
       " 0.5829105949401856,\n",
       " 0.5837838518619537,\n",
       " 0.58284476637840266,\n",
       " 0.57837543487548826,\n",
       " 0.58627489686012269,\n",
       " 0.57849691033363337,\n",
       " 0.57249778866767886,\n",
       " 0.5778120255470276,\n",
       " 0.56832442522048954,\n",
       " 0.56912591457366946,\n",
       " 0.57162504076957699,\n",
       " 0.57278132081031796,\n",
       " 0.5706016600131989,\n",
       " 0.57179424881935115,\n",
       " 0.56628861308097844,\n",
       " 0.56021588563919067,\n",
       " 0.57215287923812863,\n",
       " 0.56335068821907042,\n",
       " 0.55538209319114684,\n",
       " 0.55647035837173464,\n",
       " 0.55962642788887029,\n",
       " 0.5560606777667999,\n",
       " 0.55379572272300726,\n",
       " 0.54736967802047731,\n",
       " 0.55512505888938901,\n",
       " 0.56098681807518003,\n",
       " 0.55166816949844355,\n",
       " 0.54413838863372799,\n",
       " 0.53794752359390263,\n",
       " 0.54738056659698486,\n",
       " 0.54559847831726072,\n",
       " 0.54447687506675724,\n",
       " 0.5408895003795624,\n",
       " 0.54380323171615597,\n",
       " 0.54658499002456662,\n",
       " 0.54399252057075498,\n",
       " 0.54572289347648617,\n",
       " 0.54882686257362367,\n",
       " 0.54183111429214481,\n",
       " 0.53724119901657108,\n",
       " 0.52586765885353093,\n",
       " 0.5349587559700012,\n",
       " 0.53811520934104917,\n",
       " 0.52733330130577083,\n",
       " 0.53423594594001766,\n",
       " 0.53228352308273319,\n",
       " 0.53650526404380794,\n",
       " 0.5224393737316132,\n",
       " 0.52723647952079777,\n",
       " 0.5220276916027069,\n",
       " 0.52457660675048823,\n",
       " 0.52015430331230161,\n",
       " 0.52588893175125118,\n",
       " 0.52335767149925227,\n",
       " 0.51366611957550046,\n",
       " 0.52434586286544804,\n",
       " 0.51423745393753051,\n",
       " 0.51824408769607544,\n",
       " 0.51349029421806336,\n",
       " 0.51552203059196477,\n",
       " 0.5102045440673828,\n",
       " 0.51106615066528316,\n",
       " 0.50895344614982607,\n",
       " 0.50632065057754516,\n",
       " 0.5113853788375855,\n",
       " 0.5108246862888336,\n",
       " 0.50094305515289306,\n",
       " 0.51824803948402409,\n",
       " 0.51070929646492003,\n",
       " 0.50221475362777712,\n",
       " 0.50924546003341675,\n",
       " 0.50351765155792239,\n",
       " 0.50392148852348329,\n",
       " 0.492661691904068,\n",
       " 0.49821219801902772,\n",
       " 0.49287600040435792,\n",
       " 0.49258309721946714,\n",
       " 0.50320284724235531,\n",
       " 0.4928699195384979,\n",
       " 0.49666616320610046,\n",
       " 0.49426932930946349,\n",
       " 0.48796362042427061,\n",
       " 0.49052142739295962,\n",
       " 0.49326084971427919,\n",
       " 0.49717719435691832,\n",
       " 0.48234380841255187,\n",
       " 0.48543261885643008,\n",
       " 0.48989304900169373,\n",
       " 0.48497573137283323,\n",
       " 0.49591994047164917,\n",
       " 0.48174386858940127,\n",
       " 0.48778734207153318,\n",
       " 0.48264381885528562,\n",
       " 0.48592951536178591,\n",
       " 0.47662804722785951,\n",
       " 0.4791585922241211,\n",
       " 0.48122292637825015,\n",
       " 0.47696740865707399,\n",
       " 0.47264557003974916,\n",
       " 0.47746336817741392,\n",
       " 0.46971273064613345,\n",
       " 0.47188003182411192,\n",
       " 0.47084912180900573,\n",
       " 0.46637458920478819,\n",
       " 0.4642039394378662,\n",
       " 0.46564919233322144,\n",
       " 0.46518308520317075,\n",
       " 0.46329502344131468,\n",
       " 0.46716929674148561,\n",
       " 0.46778331875801088,\n",
       " 0.46711500406265261,\n",
       " 0.46106977701187135,\n",
       " 0.46368676662445069,\n",
       " 0.46375219821929931,\n",
       " 0.46470123648643491,\n",
       " 0.45526206374168399,\n",
       " 0.46069947957992552,\n",
       " 0.4627290940284729,\n",
       " 0.45552052259445192,\n",
       " 0.45499884247779848,\n",
       " 0.45307501912117004,\n",
       " 0.45194993019104002,\n",
       " 0.45355540037155151,\n",
       " 0.45326510548591614,\n",
       " 0.44834446549415591,\n",
       " 0.45584135890007021,\n",
       " 0.45558749079704286,\n",
       " 0.44868023991584777,\n",
       " 0.45002170324325563,\n",
       " 0.4428579330444336,\n",
       " 0.45120007514953614,\n",
       " 0.44373103380203249,\n",
       " 0.44792356491088869,\n",
       " 0.44693312764167786,\n",
       " 0.44138323307037353,\n",
       " 0.44205000042915343,\n",
       " 0.43678736686706543,\n",
       " 0.44602982521057127,\n",
       " 0.44751736521720886,\n",
       " 0.44083968400955198,\n",
       " 0.43976112604141238,\n",
       " 0.43043259024620056,\n",
       " 0.43686956644058228,\n",
       " 0.43055266261100766,\n",
       " 0.43774588108062745,\n",
       " 0.42797395706176755,\n",
       " 0.44034468770027163,\n",
       " 0.43871702432632448,\n",
       " 0.43103767395019532,\n",
       " 0.43861291766166688,\n",
       " 0.43792464852333068,\n",
       " 0.43283282041549681,\n",
       " 0.42976711988449096,\n",
       " 0.42756590843200681,\n",
       " 0.42582895755767824,\n",
       " 0.439074889421463,\n",
       " 0.43115199685096739,\n",
       " 0.41972041130065918,\n",
       " 0.42957928776741028,\n",
       " 0.41860386610031131,\n",
       " 0.42059243083000181,\n",
       " 0.4257656645774841,\n",
       " 0.41992315292358401,\n",
       " 0.42994356989860533,\n",
       " 0.42013429641723632,\n",
       " 0.42432283997535708,\n",
       " 0.42218592166900637,\n",
       " 0.41568626523017882,\n",
       " 0.41642431020736692,\n",
       " 0.41959657192230226,\n",
       " 0.41367895364761353,\n",
       " 0.40953116536140444,\n",
       " 0.41426022052764894,\n",
       " 0.41657297492027284,\n",
       " 0.41395546793937682,\n",
       " 0.41460389256477354,\n",
       " 0.4147786211967468,\n",
       " 0.40655839681625366,\n",
       " 0.41085835814476013,\n",
       " 0.40453015685081484,\n",
       " 0.41354720354080199,\n",
       " 0.41211082100868224,\n",
       " 0.4116793382167816,\n",
       " 0.40462622523307801,\n",
       " 0.40524706244468689,\n",
       " 0.40443148136138918,\n",
       " 0.4059689652919769,\n",
       " 0.40766418814659117,\n",
       " 0.40448878645896913,\n",
       " 0.40266699910163878,\n",
       " 0.40488510966300967,\n",
       " 0.4053486979007721,\n",
       " 0.4062286055088043,\n",
       " 0.40233146071434023,\n",
       " 0.39506085515022277,\n",
       " 0.40094429969787598,\n",
       " 0.39419411063194276,\n",
       " 0.40570368170738219,\n",
       " 0.40258024930953978,\n",
       " 0.39634240984916685,\n",
       " 0.40031981468200684,\n",
       " 0.39524566531181338,\n",
       " 0.4007265567779541,\n",
       " 0.39423874616622923,\n",
       " 0.39780923366546633,\n",
       " 0.40051624178886414,\n",
       " 0.38895202755928038,\n",
       " 0.39361984372138975,\n",
       " 0.39896826386451723,\n",
       " 0.39144642353057862,\n",
       " 0.39439007759094236,\n",
       " 0.3927136194705963,\n",
       " 0.38673742055892946,\n",
       " 0.39169878602027891,\n",
       " 0.38483598470687869,\n",
       " 0.39438987374305723,\n",
       " 0.38865147829055785,\n",
       " 0.38848159909248353,\n",
       " 0.39249743461608888,\n",
       " 0.38057742953300477,\n",
       " 0.39198671340942381,\n",
       " 0.39147007942199707,\n",
       " 0.39260767221450804,\n",
       " 0.38851318001747132,\n",
       " 0.38721821427345277,\n",
       " 0.38513740301132204,\n",
       " 0.38062315940856933,\n",
       " 0.38578574419021605,\n",
       " 0.38396273136138914,\n",
       " 0.3789694356918335,\n",
       " 0.38879606485366819,\n",
       " 0.37760026097297666,\n",
       " 0.37715224742889403,\n",
       " 0.37794979453086852,\n",
       " 0.37675534963607787,\n",
       " 0.37650285840034486,\n",
       " 0.3732537806034088,\n",
       " 0.37286823987960815,\n",
       " 0.38026295423507689,\n",
       " 0.3762009632587433,\n",
       " 0.3777296245098114,\n",
       " 0.37293506145477295,\n",
       " 0.37365975737571716,\n",
       " 0.3774892055988312,\n",
       " 0.37736873388290404,\n",
       " 0.37716162204742432,\n",
       " 0.36820791959762572,\n",
       " 0.37197629570960999,\n",
       " 0.37280170917510985,\n",
       " 0.36712582588195802,\n",
       " 0.37208284020423887,\n",
       " 0.3692336881160736,\n",
       " 0.36635289311408997,\n",
       " 0.36554751873016356,\n",
       " 0.36655085563659667,\n",
       " 0.37189350366592405,\n",
       " 0.36479964733123782,\n",
       " 0.35973304271697998,\n",
       " 0.36465466022491455,\n",
       " 0.37425781965255739,\n",
       " 0.36293250799179078,\n",
       " 0.36013278961181638,\n",
       " 0.36068238377571105,\n",
       " 0.36441068649291991,\n",
       " 0.36124828696250916,\n",
       " 0.35876119732856748,\n",
       " 0.35454188704490663,\n",
       " 0.35964937090873716,\n",
       " 0.35555661797523497,\n",
       " 0.36086228489875793,\n",
       " 0.36344956517219545,\n",
       " 0.3571916711330414,\n",
       " 0.35396823883056638,\n",
       " 0.35311040759086609,\n",
       " 0.3586952352523804,\n",
       " 0.36066058635711667,\n",
       " 0.35342952132225036,\n",
       " 0.35946325302124021,\n",
       " 0.35898882031440738,\n",
       " 0.35549406886100771,\n",
       " 0.35283233761787414,\n",
       " 0.36038321971893311,\n",
       " 0.35138871550559997,\n",
       " 0.35381571769714354,\n",
       " 0.35976606845855713,\n",
       " 0.35426665425300596,\n",
       " 0.35160346865653991,\n",
       " 0.35653337001800539,\n",
       " 0.36065717220306398,\n",
       " 0.35729280948638914,\n",
       " 0.35430464625358582,\n",
       " 0.34830802202224731,\n",
       " 0.35348896503448485,\n",
       " 0.34894742608070373,\n",
       " 0.34749083995819091,\n",
       " 0.34757346630096436,\n",
       " 0.34406263232231138,\n",
       " 0.34409122943878173,\n",
       " 0.3452044403553009,\n",
       " 0.34926174044609071,\n",
       " 0.34834829688072205,\n",
       " 0.34944125533103942,\n",
       " 0.34478475213050841,\n",
       " 0.34611925840377805,\n",
       " 0.35036660909652712,\n",
       " 0.34121907949447633,\n",
       " 0.34154597878456117,\n",
       " 0.34114702820777892]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_graph(batch_size, num_steps, state_size,\n",
    "                num_classes = vocab_size, keep_prob = 0.7,\n",
    "                learning_rate=1e-4):\n",
    "    \n",
    "    # wipe out all previously built graphs\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # word ids, without one-hot encoding\n",
    "    x = tf.placeholder(tf.int32, [batch_size, num_steps], name='x')\n",
    "    y = tf.placeholder(tf.int32, [batch_size, num_steps], name='y')\n",
    "\n",
    "    # vector representation for each words(word2vec)\n",
    "    word_embeddings = tf.get_variable('embedding_matrix',\n",
    "                                 [num_classes, state_size])\n",
    "\n",
    "    # rnn_inputs is a tensor of dim [batch_size,num_steps,state_size]\n",
    "    rnn_inputs = tf.nn.embedding_lookup(word_embeddings, x)\n",
    "\n",
    "    cell = rnn.GRUCell(state_size)\n",
    "    # Adding Dropout along to each stacked GRU layers\n",
    "    cell = rnn.DropoutWrapper(cell,\n",
    "                              input_keep_prob = keep_prob,\n",
    "                              output_keep_prob = keep_prob)\n",
    "    cell = rnn.MultiRNNCell([cell] * 5, state_is_tuple=True)\n",
    "    init_state = cell.zero_state(batch_size, tf.float32)\n",
    "    rnn_outputs, final_state =\\\n",
    "    tf.nn.dynamic_rnn(cell,rnn_inputs,initial_state=init_state)\n",
    "\n",
    "    with tf.variable_scope('softmax'):\n",
    "        W = tf.get_variable('W', [state_size, num_classes])\n",
    "        b = tf.get_variable('b', [num_classes],\n",
    "                            initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "    #reshape rnn_outputs and y so we can get the logits in a single matmul\n",
    "    rnn_outputs = tf.reshape(rnn_outputs, [-1, state_size])\n",
    "    y_reshaped = tf.reshape(y, [-1])\n",
    "\n",
    "    logits = tf.matmul(rnn_outputs, W) + b\n",
    "\n",
    "    # Make predictions\n",
    "    predictions = tf.nn.softmax(logits)\n",
    "\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            logits=logits, labels=y_reshaped))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "    \n",
    "    return dict(\n",
    "        x=x,\n",
    "        y=y,\n",
    "        init_state=init_state,\n",
    "        final_state=final_state,\n",
    "        loss=loss,\n",
    "        optimizer=optimizer,\n",
    "        preds=predictions,\n",
    "        saver=tf.train.Saver()\n",
    "    )\n",
    "\n",
    "batch_size = 32\n",
    "num_steps = 10\n",
    "state_size = 500\n",
    "num_epochs = 1000\n",
    "\n",
    "graph = build_graph(batch_size,num_steps,state_size)\n",
    "train_network(graph, num_epochs, batch_size, num_steps,\n",
    "              save='saves/gru_lm_1000epochs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Generating Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from saves/gru_lm_1000epochs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"<< the universal approximation theorem concerns the capacity of feedforward neural networks with a single hidden layer of finite size to approximate continuous functions and . . . . a world may propagate through a layer more than once , the cap depth is potentially unlimited . . . then time more showing hopfield , which demonstrated layer learning with the layers from nonlinear processing units used in used ) and neocognitron introduced by fukushima in in . . a cumulative concepts . . . . . . . each layer in turn as an unsupervised restricted boltzmann in data hand-written algorithm , more abstract days of the nodes by recurrent neural networks . in which a signal may propagate through a layer more than once , the cap depth is potentially unlimited . . time . . a time , pick out which features . useful . . . . . . . representation . . . . hornik . . respectively . . . . . . representation . . . . . . . . . . . . . . . . representation . . . . . . . . . . . a signal . in colleagues in 2000 , in the context of boolean threshold neurons . . . representation . . . . . . . . . . . colleagues . . a time by as aizenberg and colleagues in 2000 , in the context of boolean threshold neurons . . . . . . . . a optimization concepts of training and testing , data . are useful numbers . . . a output layer in turn as an unsupervised restricted boltzmann machine , then fine-tuning it using supervised backpropagation . . . . . representation . . . . . . . . . representation . . . . . . . . . . . . . . . . . . . an time in the representations in 2000 , in the context of boolean threshold neurons . . . . . . . a world 3-d are derived from speech recognition , and derive layered '' that redundancy in representation . . . . . . . . . representation . . . . . . . . . . . . . . . representation . . . . . a useful function . . . . . . . . . . . . researchers layer an output neural community and sets of complicated propositional input . output . output . . . than . . . a layer introduced by one the data into compact intermediate representations akin to principal components , and derive layered structures that remove redundancy in representation . . . . . . . . representation . . . . . . . . . . . . . . . an optimization threshold of depth divides shallow learning from deep learning , but most researchers in the field agree that deep learning has multiple nonlinear layers ( cap > 2 ) . . processing . turn in an data ( stimuli factors associated neuronal responses in the brain . layer . . . . cases . . . . . . . . in processing units . ( 2 ) the supervised or unsupervised learning of feature representations in each layer , with the layers forming a hierarchy from low-level to high-level features . . . . . . representation . . . . . . . . representation . . . . . . . . . . . . . . . finite size provide different amounts . . representation . . . . . . . . . . . . . . representation . . . . . . . . representation . . . . . . . . . . . . . . . . . observed data . at . finite . data . . . representation . . . . . . . representation . . . . . . . . . . representation . . . . . . . representation . . . . . . . . . . . . representation . . . . . cases distribution . . . . . . . . . . . . . trained in hierarchical representation . hierarchy with . . . . . a cumulative . . . . . . a time in the composition of a layer of nonlinear processing units used in a deep learning algorithm depends on the problem to be solved . . . a later in recurrent neural networks , in which a purpose of signal handwritten zip codes on mail . . aizenberg . . colleagues , 2000 , the neocognitron introduced by fukushima in 1980. in 1989 , lecun et al . . representation . . . . . . . . . cases superior labeled data . . . representation . . . . . . . . cases distribution . . . . . . . representation . . . . . representation . . . . . . . . . . . . . redundancy in but in in a computer identification system by the world school council london called `` alpha '' , which demonstrated the learning process . . . abstract data into they may results comparable to . and cumulative distribution function . . a handcrafted . . . . . is a time of nonlinear processing units and ( 2 ) the supervised or unsupervised learning of feature representations in each layer , with the layers forming a hierarchy from low-level to high-level features . . . abundant . . a derive layered part hornik . . . . . . . . . representation . . . . . representation . . . . . . . . . . . . . . . representation . .\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_words(graph, seed_word, n_words, trained_vars):\n",
    "    '''\n",
    "    Args:\n",
    "        seed_word: the seed word to initiate the generation\n",
    "        n_words: the number of words to generate\n",
    "        trained_vars: the path for the saved model(Variables), stored by tf.train.Saver()\n",
    "    '''\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        # restore the variables from the previous training\n",
    "        graph['saver'].restore(sess, trained_vars)\n",
    "        \n",
    "        # beginning of the sentence\n",
    "        state = None\n",
    "        current_word = vocab[seed_word]\n",
    "        i_words = [current_word]\n",
    "\n",
    "        for i in range(n_words):  \n",
    "            if state is not None:\n",
    "                feed_dict = {graph['x']:[[current_word]],\n",
    "                             graph['init_state']:state}\n",
    "            else:\n",
    "                feed_dict = {graph['x']:[[current_word]]}\n",
    "\n",
    "            preds, state = sess.run([graph['preds'],\n",
    "                                     graph['final_state']],\n",
    "                                    feed_dict)\n",
    "\n",
    "            current_word = np.random.choice(\n",
    "                a=vocab_size, size=1, p=np.squeeze(preds))[0]\n",
    "            i_words.append(current_word)\n",
    "\n",
    "    return ' '.join([rev_vocab[i] for i in i_words])\n",
    "\n",
    "graph_test = build_graph(1,1,state_size)\n",
    "generate_words(graph_test, START, 1000, 'saves/gru_lm_1000epochs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Layer Normalization(skipped)\n",
    "[Layer Normalization](https://arxiv.org/abs/1607.06450) is the RNN equivalent of Batch Normalization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
